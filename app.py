# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
import time
import logging
from pytrends.request import TrendReq
import openai
import numpy as np
from collections import Counter
import requests
import json
from dataclasses import dataclass
from urllib.parse import quote_plus
import unicodedata
import math

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('youtube_strategist.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è API –ª–∏–º–∏—Ç–æ–≤
YOUTUBE_API_DAILY_QUOTA = 10000
REQUEST_DELAY = 0.1
MAX_RETRIES = 3
REQUEST_TIMEOUT = 30

st.markdown("""
<style>
    .main-header {
        font-size: 2.8rem;
        color: #FF0000;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
        background: linear-gradient(90deg, #FF0000 0%, #FF6B6B 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .stButton>button {
        border-radius: 8px;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .custom-container {
        background: linear-gradient(135deg, rgba(42, 57, 62, 0.5), rgba(62, 77, 82, 0.3));
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #00a0dc;
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .openai-result {
        background: linear-gradient(135deg, rgba(26, 142, 95, 0.1), rgba(46, 162, 115, 0.05));
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #1a8e5f;
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .insight-box {
        background: linear-gradient(135deg, #262730, #3a3b45);
        padding: 1rem;
        border-radius: 15px;
        margin-top: 1rem;
        border: 1px solid #444;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #1e1e2e, #2a2a3e);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #444;
        margin: 0.5rem 0;
    }
    .success-alert {
        background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(34, 197, 94, 0.05));
        border: 1px solid rgba(34, 197, 94, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background: linear-gradient(135deg, rgba(251, 146, 60, 0.1), rgba(251, 146, 60, 0.05));
        border: 1px solid rgba(251, 146, 60, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. –£–¢–ò–õ–ò–¢–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø (–£–õ–£–ß–®–ï–ù–ù–´–ï) ---

def validate_youtube_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ YouTube API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    
    if api_key.startswith('AIza') and len(api_key) == 39:
        return True
    
    if len(api_key) > 30 and re.match(r'^[A-Za-z0-9_-]+$', api_key):
        return True
    
    return False

def validate_openai_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ OpenAI API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return api_key.startswith('sk-') and len(api_key) > 40

def validate_serpapi_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ SerpAPI –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return len(api_key) > 30 and all(c.isalnum() for c in api_key)

def safe_format_number(num) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        if pd.isna(num) or num is None:
            return "0"
        
        num = float(num)
        if num >= 1_000_000_000:
            return f"{num/1_000_000_000:.1f}B"
        elif num >= 1_000_000:
            return f"{num/1_000_000:.1f}M"
        elif num >= 1_000:
            return f"{num/1_000:.1f}K"
        return str(int(num))
    except (ValueError, TypeError, OverflowError):
        return "0"

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not isinstance(text, str):
        return ""
    
    text = unicodedata.normalize('NFKD', text)
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    return text.strip()

def safe_int_conversion(value, default=0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    try:
        if pd.isna(value) or value is None:
            return default
        return int(float(value))
    except (ValueError, TypeError, OverflowError):
        return default

def safe_float_conversion(value, default=0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
    try:
        if pd.isna(value) or value is None:
            return default
        return float(value)
    except (ValueError, TypeError, OverflowError):
        return default

def validate_keyword(keyword: str) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    if not keyword or not isinstance(keyword, str):
        return False
    
    keyword = keyword.strip()
    
    if len(keyword) < 2 or len(keyword) > 100:
        return False
    
    if keyword.count(' ') > 10:
        return False
    
    if re.search(r'[<>"\'\[\]{}|\\`]', keyword):
        return False
    
    return True

def extract_keywords_from_titles(titles: list, min_length=3, max_keywords=15) -> list:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    if not titles:
        return []
    
    all_words = []
    stop_words = {
        '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–∑–∞', '–æ—Ç', '–¥–æ',
        '–∏–∑', '–∫', '–æ', '—É', '–∂–µ', '–µ—â–µ', '—É–∂–µ', '–∏–ª–∏', '—Ç–∞–∫', '–Ω–æ', '–∞', '–∏—Ö', '–µ–≥–æ',
        '–µ—ë', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–µ—Å–ª–∏',
        '—á—Ç–æ–±—ã', '–∫–æ–≥–¥–∞', '–≥–¥–µ', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
        'for', 'of', 'with', 'by', 'you', 'are', 'can', 'all', 'any', 'how', 'what',
        'when', 'where', 'why', 'this', 'that', 'have', 'had', 'will', 'been', 'were',
        'was', 'are', 'is', 'am', 'be', 'do', 'did', 'does', 'has', 'get', 'got'
    }
    
    try:
        for title in titles:
            if not title:
                continue
            
            title_clean = clean_text(str(title).lower())
            words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', title_clean)
            filtered_words = [
                word for word in words
                if len(word) >= min_length and word not in stop_words
            ]
            all_words.extend(filtered_words)
        
        word_counts = Counter(all_words)
        return word_counts.most_common(max_keywords)
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return []

def retry_api_call(func, max_retries=MAX_RETRIES, delay=REQUEST_DELAY):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ API –≤—ã–∑–æ–≤–æ–≤"""
    def wrapper(*args, **kwargs):
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    logger.info(f"API –≤—ã–∑–æ–≤ —É—Å–ø–µ—à–µ–Ω —Å {attempt + 1} –ø–æ–ø—ã—Ç–∫–∏")
                return result
            
            except HttpError as e:
                last_exception = e
                status_code = e.resp.status
                
                if status_code == 403:
                    st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á –∏ –µ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ Google Cloud Console.")
                    logger.error(f"–û—à–∏–±–∫–∞ 403 (Forbidden). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code == 400:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
                    logger.error(f"–û—à–∏–±–∫–∞ 400 (Bad Request). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code in [500, 502, 503, 504]:
                    logger.warning(f"–°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))
                        continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {status_code}: {e}")
                    break
            
            except Exception as e:
                last_exception = e
                logger.warning(f"–û—à–∏–±–∫–∞ API –≤—ã–∑–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay)
                    continue
                break
        
        logger.error(f"API –≤—ã–∑–æ–≤ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_exception}")
        raise last_exception
    
    return wrapper

# --- 3. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ ---

class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {
            'search': 3600*4,
            'channels': 3600*24*7,
            'trends': 3600*8,
            'openai': 3600*24,
            'serpapi': 3600*6
        }
        self.stats = {'hits': 0, 'misses': 0, 'errors': 0, 'size_mb': 0}
        self._update_cache_stats()

    def _init_sqlite(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                with self.lock:
                    conn = sqlite3.connect(
                        self.db_path,
                        check_same_thread=False,
                        timeout=10.0
                    )
                    cursor = conn.cursor()
                    
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS cache (
                            key TEXT PRIMARY KEY,
                            value BLOB,
                            expires_at TIMESTAMP,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            access_count INTEGER DEFAULT 1,
                            category TEXT,
                            size_bytes INTEGER
                        )
                    ''')
                    
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON cache(category)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_access_count ON cache(access_count)')
                    
                    cursor.execute('PRAGMA journal_mode=WAL')
                    cursor.execute('PRAGMA synchronous=NORMAL')
                    cursor.execute('PRAGMA cache_size=10000')
                    
                    conn.commit()
                    conn.close()
                    logger.info("–ö—ç—à –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    return
            
            except sqlite3.Error as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1)
                    continue
                else:
                    st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")

    def _update_cache_stats(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                result = cursor.fetchone()
                if result:
                    self.stats['size_mb'] = round(result[0] / (1024 * 1024), 2)
                
                conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞: {e}")

    def get(self, key: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "SELECT value, expires_at, access_count FROM cache WHERE key = ?",
                    (key,)
                )
                result = cursor.fetchone()
                
                if result:
                    value_blob, expires_at, access_count = result
                    
                    if datetime.fromisoformat(expires_at) > datetime.now():
                        cursor.execute(
                            "UPDATE cache SET access_count = ? WHERE key = ?",
                            (access_count + 1, key)
                        )
                        conn.commit()
                        conn.close()
                        
                        self.stats['hits'] += 1
                        return pickle.loads(value_blob)
                    else:
                        cursor.execute("DELETE FROM cache WHERE key = ?", (key,))
                        conn.commit()
                
                conn.close()
                self.stats['misses'] += 1
                return None
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: {e}")
            return None

    def set(self, key: str, value: any, category: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            with self.lock:
                ttl = self.ttl_map.get(category, 3600)
                expires_at = datetime.now() + timedelta(seconds=ttl)
                value_blob = pickle.dumps(value)
                size_bytes = len(value_blob)
                
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT OR REPLACE INTO cache
                    (key, value, expires_at, category, size_bytes, created_at, access_count)
                    VALUES (?, ?, ?, ?, ?, ?, 1)
                """, (key, value_blob, expires_at.isoformat(), category, size_bytes, datetime.now().isoformat()))
                
                conn.commit()
                conn.close()
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")

    def clean_expired(self) -> int:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "DELETE FROM cache WHERE expires_at < ?",
                    (datetime.now().isoformat(),)
                )
                expired_count = cursor.rowcount
                
                cursor.execute("SELECT COUNT(*) FROM cache")
                total_records = cursor.fetchone()[0]
                
                if total_records > 1000:
                    cursor.execute("""
                        DELETE FROM cache WHERE key IN (
                            SELECT key FROM cache
                            ORDER BY access_count ASC, created_at ASC
                            LIMIT ?
                        )
                    """, (total_records // 10,))
                    
                    old_records = cursor.rowcount
                    logger.info(f"–£–¥–∞–ª–µ–Ω–æ {old_records} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
                
                cursor.execute("VACUUM")
                
                conn.commit()
                conn.close()
                
                self._update_cache_stats()
                return expired_count
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
            return 0

    def get_cache_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*), SUM(size_bytes) FROM cache")
                count, total_size = cursor.fetchone()
                
                cursor.execute("""
                    SELECT category, COUNT(*), SUM(size_bytes), AVG(access_count)
                    FROM cache GROUP BY category
                """)
                categories = cursor.fetchall()
                
                conn.close()
                
                return {
                    'total_records': count or 0,
                    'total_size_mb': round((total_size or 0) / (1024 * 1024), 2),
                    'categories': {cat: {'count': cnt, 'size_mb': round((size or 0) / (1024 * 1024), 2), 'avg_access': round(avg or 0, 1)} for cat, cnt, size, avg in categories},
                    'hit_rate': round(self.stats['hits'] / max(self.stats['hits'] + self.stats['misses'], 1) * 100, 1)
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ: {e}")
            return {'error': str(e)}

    def generate_key(self, *args) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–π –¥–ª—è –∫—ç—à–∞."""
        try:
            clean_args = []
            for arg in args:
                if isinstance(arg, (dict, list)):
                    s = json.dumps(arg, sort_keys=True)
                elif arg is None:
                    s = 'None'
                else:
                    s = str(arg)
                clean_args.append(s.strip()[:200])
            
            combined = "|".join(clean_args)
            return hashlib.md5(combined.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞ –∫—ç—à–∞: {e}")
            return hashlib.md5(f"error_{time.time()}_{str(args)}".encode()).hexdigest()

class YouTubeAnalyzer:
    def __init__(self, api_key: str, cache: CacheManager):
        try:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
            self.cache = cache
            self.api_key = api_key
            self.quota_used = 0
            logger.info("YouTube API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ YouTube API: {e}")
            raise

    def test_connection(self) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            self.youtube.i18nLanguages().list(part='snippet', hl='en').execute()
            logger.info("YouTube API —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ.")
            return True
        except HttpError as e:
            logger.error(f"–¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            details = e.error_details[0] if e.error_details else {}
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ YouTube: {e.resp.status} - {details.get('reason', 'Unknown')}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à API –∫–ª—é—á.")
            return False
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API: {e}")
            return False

    def _make_api_request(self, request_func, *args, **kwargs):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –∫–≤–æ—Ç"""
        try:
            if self.quota_used > YOUTUBE_API_DAILY_QUOTA * 0.9:
                st.warning("‚ö†Ô∏è –ü—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ –ª–∏–º–∏—Ç—É YouTube API –∫–≤–æ—Ç—ã")
            
            response = retry_api_call(request_func)(*args, **kwargs).execute()
            self.quota_used += 1 
            return response
        
        except HttpError as e:
            if e.resp.status == 403:
                st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
            elif e.resp.status == 400:
                st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API")
            else:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ YouTube API: {e}")
            raise
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ API: {e}")
            raise

    def get_channel_stats(self, channel_ids: list):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤"""
        if not channel_ids:
            return {}
        
        unique_ids = list(set(filter(None, channel_ids)))
        if not unique_ids:
            return {}
            
        cache_key = self.cache.generate_key('channels', sorted(unique_ids))
        if cached_data := self.cache.get(cache_key):
            return cached_data
        
        channel_stats = {}
        try:
            for i in range(0, len(unique_ids), 50):
                chunk_ids = unique_ids[i:i+50]
                
                request = self.youtube.channels().list(
                    part="statistics,snippet,brandingSettings",
                    id=",".join(chunk_ids)
                )
                response = self._make_api_request(lambda: request)
                self.quota_used += 1

                for item in response.get('items', []):
                    stats = item.get('statistics', {})
                    snippet = item.get('snippet', {})
                    branding = item.get('brandingSettings', {}).get('channel', {})
                    
                    channel_stats[item['id']] = {
                        'subscribers': safe_int_conversion(stats.get('subscriberCount', 0)),
                        'total_views': safe_int_conversion(stats.get('viewCount', 0)),
                        'video_count': safe_int_conversion(stats.get('videoCount', 0)),
                        'title': clean_text(snippet.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
                        'description': clean_text(snippet.get('description', ''))[:500],
                        'published_at': snippet.get('publishedAt', ''),
                        'country': snippet.get('country', ''),
                        'verified': 'verified' in str(snippet.get('thumbnails', {})),
                        'keywords': branding.get('keywords', '').split(',')[:10] if branding.get('keywords') else []
                    }
                
                if i + 50 < len(unique_ids):
                    time.sleep(REQUEST_DELAY)
            
            self.cache.set(cache_key, channel_stats, 'channels')
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {len(channel_stats)} –∫–∞–Ω–∞–ª–æ–≤")
            return channel_stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤: {e}")
            return {}

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        
        if not validate_keyword(keyword):
            st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            return None
        
        if max_results > 500:
            max_results = 500
            st.warning("‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 500")
        
        cache_key = self.cache.generate_key('search_v5', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_snippets = []
            next_page_token = None
            search_params = {
                'q': clean_text(keyword),
                'part': 'snippet',
                'type': 'video',
                'order': 'relevance',
            }
            
            if published_after:
                search_params['publishedAfter'] = published_after

            progress_bar = st.progress(0)
            status_text = st.empty()
            
            fetched_count = 0
            while fetched_count < max_results:
                search_params['maxResults'] = min(50, max_results - fetched_count)
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                status_text.text(f"üîç –ò—â–µ–º –≤–∏–¥–µ–æ: {fetched_count}/{max_results}")
                progress_bar.progress(fetched_count / max_results)
                
                request = self.youtube.search().list(**search_params)
                search_response = self._make_api_request(lambda: request)
                self.quota_used += 99 
                new_items = search_response.get('items', [])
                
                if not new_items:
                    logger.warning(f"–ü–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{keyword}' –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
                    break
                    
                video_snippets.extend(new_items)
                fetched_count = len(video_snippets)
                next_page_token = search_response.get('nextPageToken')
                
                if not next_page_token:
                    break
                
                time.sleep(REQUEST_DELAY)

            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(video_snippets)} –≤–∏–¥–µ–æ. –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª–∏...")
            
            if not video_snippets:
                return []

            video_ids = [item['id']['videoId'] for item in video_snippets if 'videoId' in item.get('id', {})]
            channel_ids = list(set([item['snippet']['channelId'] for item in video_snippets]))

            status_text.text("üìä –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤...")
            channel_stats = self.get_channel_stats(channel_ids)
            
            videos = []
            
            all_video_details = []
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                status_text.text(f"üìä –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –≤–∏–¥–µ–æ ({i+len(chunk_ids)}/{len(video_ids)})...")
                
                request = self.youtube.videos().list(
                    part='statistics,contentDetails,snippet,topicDetails',
                    id=','.join(chunk_ids)
                )
                stats_response = self._make_api_request(lambda: request)
                self.quota_used += 1
                all_video_details.extend(stats_response.get('items', []))
                
                if i + 50 < len(video_ids):
                    time.sleep(REQUEST_DELAY)

            video_details_map = {item['id']: item for item in all_video_details}
            
            for snippet_item in video_snippets:
                video_id = snippet_item['id'].get('videoId')
                if not video_id or video_id not in video_details_map:
                    continue
                
                details = video_details_map[video_id]
                stats = details.get('statistics', {})
                content_details = details.get('contentDetails', {})
                video_snippet = details.get('snippet', {})
                topic_details = details.get('topicDetails', {})
                
                duration = self._parse_duration(content_details.get('duration', 'PT0S'))
                channel_id = video_snippet.get('channelId')
                channel_info = channel_stats.get(channel_id, {})
                
                category_id = safe_int_conversion(video_snippet.get('categoryId', 0))
                tags = video_snippet.get('tags', [])[:20]
                
                video_data = {
                    'video_id': video_id,
                    'title': clean_text(video_snippet.get('title', '')),
                    'channel': clean_text(video_snippet.get('channelTitle', '')),
                    'channel_id': channel_id,
                    'subscribers': channel_info.get('subscribers', 0),
                    'subscribers_formatted': safe_format_number(channel_info.get('subscribers', 0)),
                    'channel_total_views': channel_info.get('total_views', 0),
                    'channel_video_count': channel_info.get('video_count', 0),
                    'channel_verified': channel_info.get('verified', False),
                    'published': video_snippet.get('publishedAt', ''),
                    'views': safe_int_conversion(stats.get('viewCount', 0)),
                    'views_formatted': safe_format_number(safe_int_conversion(stats.get('viewCount', 0))),
                    'likes': safe_int_conversion(stats.get('likeCount', 0)),
                    'likes_formatted': safe_format_number(safe_int_conversion(stats.get('likeCount', 0))),
                    'comments': safe_int_conversion(stats.get('commentCount', 0)),
                    'duration': duration,
                    'duration_formatted': self._format_duration(duration),
                    'is_short': duration <= 1.05,
                    'short_indicator': "ü©≥ Shorts" if duration <= 1.05 else "üìπ –í–∏–¥–µ–æ",
                    'tags': tags,
                    'description': clean_text(video_snippet.get('description', ''))[:1000],
                    'definition': content_details.get('definition', 'sd').upper(),
                    'category_id': category_id,
                    'language': video_snippet.get('defaultLanguage', 'ru'),
                    'topics': topic_details.get('topicCategories', [])[:5],
                    'thumbnail': snippet_item['snippet'].get('thumbnails', {}).get('medium', {}).get('url', ''),
                    'video_url': f"https://www.youtube.com/watch?v={video_id}"
                }
                videos.append(video_data)
            
            progress_bar.empty()
            status_text.empty()
            
            self.cache.set(cache_key, videos, 'search')
            logger.info(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ –¥–ª—è '{keyword}'")
            return videos
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ search_videos –¥–ª—è '{keyword}': {e}", exc_info=True)
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {e}")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not duration_str:
            return 0
            
        try:
            match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
            if not match:
                return 0
                
            h, m, s = (safe_int_conversion(g) for g in match.groups())
            return h * 60 + m + s / 60
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ '{duration_str}': {e}")
            return 0
    
    def _format_duration(self, duration_minutes: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        try:
            if duration_minutes is None:
                return "0:00"
            duration_minutes = float(duration_minutes)
            if duration_minutes < 1:
                return f"0:{int(duration_minutes * 60):02d}"
            
            total_seconds = int(duration_minutes * 60)
            hours = total_seconds // 3600
            minutes = (total_seconds % 3600) // 60
            seconds = total_seconds % 60
            
            if hours > 0:
                return f"{hours}:{minutes:02d}:{seconds:02d}"
            else:
                return f"{minutes}:{seconds:02d}"
        except Exception:
            return "0:00"
    
    def analyze_competition(self, videos: list):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        if not videos:
            return {}, pd.DataFrame()
        
        try:
            df = pd.DataFrame(videos)
            
            df['published'] = pd.to_datetime(df['published'], errors='coerce', utc=True).dt.tz_localize(None)
            df = df.dropna(subset=['published', 'views'])
            
            if df.empty:
                logger.warning("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö DataFrame –ø—É—Å—Ç")
                return {}, pd.DataFrame()
            
            df['views'] = df['views'].apply(lambda x: max(safe_int_conversion(x, 1), 1))
            df['days_ago'] = (datetime.now() - df['published']).dt.days.fillna(0)
            df['engagement_rate'] = np.where(df['views'] > 0, ((df['likes'] + df['comments']) / df['views']) * 100, 0)
            df['views_per_subscriber'] = np.where(df['subscribers'] > 0, df['views'] / df['subscribers'], 0)
            
            view_quartiles = df['views'].quantile([0.25, 0.5, 0.75, 0.9])
            
            analysis = {
                'total_videos': len(df),
                'avg_views': safe_float_conversion(df['views'].mean()),
                'median_views': safe_float_conversion(df['views'].median()),
                'top_10_avg_views': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['views'].mean()),
                'top_25_percent_views': safe_float_conversion(view_quartiles[0.75]),
                'engagement_rate': safe_float_conversion(df['engagement_rate'].mean()),
                'videos_last_week': len(df[df['days_ago'] <= 7]),
                'videos_last_month': len(df[df['days_ago'] <= 30]),
                'shorts_percentage': safe_float_conversion(df['is_short'].mean() * 100),
                'avg_days_to_top_10': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['days_ago'].mean()),
                'unique_channels': df['channel'].nunique(),
                'avg_channel_subscribers': safe_float_conversion(df.drop_duplicates(subset=['channel_id'])['subscribers'].mean()),
                'avg_duration': safe_float_conversion(df[~df['is_short']]['duration'].mean()),
                'hd_percentage': safe_float_conversion((df['definition'] == 'HD').mean() * 100),
                'verified_channels_count': safe_int_conversion(df['channel_verified'].sum()),
                'avg_likes_per_view': safe_float_conversion(np.where(df['views'] > 0, (df['likes'] / df['views']), 0).mean() * 100),
                'avg_comments_per_view': safe_float_conversion(np.where(df['views'] > 0, (df['comments'] / df['views']), 0).mean() * 100)
            }

            score = 0
            
            if analysis['top_10_avg_views'] < 20000: score += 4
            elif analysis['top_10_avg_views'] < 50000: score += 3
            elif analysis['top_10_avg_views'] < 200000: score += 2
            elif analysis['top_10_avg_views'] < 500000: score += 1
            
            if analysis['videos_last_week'] < 2: score += 3
            elif analysis['videos_last_week'] < 5: score += 2
            elif analysis['videos_last_week'] < 15: score += 1
            
            if analysis['unique_channels'] < 15: score += 2
            elif analysis['unique_channels'] < 30: score += 1
            
            if analysis['engagement_rate'] < 1.5: score += 2
            elif analysis['engagement_rate'] < 3: score += 1
            
            competition_levels = {
                0: '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥', 1: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥', 2: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
                3: '–í—ã—Å–æ–∫–∞—è üü†', 4: '–í—ã—Å–æ–∫–∞—è üü†', 5: '–°—Ä–µ–¥–Ω—è—è üü°', 6: '–°—Ä–µ–¥–Ω—è—è üü°',
                7: '–ù–∏–∑–∫–∞—è üü¢', 8: '–ù–∏–∑–∫–∞—è üü¢', 9: '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢', 10: '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è üü¢'
            }
            
            analysis['competition_level'] = competition_levels.get(score, '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥')
            analysis['competition_score'] = score
            analysis['opportunity_rating'] = min(score * 10, 100)
            
            return analysis, df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}", exc_info=True)
            return {}, pd.DataFrame()

class AdvancedTrendsAnalyzer:
    def __init__(self, cache: CacheManager):
        self.cache = cache
        
    def _get_pytrends(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ pytrends —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return TrendReq(hl='ru-RU', tz=180, timeout=(10, 25), retries=2, backoff_factor=0.1)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Trends: {e}")
            return None

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
            
        pytrends = self._get_pytrends()
        if not pytrends:
            return None
            
        try:
            pytrends.build_payload([keyword], timeframe='today 12-m', geo='')
            interest_12m = pytrends.interest_over_time()
            
            if interest_12m.empty or keyword not in interest_12m.columns:
                 st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è '{keyword}'.")
                 return None
            
            try:
                pytrends.build_payload([keyword], timeframe='today 5-y', geo='')
                interest_5y = pytrends.interest_over_time()
            except:
                interest_5y = pd.DataFrame()
            
            try:
                related_queries = pytrends.related_queries()
                rising_queries = related_queries.get(keyword, {}).get('rising', pd.DataFrame())
                top_queries = related_queries.get(keyword, {}).get('top', pd.DataFrame())
            except:
                rising_queries = pd.DataFrame()
                top_queries = pd.DataFrame()
            
            series = interest_12m[keyword]
            recent_avg = series.tail(4).mean()
            previous_avg = series.iloc[-8:-4].mean()
            overall_avg = series.mean()
            
            if recent_avg > previous_avg * 1.2: trend_direction = "–ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—É—â–∏–π üöÄ"
            elif recent_avg > previous_avg * 1.1: trend_direction = "–†–∞—Å—Ç—É—â–∏–π üìà"
            elif recent_avg < previous_avg * 0.8: trend_direction = "–ü–∞–¥–∞—é—â–∏–π üìâ"
            elif recent_avg < previous_avg * 0.9: trend_direction = "–°–ª–∞–±–æ –ø–∞–¥–∞—é—â–∏–π üìâ"
            else: trend_direction = "–°—Ç–∞–±–∏–ª—å–Ω—ã–π ‚û°Ô∏è"
            
            monthly_avg = series.groupby(series.index.month).mean()
            peak_months = monthly_avg.nlargest(3).index.tolist()
            
            result = {
                'interest_df': interest_12m,
                'interest_5y_df': interest_5y,
                'trend_direction': trend_direction,
                'recent_avg': recent_avg,
                'overall_avg': overall_avg,
                'trend_strength': abs(recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0,
                'rising_queries': rising_queries,
                'top_queries': top_queries,
                'peak_months': peak_months,
                'current_interest': series.iloc[-1] if not series.empty else 0
            }
            
            self.cache.set(cache_key, result, 'trends')
            return result
            
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

# --- 4. –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–ï–ì–û–í ---

@dataclass
class TagScore:
    keyword: str
    search_volume: int
    competition_score: int
    seo_score: int
    overall_score: int
    difficulty: str

class YouTubeTagAnalyzer:
    def __init__(self, serpapi_key: str = None, cache: CacheManager = None):
        self.serpapi_key = serpapi_key
        self.use_serpapi = bool(serpapi_key)
        self.cache = cache
        self.base_serpapi = "https://serpapi.com/search"
        
    def get_search_volume_serpapi(self, keyword: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._estimate_search_volume_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_volume', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key): return cached_data
        
        try:
            params = {'api_key': self.serpapi_key, 'engine': 'youtube', 'search_query': keyword}
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            search_info = data.get('search_information', {})
            if 'total_results' in search_info: volume = safe_int_conversion(search_info['total_results'])
            elif 'video_results' in data: volume = len(data['video_results']) * 150
            else: volume = self._estimate_search_volume_basic(keyword)

            if cache_key and self.cache: self.cache.set(cache_key, volume, 'serpapi')
            return volume
                
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ SerpAPI –¥–ª—è '{keyword}': {e}")
        return self._estimate_search_volume_basic(keyword)
    
    def _estimate_search_volume_basic(self, keyword: str) -> int:
        """–ë–∞–∑–æ–≤–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API"""
        word_count = len(keyword.split())
        char_count = len(keyword)
        base_volume = max(1000, 5000 - (word_count * 500) - (char_count * 10))
        popular_words = {'–∫–∞–∫', '—á—Ç–æ', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–æ–±–∑–æ—Ä', '—É—Ä–æ–∫', '—Ç—É—Ç–æ—Ä–∏–∞–ª', 'guide', 'tutorial', 'how', 'what', 'review', 'tips'}
        bonus = sum(300 for word in keyword.lower().split() if word in popular_words)
        return min(base_volume + bonus, 50000)
    
    def analyze_competition_serpapi(self, keyword: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._analyze_competition_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_competition', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key): return cached_data
        
        try:
            params = {'api_key': self.serpapi_key, 'engine': 'youtube', 'search_query': keyword}
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if 'video_results' not in data: return self._analyze_competition_basic(keyword)
            
            videos = data['video_results'][:20]
            analysis = self._process_competition_data(videos, keyword)
            
            if cache_key and self.cache: self.cache.set(cache_key, analysis, 'serpapi')
            return analysis
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
            return self._analyze_competition_basic(keyword)
    
    def _analyze_competition_basic(self, keyword: str) -> dict:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API)"""
        word_count = len(keyword.split())
        if word_count == 1: competition_level, optimized_ratio = "High", 0.8
        elif word_count == 2: competition_level, optimized_ratio = "Medium", 0.6
        else: competition_level, optimized_ratio = "Low", 0.4
        
        return {
            'total_videos': 20, 'optimized_titles': int(20 * optimized_ratio),
            'high_view_videos': max(1, int(20 * (1 - optimized_ratio))),
            'verified_channels': max(1, int(20 * 0.3)), 'avg_views': 15000 if word_count == 1 else 8000,
            'keyword_in_title': int(20 * optimized_ratio), 'recent_videos': 3,
            'competition_level': competition_level
        }
    
    def _process_competition_data(self, videos: list, keyword: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        analysis = {'total_videos': len(videos), 'optimized_titles': 0, 'high_view_videos': 0, 'verified_channels': 0, 'avg_views': 0, 'keyword_in_title': 0, 'recent_videos': 0}
        total_views = 0
        keyword_lower = keyword.lower()
        
        for video in videos:
            title = video.get('title', '').lower()
            if keyword_lower in title:
                analysis['optimized_titles'] += 1
                analysis['keyword_in_title'] += 1
            
            views = self._extract_views(video.get('view_count_text', '0'))
            total_views += views
            if views > 100000: analysis['high_view_videos'] += 1
            if video.get('channel', {}).get('verified', False): analysis['verified_channels'] += 1
            if self._is_recent(video.get('published_date', '')): analysis['recent_videos'] += 1
        
        if analysis['total_videos'] > 0: analysis['avg_views'] = total_views // analysis['total_videos']
        return analysis
    
    def _extract_views(self, views_str: str) -> int:
        """–ù–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞."""
        if not views_str or not isinstance(views_str, str): return 0
        views_str = views_str.lower().replace(',', '').replace(' views', '').strip()
        num_part = re.match(r'[\d.]+', views_str)
        if not num_part: return 0
        num = float(num_part.group(0))
        if 'k' in views_str: return int(num * 1000)
        if 'm' in views_str: return int(num * 1000000)
        if 'b' in views_str: return int(num * 1000000000)
        return int(num)
    
    def _is_recent(self, date_str: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –≤–∏–¥–µ–æ"""
        if not date_str: return False
        recent_indicators = ['day', 'days', 'week', 'weeks', 'hour', 'hours', '—á–∞—Å', '–¥–µ–Ω—å', '–Ω–µ–¥–µ–ª']
        return any(indicator in date_str.lower() for indicator in recent_indicators)
    
    def calculate_scores(self, keyword: str, analysis: dict, search_volume: int) -> TagScore:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ç–µ–≥–∞"""
        total = analysis.get('total_videos', 0)
        if total == 0:
            competition_score = 50
        else:
            optimized_ratio = analysis.get('optimized_titles', 0) / total
            high_views_ratio = analysis.get('high_view_videos', 0) / total
            verified_ratio = analysis.get('verified_channels', 0) / total
            avg_views_factor = min(analysis.get('avg_views', 0) / 500000, 1.0)
            competition_score = min(int((optimized_ratio * 0.3 + high_views_ratio * 0.25 + verified_ratio * 0.2 + avg_views_factor * 0.25) * 100), 100)
        
        if total > 0:
            keyword_optimization = analysis.get('keyword_in_title', 0) / total
            seo_score = max(int((1.0 - keyword_optimization) * 100), 10)
        else:
            seo_score = 50
        
        volume_score = min(math.log10(max(search_volume, 1)) * 20, 100)
        competition_inverted = 100 - competition_score
        overall_score = min(int(volume_score * 0.4 + competition_inverted * 0.35 + seo_score * 0.25), 100)
        
        if competition_score <= 20: difficulty = "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 40: difficulty = "–ù–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 60: difficulty = "–°—Ä–µ–¥–Ω—è—è üü°"
        elif competition_score <= 80: difficulty = "–í—ã—Å–æ–∫–∞—è üü†"
        else: difficulty = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥"
        
        return TagScore(keyword=keyword, search_volume=search_volume, competition_score=competition_score, seo_score=seo_score, overall_score=overall_score, difficulty=difficulty)
    
    def analyze_keyword(self, keyword: str) -> TagScore:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        search_volume = self.get_search_volume_serpapi(keyword)
        competition_analysis = self.analyze_competition_serpapi(keyword)
        return self.calculate_scores(keyword, competition_analysis, search_volume)
    
    def analyze_multiple_keywords(self, keywords: list) -> list:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        results = []
        if not keywords: return []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, keyword in enumerate(keywords):
            try:
                status_text.text(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–≥: {keyword} ({i+1}/{len(keywords)})")
                progress_bar.progress((i + 1) / len(keywords))
                result = self.analyze_keyword(keyword)
                results.append(result)
                if self.use_serpapi: time.sleep(0.7)
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ '{keyword}': {e}")
                continue
        
        progress_bar.empty()
        status_text.empty()
        return sorted(results, key=lambda x: x.overall_score, reverse=True)

class ContentStrategist:
    def __init__(self, openai_key=None, openai_model=None):
        self.use_openai = bool(openai_key and openai_model)
        if self.use_openai:
            try:
                self.client = openai.OpenAI(api_key=openai_key)
                self.model = openai_model
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
                self.use_openai = False

    def get_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame, cache: CacheManager):
        if not comp_analysis: return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."
        cache_key = None
        if self.use_openai:
            cache_key = cache.generate_key('openai_v4', keyword, self.model, comp_analysis, trends_data)
            if cached_strategy := cache.get(cache_key):
                st.toast("ü§ñ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞!", icon="üß†")
                return cached_strategy
        
        if self.use_openai: strategy = self._get_ai_strategy(keyword, comp_analysis, trends_data, df)
        else: strategy = self._get_rule_based_strategy(keyword, comp_analysis, df)
        
        if self.use_openai and cache_key and "–û—à–∏–±–∫–∞" not in strategy:
            cache.set(cache_key, strategy, 'openai')
        return strategy

    def _get_rule_based_strategy(self, keyword: str, comp_analysis: dict, df: pd.DataFrame):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–µ–∑ AI"""
        if not df.empty:
            titles = df['title'].tolist()
            popular_words = extract_keywords_from_titles(titles)
            top_words = [word for word, count in popular_words[:5]]
        else:
            top_words = []
        
        competition_level = comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        avg_views = comp_analysis.get('avg_views', 0)
        shorts_percentage = comp_analysis.get('shorts_percentage', 0)
        strategy_parts = []
        
        if '–Ω–∏–∑–∫–∞—è' in competition_level.lower(): verdict = "üéØ **–û–¢–õ–ò–ß–ù–ê–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–¨!** –ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —à–∞–Ω—Å—ã –¥–ª—è —Ä–æ—Å—Ç–∞."
        elif '—Å—Ä–µ–¥–Ω—è—è' in competition_level.lower(): verdict = "‚ö° **–•–û–†–û–®–ò–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–´** —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º. –ù—É–∂–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è."
        else: verdict = "üî• **–í–´–°–û–ö–ê–Ø –ö–û–ù–ö–£–†–ï–ù–¶–ò–Ø** - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
        strategy_parts.append(f"### üéØ –í–µ—Ä–¥–∏–∫—Ç\n{verdict}")
        
        insights = []
        if avg_views < 50000: insights.append("–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ–≤—ã—Å–æ–∫–∏–µ - –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–æ–º")
        if shorts_percentage > 50: insights.append("–ú–Ω–æ–≥–æ Shorts –≤ –Ω–∏—à–µ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç")
        if top_words: insights.append(f"–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {', '.join(top_words[:3])}")
        if insights: strategy_parts.append("### üîç –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã\n- " + "\n- ".join(insights))

        content_ideas = [
            f"**–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword}** - –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
            f"**–¢–æ–ø-5 –æ—à–∏–±–æ–∫ –≤ {keyword}** - —Ä–∞–∑–±–æ—Ä —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º",
            f"**{keyword}: –¥–æ –∏ –ø–æ—Å–ª–µ** - –∫–µ–π—Å—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
            f"**–ö–∞–∫ –Ω–∞—á–∞—Ç—å –≤ {keyword} –±–µ–∑ –æ–ø—ã—Ç–∞** - –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω",
            f"**–°–µ–∫—Ä–µ—Ç—ã {keyword}, –æ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –≥–æ–≤–æ—Ä—è—Ç** - –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        ]
        if shorts_percentage > 30:
            content_ideas.extend([f"**{keyword} –∑–∞ 60 —Å–µ–∫—É–Ω–¥** - –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±—É—á–∞—é—â–∏–µ –≤–∏–¥–µ–æ", f"**–ë—ã—Å—Ç—Ä—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ {keyword}** - —Å–µ—Ä–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–æ–ª–∏–∫–æ–≤"])
        strategy_parts.append("### üí° –ò–¥–µ–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n- " + "\n- ".join(content_ideas))
        
        optimization_tips = [
            "üé® **–Ø—Ä–∫–∏–µ –ø—Ä–µ–≤—å—é** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Ü–≤–µ—Ç–∞ –∏ —á–µ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
            "‚è∞ **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏** - —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ 18:00-21:00 –ø–æ –ú–°–ö",
            "üéØ **–¶–µ–ø–ª—è—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–∏—Å–ª–∞, –≤–æ–ø—Ä–æ—Å—ã, –∏–Ω—Ç—Ä–∏–≥—É",
            "üìù **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è** - –¥–æ–±–∞–≤—å—Ç–µ —Ç–∞–π–º-–∫–æ–¥—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏",
            "üè∑Ô∏è **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–≥–∏** - –º–∏–∫—Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏ –Ω–∏—à–µ–≤—ã—Ö —Ç–µ–≥–æ–≤"
        ]
        strategy_parts.append("### üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n- " + "\n- ".join(optimization_tips))
        return "\n\n".join(strategy_parts)
    
    def _get_ai_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame):
        st.toast("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ OpenAI...", icon="üß†")
        
        top_titles, top_channels = [], []
        if not df.empty:
            top_videos = df.nlargest(10, 'views')
            top_titles = top_videos['title'].tolist()
            top_channels = top_videos['channel'].value_counts().head(5).to_dict()
        
        trends_info = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        if trends_data:
            trends_info = f"{trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
            if 'recent_avg' in trends_data: trends_info += f" (—Ç–µ–∫—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å: {trends_data['recent_avg']:.0f})"
        
        prompt = f"""
        –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π YouTube-—Å—Ç—Ä–∞—Ç–µ–≥ —Å –æ–ø—ã—Ç–æ–º –±–æ–ª–µ–µ 10 –ª–µ—Ç. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∏—à–∏ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è.

        **–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–ê–Ø –¢–ï–ú–ê:** "{keyword}"

        **–î–ê–ù–ù–´–ï –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:**
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ: {comp_analysis.get('total_videos', 0)}
        - –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
        - –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('avg_views', 0)):,}
        - –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('median_views', 0)):,}
        - –ü—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–ø-10: {int(comp_analysis.get('top_10_avg_views', 0)):,}
        - –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: {comp_analysis.get('engagement_rate', 0):.2f}%
        - –ü—Ä–æ—Ü–µ–Ω—Ç Shorts: {comp_analysis.get('shorts_percentage', 0):.1f}%
        - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {comp_analysis.get('unique_channels', 0)}
        - –í–∏–¥–µ–æ –∑–∞ –Ω–µ–¥–µ–ª—é: {comp_analysis.get('videos_last_week', 0)}

        **–¢–†–ï–ù–î–´ GOOGLE:**
        {trends_info}

        **–¢–û–ü-5 –ó–ê–ì–û–õ–û–í–ö–û–í –ö–û–ù–ö–£–†–ï–ù–¢–û–í:**
        {chr(10).join(f"‚Ä¢ {title}" for title in top_titles[:5])}

        **–í–ï–î–£–©–ò–ï –ö–ê–ù–ê–õ–´:**
        {chr(10).join(f"‚Ä¢ {channel}: {count} –≤–∏–¥–µ–æ" for channel, count in list(top_channels.items())[:3])}

        **–ó–ê–î–ê–ù–ò–ï:**
        –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:

        1. **üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–¥–∏–∫—Ç** - –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ –Ω–∏—à–∏ –∏ –≥–ª–∞–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
        2. **üìä –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π** - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω –Ω–∏—à–∏
        3. **üé¨ –ö–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è** - 7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π –¥–ª—è –≤–∏–¥–µ–æ —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ (–ó–∞–≥–æ–ª–æ–≤–æ–∫, –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –§–æ—Ä–º–∞—Ç, –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
        4. **üöÄ –¢–∞–∫—Ç–∏–∫–∞ —Ä–æ—Å—Ç–∞** - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 30 –¥–Ω–µ–π
        5. **üí∞ –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è** - 3 —Å–ø–æ—Å–æ–±–∞ –∑–∞—Ä–∞–±–æ—Ç–∫–∞ + –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏
        6. **üè∑Ô∏è SEO –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ–≥–∞–º, –ø—Ä–µ–≤—å—é, –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

        –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ actionable —Å–æ–≤–µ—Ç–∞—Ö.
        """

        try:
            response = self.client.chat.completions.create(model=self.model, messages=[{"role": "user", "content": prompt}], temperature=0.7, max_tokens=2000)
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI: {e}", exc_info=True)
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

# --- 5. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ---

def main():
    st.markdown('<h1 class="main-header">YouTube AI Strategist üß†</h1>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.subheader("üîë YouTube API")
        youtube_api_key = st.text_input("YouTube API Key", type="password", help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –≤ Google Cloud Console", key="youtube_api_key")
        
        if youtube_api_key:
            if validate_youtube_api_key(youtube_api_key): st.success("‚úÖ YouTube API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else: st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
        
        st.markdown("---")
        st.subheader("ü§ñ AI-—Å—Ç—Ä–∞—Ç–µ–≥")
        use_openai = st.toggle("–í–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑ (OpenAI)", value=True, key="use_openai")
        openai_api_key, openai_model = "", "gpt-4o-mini"
        if use_openai:
            openai_api_key = st.text_input("OpenAI API Key", type="password", help="–ö–ª—é—á –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å—Ç—Ä–∞—Ç–µ–≥–∏–π", key="openai_api_key")
            if openai_api_key:
                if validate_openai_api_key(openai_api_key): st.success("‚úÖ OpenAI API –∫–ª—é—á –≤–∞–ª–∏–¥–µ–Ω")
                else: st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π OpenAI API –∫–ª—é—á")
            openai_model = st.selectbox("–ú–æ–¥–µ–ª—å OpenAI", ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"], index=1, help="gpt-4o-mini - –±—ã—Å—Ç—Ä–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ", key="openai_model")
        
        st.markdown("---")
        st.subheader("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤")
        use_serpapi = st.toggle("–í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (SerpAPI)", value=False, key="use_serpapi")
        serpapi_key = ""
        if use_serpapi:
            serpapi_key = st.text_input("SerpAPI Key", type="password", help="–ö–ª—é—á –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤", key="serpapi_key")
            if serpapi_key:
                if validate_serpapi_key(serpapi_key): st.success("‚úÖ SerpAPI –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                else: st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
        
        st.markdown("---")
        st.subheader("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 20, 200, 100, 10, key="max_results")
        date_range_options = {"–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None, "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365, "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180, "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90, "–ó–∞ –º–µ—Å—è—Ü": 30}
        selected_date_range = st.selectbox("–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:", list(date_range_options.keys()), index=1, key="date_range")
        days_limit = date_range_options[selected_date_range]
        
        if not youtube_api_key:
            st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
            st.stop()
        
        cache = CacheManager()
        st.markdown("---")
        st.subheader("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
        cache_info = cache.get_cache_info()
        if 'error' not in cache_info:
            st.info(f"–ó–∞–ø–∏—Å–µ–π: {cache_info.get('total_records', 0)}, –†–∞–∑–º–µ—Ä: {cache_info.get('total_size_mb', 0)} MB, Hit Rate: {cache_info.get('hit_rate', 0)}%")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–π"):
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {cache.clean_expired()} –∑–∞–ø–∏—Å–µ–π")
                st.rerun()
        with col2:
            if st.button("üí• –û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"):
                if cache.db_path.exists(): cache.db_path.unlink(missing_ok=True)
                st.success("–ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω"); st.rerun()
        
        st.markdown("---")
        st.info("–ê–≤—Ç–æ—Ä: [Telegram](https://t.me/i_gma)")

    keyword = st.text_input("üéØ –í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö...", key="keyword_input")
    
    col1, col2, col3 = st.columns(3)
    examples = ["python –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö", "–º–æ–Ω—Ç–∞–∂ –≤–∏–¥–µ–æ", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –∞–∫—Ü–∏–∏"]
    if col1.button(f"üìå {examples[0]}", use_container_width=True): st.session_state.keyword_input = examples[0]; st.rerun()
    if col2.button(f"üìå {examples[1]}", use_container_width=True): st.session_state.keyword_input = examples[1]; st.rerun()
    if col3.button(f"üìå {examples[2]}", use_container_width=True): st.session_state.keyword_input = examples[2]; st.rerun()
            
    if st.button("üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!", type="primary", use_container_width=True, disabled=not keyword):
        try:
            analyzer = YouTubeAnalyzer(youtube_api_key, cache)
            if not analyzer.test_connection(): st.stop()
            
            spinner_text = "üåä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é YouTube..."
            if use_openai and openai_api_key and validate_openai_api_key(openai_api_key): spinner_text += " –ü—Ä–∏–≤–ª–µ–∫–∞—é AI..."

            with st.spinner(spinner_text):
                published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z" if days_limit else None
                videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
                
                if not videos:
                    st.warning(f"üîç –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{keyword}'. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞.")
                    st.stop()
                
                comp_analysis, df = analyzer.analyze_competition(videos)
                trends_analyzer = AdvancedTrendsAnalyzer(cache)
                trends_data = trends_analyzer.analyze_keyword_trends(keyword)
                
                strategist = ContentStrategist(openai_api_key if use_openai and validate_openai_api_key(openai_api_key) else None, openai_model if use_openai else None)
                strategy_output = strategist.get_strategy(keyword, comp_analysis, trends_data, df, cache)

            st.markdown(f"# üìä –ê–Ω–∞–ª–∏–∑ –Ω–∏—à–∏: **{keyword}**")
            
            cols = st.columns(5)
            cols[0].metric("üìπ –í–∏–¥–µ–æ", f"{len(df)}")
            cols[1].metric("üèÜ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", comp_analysis.get('competition_level', 'N/A').split()[0])
            cols[2].metric("üëÄ –°—Ä–µ–¥. –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", safe_format_number(int(comp_analysis.get('avg_views', 0))))
            cols[3].metric("üí¨ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", f"{comp_analysis.get('engagement_rate', 0):.1f}%")
            cols[4].metric("üì∫ –ö–∞–Ω–∞–ª–æ–≤", comp_analysis.get('unique_channels', 0))

            tab1, tab2, tab3, tab4, tab5 = st.tabs(["üéØ AI –°–æ–≤–µ—Ç—ã", "üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤", "üìà –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å", "üèÜ –¢–æ–ø –≤–∏–¥–µ–æ", "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"])

            with tab1:
                css_class = "openai-result" if strategist.use_openai else "custom-container"
                st.markdown(f'<div class="{css_class}">{strategy_output}</div>', unsafe_allow_html=True)
                
            with tab2:
                all_tags = [tag.lower() for v in videos if v.get('tags') for tag in v['tags']]
                if all_tags:
                    tag_popularity = Counter(all_tags)
                    st.markdown("#### –í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                    selected_tags = st.multiselect("–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏:", [t for t, c in tag_popularity.most_common(20)], default=[t for t,c in tag_popularity.most_common(5)])
                    custom_tags = st.text_input("–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é:")
                    if custom_tags: selected_tags.extend([t.strip().lower() for t in custom_tags.split(',') if t.strip()])
                    
                    if st.button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏", type="secondary"):
                        unique_tags_to_analyze = list(set(selected_tags))[:20]
                        tag_analyzer = YouTubeTagAnalyzer(serpapi_key if use_serpapi and validate_serpapi_key(serpapi_key) else None, cache)
                        with st.spinner(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(unique_tags_to_analyze)} —Ç–µ–≥–æ–≤..."):
                            tag_results = tag_analyzer.analyze_multiple_keywords(unique_tags_to_analyze)
                        if tag_results:
                            results_df = pd.DataFrame([vars(r) for r in tag_results])
                            st.dataframe(results_df[['keyword', 'search_volume', 'competition_score', 'seo_score', 'overall_score', 'difficulty']].rename(columns={'keyword':'–¢–µ–≥','search_volume':'–û–±—ä–µ–º','competition_score':'–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è','seo_score':'SEO','overall_score':'–û—Ü–µ–Ω–∫–∞','difficulty':'–°–ª–æ–∂–Ω–æ—Å—Ç—å'}), hide_index=True)
                else:
                    st.warning("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.")

            with tab3:
                if trends_data and 'interest_df' in trends_data and not trends_data['interest_df'].empty:
                    fig = px.line(trends_data['interest_df'], x=trends_data['interest_df'].index, y=keyword, title=f'–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º—ã: "{keyword}"')
                    fig.update_layout(template='plotly_dark')
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("üìà –î–∞–Ω–Ω—ã–µ Google Trends –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")

            with tab4:
                st.markdown("### üèÜ –¢–æ–ø –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º")
                if not df.empty:
                    for _, video in df.nlargest(10, 'views').iterrows():
                        with st.container(border=True):
                            col1, col2 = st.columns([1, 4])
                            with col1: st.image(video.get('thumbnail', ''))
                            # –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞—Ö
                            with col2: st.markdown(f"""
                                **[{video['title']}]({video['video_url']})**<br>
                                üì∫ **{video['channel']}** ({video['subscribers_formatted']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)<br>
                                üëÄ {video['views_formatted']} ‚Ä¢ üëç {video['likes_formatted']} ‚Ä¢ ‚è±Ô∏è {video['duration_formatted']}
                                """, unsafe_allow_html=True)
            
            with tab5:
                st.markdown("### üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –≤–∏–¥–µ–æ")
                if not df.empty:
                    # –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'subscribers' –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω
                    display_df = df[['title', 'channel', 'subscribers', 'views', 'likes', 'duration_formatted', 'published']]
                    st.dataframe(display_df.rename(columns={
                        'title':'–ó–∞–≥–æ–ª–æ–≤–æ–∫',
                        'channel':'–ö–∞–Ω–∞–ª',
                        'subscribers': '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏',
                        'views':'–ü—Ä–æ—Å–º–æ—Ç—Ä—ã',
                        'likes':'–õ–∞–π–∫–∏',
                        'duration_formatted':'–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                        'published':'–î–∞—Ç–∞'
                    }), use_container_width=True, hide_index=True)

                    csv_data = df.to_csv(index=False).encode('utf-8')
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV)", csv_data, f'youtube_analysis_{keyword.replace(" ", "_")}.csv', 'text/csv')

        except Exception as e:
            st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}", exc_info=True)

if __name__ == "__main__":
    main()