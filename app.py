# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
import time
import logging
from pytrends.request import TrendReq
import openai
import numpy as np
from collections import Counter
import requests
import json
from dataclasses import dataclass
from urllib.parse import quote_plus
import concurrent.futures
import unicodedata

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('youtube_strategist.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è API –ª–∏–º–∏—Ç–æ–≤
YOUTUBE_API_DAILY_QUOTA = 10000
REQUEST_DELAY = 0.1
MAX_RETRIES = 3
REQUEST_TIMEOUT = 30

st.markdown("""
<style>
    .main-header { 
        font-size: 2.8rem; 
        color: #FF0000; 
        text-align: center; 
        margin-bottom: 2rem; 
        font-weight: bold; 
        background: linear-gradient(90deg, #FF0000 0%, #FF6B6B 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .stButton>button { 
        border-radius: 8px; 
        font-weight: bold; 
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .custom-container {
        background: linear-gradient(135deg, rgba(42, 57, 62, 0.5), rgba(62, 77, 82, 0.3));
        padding: 1.5rem; 
        border-radius: 15px;
        border-left: 5px solid #00a0dc; 
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .openai-result {
        background: linear-gradient(135deg, rgba(26, 142, 95, 0.1), rgba(46, 162, 115, 0.05));
        padding: 1.5rem; 
        border-radius: 15px;
        border-left: 5px solid #1a8e5f; 
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .insight-box {
        background: linear-gradient(135deg, #262730, #3a3b45);
        padding: 1rem; 
        border-radius: 15px; 
        margin-top: 1rem;
        border: 1px solid #444;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #1e1e2e, #2a2a3e);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #444;
        margin: 0.5rem 0;
    }
    .success-alert {
        background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(34, 197, 94, 0.05));
        border: 1px solid rgba(34, 197, 94, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background: linear-gradient(135deg, rgba(251, 146, 60, 0.1), rgba(251, 146, 60, 0.05));
        border: 1px solid rgba(251, 146, 60, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. –£–¢–ò–õ–ò–¢–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø (–£–õ–£–ß–®–ï–ù–ù–´–ï) ---

def validate_youtube_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ YouTube API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    
    if api_key.startswith('AIza') and len(api_key) == 39:
        return True
    
    if len(api_key) > 30 and all(c.isalnum() or c in '-_' for c in api_key):
        return True
    
    return False

def validate_openai_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ OpenAI API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return api_key.startswith('sk-') and len(api_key) > 40

def validate_serpapi_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ SerpAPI –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return len(api_key) > 30 and all(c.isalnum() for c in api_key)

def safe_format_number(num) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        if pd.isna(num) or num is None:
            return "0"
        
        num = float(num)
        if num >= 1_000_000_000:
            return f"{num/1_000_000_000:.1f}B"
        elif num >= 1_000_000:
            return f"{num/1_000_000:.1f}M"
        elif num >= 1_000:
            return f"{num/1_000:.1f}K"
        return str(int(num))
    except (ValueError, TypeError, OverflowError):
        return "0"

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not isinstance(text, str):
        return ""
    
    text = unicodedata.normalize('NFKD', text)
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    return text.strip()

def safe_int_conversion(value, default=0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    try:
        if pd.isna(value) or value is None:
            return default
        return int(float(value))
    except (ValueError, TypeError, OverflowError):
        return default

def safe_float_conversion(value, default=0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
    try:
        if pd.isna(value) or value is None:
            return default
        return float(value)
    except (ValueError, TypeError, OverflowError):
        return default

def validate_keyword(keyword: str) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    if not keyword or not isinstance(keyword, str):
        return False
    
    keyword = keyword.strip()
    
    if len(keyword) < 2 or len(keyword) > 100:
        return False
    
    if keyword.count(' ') > 10:
        return False
    
    if re.search(r'[<>"\'\[\]{}|\\`]', keyword):
        return False
    
    return True

def extract_keywords_from_titles(titles: list, min_length=3, max_keywords=15) -> list:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    if not titles:
        return []
    
    all_words = []
    stop_words = {
        '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–∑–∞', '–æ—Ç', '–¥–æ', 
        '–∏–∑', '–∫', '–æ', '—É', '–∂–µ', '–µ—â–µ', '—É–∂–µ', '–∏–ª–∏', '—Ç–∞–∫', '–Ω–æ', '–∞', '–∏—Ö', '–µ–≥–æ', 
        '–µ—ë', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–µ—Å–ª–∏', 
        '—á—Ç–æ–±—ã', '–∫–æ–≥–¥–∞', '–≥–¥–µ', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 
        'for', 'of', 'with', 'by', 'you', 'are', 'can', 'all', 'any', 'how', 'what', 
        'when', 'where', 'why', 'this', 'that', 'have', 'had', 'will', 'been', 'were',
        'was', 'are', 'is', 'am', 'be', 'do', 'did', 'does', 'has', 'get', 'got'
    }
    
    try:
        for title in titles:
            if not title:
                continue
            
            title_clean = clean_text(str(title).lower())
            words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', title_clean)
            filtered_words = [
                word for word in words 
                if len(word) >= min_length and word not in stop_words
            ]
            all_words.extend(filtered_words)
        
        word_counts = Counter(all_words)
        return word_counts.most_common(max_keywords)
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return []

def retry_api_call(func, max_retries=MAX_RETRIES, delay=REQUEST_DELAY):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ API –≤—ã–∑–æ–≤–æ–≤"""
    def wrapper(*args, **kwargs):
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    logger.info(f"API –≤—ã–∑–æ–≤ —É—Å–ø–µ—à–µ–Ω —Å {attempt + 1} –ø–æ–ø—ã—Ç–∫–∏")
                return result
            
            except HttpError as e:
                last_exception = e
                status_code = e.resp.status
                
                if status_code == 403:
                    st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                    break
                elif status_code == 400:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API")
                    break
                elif status_code in [500, 502, 503, 504]:
                    logger.warning(f"–°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))
                        continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {status_code}: {e}")
                    break
            
            except Exception as e:
                last_exception = e
                logger.warning(f"–û—à–∏–±–∫–∞ API –≤—ã–∑–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay)
                    continue
                break
        
        logger.error(f"API –≤—ã–∑–æ–≤ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_exception}")
        raise last_exception
    
    return wrapper

# --- 3. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ ---

class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {
            'search': 3600*4,
            'channels': 3600*24*7,
            'trends': 3600*8,
            'openai': 3600*24,
            'serpapi': 3600*6
        }
        self.stats = {'hits': 0, 'misses': 0, 'errors': 0, 'size_mb': 0}
        self._update_cache_stats()

    def _init_sqlite(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                with self.lock:
                    conn = sqlite3.connect(
                        self.db_path, 
                        check_same_thread=False,
                        timeout=10.0
                    )
                    cursor = conn.cursor()
                    
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS cache (
                            key TEXT PRIMARY KEY, 
                            value BLOB, 
                            expires_at TIMESTAMP,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            access_count INTEGER DEFAULT 1,
                            category TEXT,
                            size_bytes INTEGER
                        )
                    ''')
                    
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON cache(category)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_access_count ON cache(access_count)')
                    
                    cursor.execute('PRAGMA journal_mode=WAL')
                    cursor.execute('PRAGMA synchronous=NORMAL')
                    cursor.execute('PRAGMA cache_size=10000')
                    
                    conn.commit()
                    conn.close()
                    logger.info("–ö—ç—à –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    return
            
            except sqlite3.Error as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1)
                    continue
                else:
                    st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")

    def _update_cache_stats(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                result = cursor.fetchone()
                if result:
                    self.stats['size_mb'] = round(result[0] / (1024 * 1024), 2)
                
                conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞: {e}")

    def get(self, key: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "SELECT value, expires_at, access_count FROM cache WHERE key = ?", 
                    (key,)
                )
                result = cursor.fetchone()
                
                if result:
                    value_blob, expires_at, access_count = result
                    
                    if datetime.fromisoformat(expires_at) > datetime.now():
                        cursor.execute(
                            "UPDATE cache SET access_count = ? WHERE key = ?",
                            (access_count + 1, key)
                        )
                        conn.commit()
                        conn.close()
                        
                        self.stats['hits'] += 1
                        return pickle.loads(value_blob)
                    else:
                        cursor.execute("DELETE FROM cache WHERE key = ?", (key,))
                        conn.commit()
                
                conn.close()
                self.stats['misses'] += 1
                return None
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: {e}")
            return None

    def set(self, key: str, value: any, category: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            with self.lock:
                ttl = self.ttl_map.get(category, 3600)
                expires_at = datetime.now() + timedelta(seconds=ttl)
                value_blob = pickle.dumps(value)
                size_bytes = len(value_blob)
                
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT OR REPLACE INTO cache 
                    (key, value, expires_at, category, size_bytes, created_at, access_count) 
                    VALUES (?, ?, ?, ?, ?, ?, 1)
                """, (key, value_blob, expires_at.isoformat(), category, size_bytes, datetime.now().isoformat()))
                
                conn.commit()
                conn.close()
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")

    def clean_expired(self) -> int:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "DELETE FROM cache WHERE expires_at < ?", 
                    (datetime.now().isoformat(),)
                )
                expired_count = cursor.rowcount
                
                cursor.execute("SELECT COUNT(*) FROM cache")
                total_records = cursor.fetchone()[0]
                
                if total_records > 1000:
                    cursor.execute("""
                        DELETE FROM cache WHERE key IN (
                            SELECT key FROM cache 
                            ORDER BY access_count ASC, created_at ASC 
                            LIMIT ?
                        )
                    """, (total_records // 10,))
                    
                    old_records = cursor.rowcount
                    logger.info(f"–£–¥–∞–ª–µ–Ω–æ {old_records} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
                
                cursor.execute("VACUUM")
                
                conn.commit()
                conn.close()
                
                self._update_cache_stats()
                return expired_count
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
            return 0

    def get_cache_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*), SUM(size_bytes) FROM cache")
                count, total_size = cursor.fetchone()
                
                cursor.execute("""
                    SELECT category, COUNT(*), SUM(size_bytes), AVG(access_count)
                    FROM cache GROUP BY category
                """)
                categories = cursor.fetchall()
                
                conn.close()
                
                return {
                    'total_records': count or 0,
                    'total_size_mb': round((total_size or 0) / (1024 * 1024), 2),
                    'categories': {cat: {'count': cnt, 'size_mb': round((size or 0) / (1024 * 1024), 2), 'avg_access': round(avg or 0, 1)} for cat, cnt, size, avg in categories},
                    'hit_rate': round(self.stats['hits'] / max(self.stats['hits'] + self.stats['misses'], 1) * 100, 1)
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ: {e}")
            return {'error': str(e)}

    def generate_key(self, *args) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–π"""
        try:
            clean_args = []
            for arg in args:
                if arg is None:
                    clean_args.append('None')
                else:
                    clean_args.append(str(arg).strip()[:100])
            
            combined = "|".join(clean_args)
            return hashlib.md5(combined.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞: {e}")
            return hashlib.md5(f"error_{time.time()}".encode()).hexdigest()

class YouTubeAnalyzer:
    def __init__(self, api_key: str, cache: CacheManager):
        try:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
            self.cache = cache
            self.api_key = api_key
            self.quota_used = 0
            logger.info("YouTube API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ YouTube API: {e}")
            raise

    def test_connection(self) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            return hasattr(self.youtube, 'search') and hasattr(self.youtube, 'videos')
        except Exception as e:
            logger.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ API: {e}")
            return True

    def _make_api_request(self, request_func, *args, **kwargs):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –∫–≤–æ—Ç"""
        try:
            if self.quota_used > YOUTUBE_API_DAILY_QUOTA * 0.9:
                st.warning("‚ö†Ô∏è –ü—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ –ª–∏–º–∏—Ç—É YouTube API –∫–≤–æ—Ç—ã")
            
            response = retry_api_call(request_func)(*args, **kwargs)
            self.quota_used += 1
            return response
        
        except HttpError as e:
            if e.resp.status == 403:
                st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
            elif e.resp.status == 400:
                st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API")
            else:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ YouTube API: {e}")
            raise
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ API: {e}")
            raise

    def get_channel_stats(self, channel_ids: list):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤"""
        if not channel_ids:
            return {}
        
        unique_ids = list(set(filter(None, channel_ids)))
        if not unique_ids:
            return {}
            
        cache_key = self.cache.generate_key('channels', sorted(unique_ids))
        if cached_data := self.cache.get(cache_key):
            return cached_data
        
        channel_stats = {}
        try:
            for i in range(0, len(unique_ids), 50):
                chunk_ids = unique_ids[i:i+50]
                
                request = self.youtube.channels().list(
                    part="statistics,snippet,brandingSettings", 
                    id=",".join(chunk_ids)
                )
                response = self._make_api_request(request.execute)
                
                for item in response.get('items', []):
                    stats = item.get('statistics', {})
                    snippet = item.get('snippet', {})
                    branding = item.get('brandingSettings', {}).get('channel', {})
                    
                    channel_stats[item['id']] = {
                        'subscribers': safe_int_conversion(stats.get('subscriberCount', 0)),
                        'total_views': safe_int_conversion(stats.get('viewCount', 0)),
                        'video_count': safe_int_conversion(stats.get('videoCount', 0)),
                        'title': clean_text(snippet.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
                        'description': clean_text(snippet.get('description', ''))[:500],
                        'published_at': snippet.get('publishedAt', ''),
                        'country': snippet.get('country', ''),
                        'verified': snippet.get('customUrl', '').startswith('@'),
                        'keywords': branding.get('keywords', '').split(',')[:10] if branding.get('keywords') else []
                    }
                
                if i + 50 < len(unique_ids):
                    time.sleep(REQUEST_DELAY)
            
            self.cache.set(cache_key, channel_stats, 'channels')
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {len(channel_stats)} –∫–∞–Ω–∞–ª–æ–≤")
            return channel_stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤: {e}")
            return {}

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        
        if not validate_keyword(keyword):
            st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            return None
        
        if max_results > 500:
            max_results = 500
            st.warning("‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 500")
        
        cache_key = self.cache.generate_key('search_v2', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_snippets = []
            next_page_token = None
            search_params = {
                'q': clean_text(keyword),
                'part': 'snippet',
                'type': 'video',
                'order': 'relevance',
                'regionCode': 'RU',
                'relevanceLanguage': 'ru'
            }
            
            if published_after:
                search_params['publishedAfter'] = published_after

            progress_bar = st.progress(0)
            status_text = st.empty()
            
            while len(video_snippets) < max_results:
                search_params['maxResults'] = min(50, max_results - len(video_snippets))
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                status_text.text(f"üîç –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(video_snippets)}/{max_results}")
                progress_bar.progress(len(video_snippets) / max_results)
                
                request = self.youtube.search().list(**search_params)
                search_response = self._make_api_request(request.execute)
                new_items = search_response.get('items', [])
                
                if not new_items:
                    logger.warning("–ü–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    break
                    
                video_snippets.extend(new_items)
                next_page_token = search_response.get('nextPageToken')
                
                if not next_page_token:
                    break
                
                time.sleep(REQUEST_DELAY)

            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(video_snippets)} –≤–∏–¥–µ–æ")
            
            if not video_snippets:
                return []

            video_ids = [item['id']['videoId'] for item in video_snippets if 'videoId' in item.get('id', {})]
            channel_ids = list(set([item['snippet']['channelId'] for item in video_snippets]))

            status_text.text("üìä –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É...")
            channel_stats = self.get_channel_stats(channel_ids)
            
            videos = []
            
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                
                request = self.youtube.videos().list(
                    part='statistics,contentDetails,snippet,topicDetails', 
                    id=','.join(chunk_ids)
                )
                stats_response = self._make_api_request(request.execute)
                
                video_details_map = {item['id']: item for item in stats_response.get('items', [])}
                
                for snippet in video_snippets[i:i+50]:
                    video_id = snippet['id'].get('videoId')
                    if not video_id:
                        continue
                        
                    details = video_details_map.get(video_id)
                    if not details:
                        continue
                    
                    stats = details.get('statistics', {})
                    content_details = details.get('contentDetails', {})
                    video_snippet = details.get('snippet', {})
                    topic_details = details.get('topicDetails', {})
                    
                    duration = self._parse_duration(content_details.get('duration', 'PT0S'))
                    channel_id = snippet['snippet']['channelId']
                    channel_info = channel_stats.get(channel_id, {})
                    
                    category_id = safe_int_conversion(video_snippet.get('categoryId', 0))
                    tags = video_snippet.get('tags', [])[:20]
                    
                    video_data = {
                        'video_id': video_id,
                        'title': clean_text(snippet['snippet']['title']),
                        'channel': clean_text(snippet['snippet']['channelTitle']),
                        'channel_id': channel_id,
                        'subscribers': channel_info.get('subscribers', 0),
                        'subscribers_formatted': safe_format_number(channel_info.get('subscribers', 0)),
                        'channel_total_views': channel_info.get('total_views', 0),
                        'channel_video_count': channel_info.get('video_count', 0),
                        'channel_verified': channel_info.get('verified', False),
                        'published': snippet['snippet']['publishedAt'],
                        'views': safe_int_conversion(stats.get('viewCount', 0)),
                        'views_formatted': safe_format_number(safe_int_conversion(stats.get('viewCount', 0))),
                        'likes': safe_int_conversion(stats.get('likeCount', 0)),
                        'likes_formatted': safe_format_number(safe_int_conversion(stats.get('likeCount', 0))),
                        'comments': safe_int_conversion(stats.get('commentCount', 0)),
                        'duration': duration,
                        'duration_formatted': self._format_duration(duration),
                        'is_short': duration <= 1.05,
                        'short_indicator': "ü©≥ Shorts" if duration <= 1.05 else "üìπ –í–∏–¥–µ–æ",
                        'tags': tags,
                        'description': clean_text(video_snippet.get('description', ''))[:1000],
                        'definition': content_details.get('definition', 'sd').upper(),
                        'category_id': category_id,
                        'language': video_snippet.get('defaultLanguage', 'ru'),
                        'topics': topic_details.get('topicCategories', [])[:5],
                        'thumbnail': snippet['snippet'].get('thumbnails', {}).get('medium', {}).get('url', ''),
                        'video_url': f"https://www.youtube.com/watch?v={video_id}"
                    }
                    videos.append(video_data)
                
                if i + 50 < len(video_ids):
                    time.sleep(REQUEST_DELAY)
            
            progress_bar.empty()
            status_text.empty()
            
            self.cache.set(cache_key, videos, 'search')
            logger.info(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ")
            return videos
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {e}")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not duration_str:
            return 0
            
        try:
            match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
            if not match:
                return 0
                
            h, m, s = (safe_int_conversion(g) for g in match.groups())
            return h * 60 + m + s / 60
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ '{duration_str}': {e}")
            return 0
    
    def _format_duration(self, duration_minutes: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        try:
            if duration_minutes < 1:
                return f"0:{int(duration_minutes * 60):02d}"
            
            hours = int(duration_minutes // 60)
            minutes = int(duration_minutes % 60)
            seconds = int((duration_minutes % 1) * 60)
            
            if hours > 0:
                return f"{hours}:{minutes:02d}:{seconds:02d}"
            else:
                return f"{minutes}:{seconds:02d}"
        except Exception:
            return "0:00"
    
    def analyze_competition(self, videos: list):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        if not videos:
            return {}, pd.DataFrame()
        
        try:
            df = pd.DataFrame(videos)
            
            df['published'] = pd.to_datetime(df['published'], errors='coerce', utc=True).dt.tz_localize(None)
            df = df.dropna(subset=['published', 'views'])
            
            if df.empty:
                logger.warning("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö DataFrame –ø—É—Å—Ç")
                return {}, pd.DataFrame()
            
            df['views'] = df['views'].apply(lambda x: max(safe_int_conversion(x, 1), 1))
            df['days_ago'] = (datetime.now() - df['published']).dt.days.fillna(0)
            df['engagement_rate'] = ((df['likes'] + df['comments']) / df['views']) * 100
            df['views_per_subscriber'] = df['views'] / (df['subscribers'] + 1)
            
            view_quartiles = df['views'].quantile([0.25, 0.5, 0.75, 0.9])
            
            analysis = {
                'total_videos': len(df),
                'avg_views': safe_float_conversion(df['views'].mean()),
                'median_views': safe_float_conversion(df['views'].median()),
                'top_10_avg_views': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['views'].mean()),
                'top_25_percent_views': safe_float_conversion(view_quartiles[0.75]),
                'engagement_rate': safe_float_conversion(df['engagement_rate'].mean()),
                'videos_last_week': len(df[df['days_ago'] <= 7]),
                'videos_last_month': len(df[df['days_ago'] <= 30]),
                'shorts_percentage': safe_float_conversion(df['is_short'].mean() * 100),
                'avg_days_to_top_10': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['days_ago'].mean()),
                'unique_channels': df['channel'].nunique(),
                'avg_channel_subscribers': safe_float_conversion(df['subscribers'].mean()),
                'avg_duration': safe_float_conversion(df[~df['is_short']]['duration'].mean()),
                'hd_percentage': safe_float_conversion((df['definition'] == 'HD').mean() * 100),
                'verified_channels_count': safe_int_conversion(df['channel_verified'].sum()),
                'avg_likes_per_view': safe_float_conversion((df['likes'] / df['views']).mean() * 100),
                'avg_comments_per_view': safe_float_conversion((df['comments'] / df['views']).mean() * 100)
            }

            score = 0
            
            if analysis['top_10_avg_views'] < 20000:
                score += 4
            elif analysis['top_10_avg_views'] < 50000:
                score += 3
            elif analysis['top_10_avg_views'] < 200000:
                score += 2
            elif analysis['top_10_avg_views'] < 500000:
                score += 1
            
            if analysis['videos_last_week'] < 2:
                score += 3
            elif analysis['videos_last_week'] < 5:
                score += 2
            elif analysis['videos_last_week'] < 15:
                score += 1
            
            if analysis['unique_channels'] < 15:
                score += 2
            elif analysis['unique_channels'] < 30:
                score += 1
            
            if analysis['engagement_rate'] < 1.5:
                score += 2
            elif analysis['engagement_rate'] < 3:
                score += 1
            
            competition_levels = {
                0: '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥',
                1: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥', 
                2: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
                3: '–í—ã—Å–æ–∫–∞—è üü†',
                4: '–í—ã—Å–æ–∫–∞—è üü†',
                5: '–°—Ä–µ–¥–Ω—è—è üü°',
                6: '–°—Ä–µ–¥–Ω—è—è üü°',
                7: '–ù–∏–∑–∫–∞—è üü¢',
                8: '–ù–∏–∑–∫–∞—è üü¢',
                9: '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢',
                10: '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è üü¢'
            }
            
            analysis['competition_level'] = competition_levels.get(score, '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥')
            analysis['competition_score'] = score
            analysis['opportunity_rating'] = min(score * 10, 100)
            
            return analysis, df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
            return {}, pd.DataFrame()

class AdvancedTrendsAnalyzer:
    def __init__(self, cache: CacheManager):
        self.cache = cache
        
    def _get_pytrends(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ pytrends —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return TrendReq(hl='ru-RU', tz=180, timeout=(10, 25), retries=2, backoff_factor=0.1)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Trends: {e}")
            return None

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
            
        pytrends = self._get_pytrends()
        if not pytrends:
            return None
            
        try:
            pytrends.build_payload([keyword], timeframe='today 12-m', geo='RU')
            interest_12m = pytrends.interest_over_time()
            
            if interest_12m.empty:
                return None
            
            try:
                pytrends.build_payload([keyword], timeframe='today 5-y', geo='RU')
                interest_5y = pytrends.interest_over_time()
            except:
                interest_5y = pd.DataFrame()
            
            try:
                related_queries = pytrends.related_queries()
                rising_queries = related_queries.get(keyword, {}).get('rising', pd.DataFrame())
                top_queries = related_queries.get(keyword, {}).get('top', pd.DataFrame())
            except:
                rising_queries = pd.DataFrame()
                top_queries = pd.DataFrame()
            
            series = interest_12m[keyword]
            recent_avg = series.tail(4).mean()
            previous_avg = series.iloc[-8:-4].mean()
            overall_avg = series.mean()
            
            if recent_avg > previous_avg * 1.2:
                trend_direction = "–ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—É—â–∏–π üöÄ"
            elif recent_avg > previous_avg * 1.1:
                trend_direction = "–†–∞—Å—Ç—É—â–∏–π üìà"
            elif recent_avg < previous_avg * 0.8:
                trend_direction = "–ü–∞–¥–∞—é—â–∏–π üìâ"
            elif recent_avg < previous_avg * 0.9:
                trend_direction = "–°–ª–∞–±–æ –ø–∞–¥–∞—é—â–∏–π üìâ"
            else:
                trend_direction = "–°—Ç–∞–±–∏–ª—å–Ω—ã–π ‚û°Ô∏è"
            
            monthly_avg = series.groupby(series.index.month).mean()
            peak_months = monthly_avg.nlargest(3).index.tolist()
            
            result = {
                'interest_df': interest_12m,
                'interest_5y_df': interest_5y,
                'trend_direction': trend_direction,
                'recent_avg': recent_avg,
                'overall_avg': overall_avg,
                'trend_strength': abs(recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0,
                'rising_queries': rising_queries,
                'top_queries': top_queries,
                'peak_months': peak_months,
                'current_interest': series.iloc[-1] if not series.empty else 0
            }
            
            self.cache.set(cache_key, result, 'trends')
            return result
            
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

# --- 4. –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–ï–ì–û–í ---

@dataclass
class TagScore:
    keyword: str
    search_volume: int
    competition_score: int
    seo_score: int
    overall_score: int
    difficulty: str

class YouTubeTagAnalyzer:
    def __init__(self, serpapi_key: str = None, cache: CacheManager = None):
        self.serpapi_key = serpapi_key
        self.use_serpapi = bool(serpapi_key)
        self.cache = cache
        self.base_serpapi = "https://serpapi.com/search"
        
    def get_search_volume_serpapi(self, keyword: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._estimate_search_volume_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_volume', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
                'gl': 'us',
                'hl': 'en'
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            data = response.json()
            
            if 'video_results' in data:
                result_count = len(data['video_results'])
                volume = min(result_count * 150, 100000)
                
                if cache_key and self.cache:
                    self.cache.set(cache_key, volume, 'search')
                return volume
                
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ SerpAPI –¥–ª—è '{keyword}': {e}")
            
        return self._estimate_search_volume_basic(keyword)
    
    def _estimate_search_volume_basic(self, keyword: str) -> int:
        """–ë–∞–∑–æ–≤–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API"""
        word_count = len(keyword.split())
        char_count = len(keyword)
        
        base_volume = max(1000, 5000 - (word_count * 500) - (char_count * 10))
        
        popular_words = {
            '–∫–∞–∫', '—á—Ç–æ', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–æ–±–∑–æ—Ä', '—É—Ä–æ–∫', '—Ç—É—Ç–æ—Ä–∏–∞–ª',
            'guide', 'tutorial', 'how', 'what', 'review', 'tips'
        }
        
        bonus = sum(300 for word in keyword.lower().split() if word in popular_words)
        
        return min(base_volume + bonus, 50000)
    
    def analyze_competition_serpapi(self, keyword: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._analyze_competition_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_competition', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
                'gl': 'us',
                'hl': 'en'
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            data = response.json()
            
            if 'video_results' not in data:
                return self._analyze_competition_basic(keyword)
            
            videos = data['video_results'][:20]
            analysis = self._process_competition_data(videos, keyword)
            
            if cache_key and self.cache:
                self.cache.set(cache_key, analysis, 'search')
            
            return analysis
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
            return self._analyze_competition_basic(keyword)
    
    def _analyze_competition_basic(self, keyword: str) -> dict:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API)"""
        word_count = len(keyword.split())
        
        if word_count == 1:
            competition_level = "High"
            optimized_ratio = 0.8
        elif word_count == 2:
            competition_level = "Medium"
            optimized_ratio = 0.6
        else:
            competition_level = "Low"
            optimized_ratio = 0.4
        
        return {
            'total_videos': 20,
            'optimized_titles': int(20 * optimized_ratio),
            'high_view_videos': max(1, int(20 * (1 - optimized_ratio))),
            'verified_channels': max(1, int(20 * 0.3)),
            'avg_views': 15000 if word_count == 1 else 8000,
            'keyword_in_title': int(20 * optimized_ratio),
            'recent_videos': 3,
            'competition_level': competition_level
        }
    
    def _process_competition_data(self, videos: list, keyword: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        analysis = {
            'total_videos': len(videos),
            'optimized_titles': 0,
            'high_view_videos': 0,
            'verified_channels': 0,
            'avg_views': 0,
            'keyword_in_title': 0,
            'recent_videos': 0
        }
        
        total_views = 0
        keyword_lower = keyword.lower()
        
        for video in videos:
            title = video.get('title', '').lower()
            if any(word in title for word in keyword_lower.split()):
                analysis['optimized_titles'] += 1
                analysis['keyword_in_title'] += 1
            
            views = self._extract_views(video.get('views', '0'))
            total_views += views
            
            if views > 100000:
                analysis['high_view_videos'] += 1
            
            channel = video.get('channel', {})
            if 'verified' in str(channel).lower():
                analysis['verified_channels'] += 1
            
            published = video.get('published_date', '')
            if self._is_recent(published):
                analysis['recent_videos'] += 1
        
        if analysis['total_videos'] > 0:
            analysis['avg_views'] = total_views // analysis['total_videos']
        
        return analysis
    
    def _extract_views(self, views_str: str) -> int:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"""
        if not views_str:
            return 0
        
        clean = ''.join(c for c in str(views_str) if c.isdigit() or c in [',', '.'])
        try:
            number_str = clean.replace(',', '').replace('.', '')
            return int(number_str) if number_str else 0
        except:
            return 0
    
    def _is_recent(self, date_str: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –≤–∏–¥–µ–æ"""
        if not date_str:
            return False
        
        recent_indicators = ['day', 'days', 'week', 'weeks', 'hour', 'hours']
        return any(indicator in date_str.lower() for indicator in recent_indicators)
    
    def calculate_scores(self, keyword: str, analysis: dict, search_volume: int) -> TagScore:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ç–µ–≥–∞"""
        total = analysis['total_videos']
        if total == 0:
            competition_score = 50
        else:
            optimized_ratio = analysis['optimized_titles'] / total
            high_views_ratio = analysis['high_view_videos'] / total
            verified_ratio = analysis['verified_channels'] / total
            
            avg_views_factor = min(analysis['avg_views'] / 500000, 1.0)
            
            competition_score = min(int((
                optimized_ratio * 0.3 +
                high_views_ratio * 0.25 +
                verified_ratio * 0.2 +
                avg_views_factor * 0.25
            ) * 100), 100)
        
        if total > 0:
            keyword_optimization = analysis['keyword_in_title'] / total
            seo_score = max(int((1.0 - keyword_optimization) * 100), 10)
        else:
            seo_score = 50
        
        import math
        volume_score = min(math.log10(max(search_volume, 1)) * 25, 100)
        competition_inverted = 100 - competition_score
        
        overall_score = min(int(
            volume_score * 0.4 +
            competition_inverted * 0.35 +
            seo_score * 0.25
        ), 100)
        
        if competition_score <= 20:
            difficulty = "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 40:
            difficulty = "–ù–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 60:
            difficulty = "–°—Ä–µ–¥–Ω—è—è üü°"
        elif competition_score <= 80:
            difficulty = "–í—ã—Å–æ–∫–∞—è üü†"
        else:
            difficulty = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥"
        
        return TagScore(
            keyword=keyword,
            search_volume=search_volume,
            competition_score=competition_score,
            seo_score=seo_score,
            overall_score=overall_score,
            difficulty=difficulty
        )
    
    def analyze_keyword(self, keyword: str) -> TagScore:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        search_volume = self.get_search_volume_serpapi(keyword)
        competition_analysis = self.analyze_competition_serpapi(keyword)
        
        return self.calculate_scores(keyword, competition_analysis, search_volume)
    
    def analyze_multiple_keywords(self, keywords: list) -> list:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, keyword in enumerate(keywords):
            try:
                status_text.text(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–≥: {keyword}")
                progress_bar.progress((i + 1) / len(keywords))
                
                result = self.analyze_keyword(keyword)
                results.append(result)
                
                if self.use_serpapi:
                    time.sleep(1)
                    
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ '{keyword}': {e}")
                continue
        
        progress_bar.empty()
        status_text.empty()
        
        return sorted(results, key=lambda x: x.overall_score, reverse=True)

class ContentStrategist:
    def __init__(self, openai_key=None, openai_model=None):
        self.use_openai = bool(openai_key and openai_model)
        if self.use_openai:
            self.client = openai.OpenAI(api_key=openai_key)
            self.model = openai_model

    def get_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame, cache: CacheManager):
        cache_key = None
        if self.use_openai:
            cache_key = cache.generate_key('openai_v3', keyword, self.model, str(comp_analysis)[:100])
            if cached_strategy := cache.get(cache_key):
                st.toast("ü§ñ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞!", icon="üß†")
                return cached_strategy
        
        if self.use_openai:
            strategy = self._get_ai_strategy(keyword, comp_analysis, trends_data, df)
        else:
            strategy = self._get_rule_based_strategy(keyword, comp_analysis, df)
        
        if self.use_openai and cache_key and "–û—à–∏–±–∫–∞" not in strategy:
            cache.set(cache_key, strategy, 'openai')
        
        return strategy

    def _get_rule_based_strategy(self, keyword: str, comp_analysis: dict, df: pd.DataFrame):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–µ–∑ AI"""
        
        if not df.empty:
            titles = df['title'].tolist()
            popular_words = extract_keywords_from_titles(titles)
            top_words = [word for word, count in popular_words[:5]]
        else:
            top_words = []
        
        competition_level = comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        avg_views = comp_analysis.get('avg_views', 0)
        shorts_percentage = comp_analysis.get('shorts_percentage', 0)
        
        strategy_parts = []
        
        if '–Ω–∏–∑–∫–∞—è' in competition_level.lower():
            verdict = "üéØ **–û–¢–õ–ò–ß–ù–ê–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–¨!** –ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —à–∞–Ω—Å—ã –¥–ª—è —Ä–æ—Å—Ç–∞."
        elif '—Å—Ä–µ–¥–Ω—è—è' in competition_level.lower():
            verdict = "‚ö° **–•–û–†–û–®–ò–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–´** —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º. –ù—É–∂–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è."
        else:
            verdict = "üî• **–í–´–°–û–ö–ê–Ø –ö–û–ù–ö–£–†–ï–ù–¶–ò–Ø** - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
        
        strategy_parts.append(f"### üéØ –í–µ—Ä–¥–∏–∫—Ç\n{verdict}")
        
        insights = []
        if avg_views < 50000:
            insights.append("üí° –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ–≤—ã—Å–æ–∫–∏–µ - –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–æ–º")
        if shorts_percentage > 50:
            insights.append("üì± –ú–Ω–æ–≥–æ Shorts –≤ –Ω–∏—à–µ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç")
        if top_words:
            insights.append(f"üî§ –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {', '.join(top_words[:3])}")
        
        strategy_parts.append("### üîç –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã\n" + "\n".join(insights))
        
        content_ideas = [
            f"**–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword}** - –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
            f"**–¢–æ–ø-5 –æ—à–∏–±–æ–∫ –≤ {keyword}** - —Ä–∞–∑–±–æ—Ä —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º",
            f"**{keyword}: –¥–æ –∏ –ø–æ—Å–ª–µ** - –∫–µ–π—Å—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
            f"**–ö–∞–∫ –Ω–∞—á–∞—Ç—å –≤ {keyword} –±–µ–∑ –æ–ø—ã—Ç–∞** - –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω",
            f"**–°–µ–∫—Ä–µ—Ç—ã {keyword}, –æ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –≥–æ–≤–æ—Ä—è—Ç** - –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        ]
        
        if shorts_percentage > 30:
            content_ideas.extend([
                f"**{keyword} –∑–∞ 60 —Å–µ–∫—É–Ω–¥** - –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±—É—á–∞—é—â–∏–µ –≤–∏–¥–µ–æ",
                f"**–ë—ã—Å—Ç—Ä—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ {keyword}** - —Å–µ—Ä–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–æ–ª–∏–∫–æ–≤"
            ])
        
        strategy_parts.append("### üí° –ò–¥–µ–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n" + "\n".join(content_ideas))
        
        optimization_tips = [
            "üé® **–Ø—Ä–∫–∏–µ –ø—Ä–µ–≤—å—é** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Ü–≤–µ—Ç–∞ –∏ —á–µ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
            "‚è∞ **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏** - —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ 18:00-21:00 –ø–æ –ú–°–ö",
            "üéØ **–¶–µ–ø–ª—è—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–∏—Å–ª–∞, –≤–æ–ø—Ä–æ—Å—ã, –∏–Ω—Ç—Ä–∏–≥—É",
            "üìù **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è** - –¥–æ–±–∞–≤—å—Ç–µ —Ç–∞–π–º-–∫–æ–¥—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏",
            "üè∑Ô∏è **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–≥–∏** - –º–∏–∫—Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏ –Ω–∏—à–µ–≤—ã—Ö —Ç–µ–≥–æ–≤"
        ]
        
        strategy_parts.append("### üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n" + "\n".join(optimization_tips))
        
        return "\n\n".join(strategy_parts)
    
    def _get_ai_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame):
        st.toast("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ OpenAI...", icon="üß†")
        
        top_titles = []
        top_channels = []
        if not df.empty:
            top_videos = df.nlargest(10, 'views')
            top_titles = top_videos['title'].tolist()
            top_channels = top_videos['channel'].value_counts().head(5).to_dict()
        
        trends_info = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        if trends_data:
            trends_info = f"{trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
            if 'recent_avg' in trends_data:
                trends_info += f" (—Ç–µ–∫—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å: {trends_data['recent_avg']:.0f})"
        
        prompt = f"""
        –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π YouTube-—Å—Ç—Ä–∞—Ç–µ–≥ —Å –æ–ø—ã—Ç–æ–º –±–æ–ª–µ–µ 10 –ª–µ—Ç. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∏—à–∏ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è.

        **–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–ê–Ø –¢–ï–ú–ê:** "{keyword}"

        **–î–ê–ù–ù–´–ï –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:**
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ: {comp_analysis.get('total_videos', 0)}
        - –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
        - –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('avg_views', 0)):,}
        - –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('median_views', 0)):,}
        - –ü—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–ø-10: {int(comp_analysis.get('top_10_avg_views', 0)):,}
        - –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: {comp_analysis.get('engagement_rate', 0):.2f}%
        - –ü—Ä–æ—Ü–µ–Ω—Ç Shorts: {comp_analysis.get('shorts_percentage', 0):.1f}%
        - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {comp_analysis.get('unique_channels', 0)}
        - –í–∏–¥–µ–æ –∑–∞ –Ω–µ–¥–µ–ª—é: {comp_analysis.get('videos_last_week', 0)}

        **–¢–†–ï–ù–î–´ GOOGLE:**
        {trends_info}

        **–¢–û–ü-5 –ó–ê–ì–û–õ–û–í–ö–û–í –ö–û–ù–ö–£–†–ï–ù–¢–û–í:**
        {chr(10).join(f"‚Ä¢ {title}" for title in top_titles[:5])}

        **–í–ï–î–£–©–ò–ï –ö–ê–ù–ê–õ–´:**
        {chr(10).join(f"‚Ä¢ {channel}: {count} –≤–∏–¥–µ–æ" for channel, count in list(top_channels.items())[:3])}

        **–ó–ê–î–ê–ù–ò–ï:**
        –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:

        1. **üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–¥–∏–∫—Ç** - –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ –Ω–∏—à–∏ –∏ –≥–ª–∞–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        2. **üìä –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π** - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω –Ω–∏—à–∏

        3. **üé¨ –ö–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è** - 7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π –¥–ª—è –≤–∏–¥–µ–æ —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏:
           - –ó–∞–≥–æ–ª–æ–≤–æ–∫
           - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
           - –§–æ—Ä–º–∞—Ç (—Ç—É—Ç–æ—Ä–∏–∞–ª/–æ–±–∑–æ—Ä/–∫–µ–π—Å/–∏ —Ç.–¥.)
           - –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

        4. **üöÄ –¢–∞–∫—Ç–∏–∫–∞ —Ä–æ—Å—Ç–∞** - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 30 –¥–Ω–µ–π

        5. **üí∞ –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è** - 3 —Å–ø–æ—Å–æ–±–∞ –∑–∞—Ä–∞–±–æ—Ç–∫–∞ + –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏

        6. **üè∑Ô∏è SEO –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ–≥–∞–º, –ø—Ä–µ–≤—å—é, –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

        –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ actionable —Å–æ–≤–µ—Ç–∞—Ö.
        """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

# --- 5. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ---

def main():
    st.markdown('<h1 class="main-header">YouTube AI Strategist üß†</h1>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        
        st.subheader("üîë YouTube API")
        youtube_api_key = st.text_input(
            "YouTube API Key", 
            type="password",
            help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –≤ Google Cloud Console"
        )
        
        if youtube_api_key:
            if validate_youtube_api_key(youtube_api_key):
                st.success("‚úÖ YouTube API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
                st.info("üí° YouTube –∫–ª—é—á–∏ –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'AIza...' –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç 39 —Å–∏–º–≤–æ–ª–æ–≤")
        
        st.markdown("---")
        
        st.subheader("ü§ñ AI-—Å—Ç—Ä–∞—Ç–µ–≥")
        use_openai = st.toggle("–í–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑ (OpenAI)", value=True)
        
        openai_api_key = ""
        openai_model = "gpt-4o-mini"
        
        if use_openai:
            openai_api_key = st.text_input(
                "OpenAI API Key", 
                type="password",
                help="–ö–ª—é—á –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å—Ç—Ä–∞—Ç–µ–≥–∏–π"
            )
            
            if openai_api_key:
                if validate_openai_api_key(openai_api_key):
                    st.success("‚úÖ OpenAI API –∫–ª—é—á –≤–∞–ª–∏–¥–µ–Ω")
                else:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π OpenAI API –∫–ª—é—á")
                    st.info("üí° –ö–ª—é—á –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'sk-'")
            
            openai_model = st.selectbox(
                "–ú–æ–¥–µ–ª—å OpenAI", 
                ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"], 
                index=1,
                help="gpt-4o-mini - –±—ã—Å—Ç—Ä–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ"
            )
        
        st.markdown("---")
        
        st.subheader("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤")
        use_serpapi = st.toggle("–í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (SerpAPI)", value=False)
        
        serpapi_key = ""
        if use_serpapi:
            serpapi_key = st.text_input(
                "SerpAPI Key", 
                type="password",
                help="–ö–ª—é—á –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            )
            
            if serpapi_key:
                if validate_serpapi_key(serpapi_key):
                    st.success("‚úÖ SerpAPI –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                else:
                    st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
            
            st.info("üí° SerpAPI –¥–∞–µ—Ç 100 –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü")
        
        st.markdown("---")
        
        st.subheader("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 20, 200, 100, 10)
        
        date_range_options = {
            "–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None,
            "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365,
            "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180,
            "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90,
            "–ó–∞ –º–µ—Å—è—Ü": 30
        }
        
        selected_date_range = st.selectbox(
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:", 
            list(date_range_options.keys()), 
            index=1
        )
        days_limit = date_range_options[selected_date_range]
        
        if not youtube_api_key:
            st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
            st.info("üìö [–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á](https://developers.google.com/youtube/v3/getting-started)")
            st.stop()
        
        cache = CacheManager()
        
        st.markdown("---")
        st.subheader("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
        
        cache_info = cache.get_cache_info()
        if 'error' not in cache_info:
            st.info(f"""
            **üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
            ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {cache_info['total_records']}
            ‚Ä¢ –†–∞–∑–º–µ—Ä: {cache_info['total_size_mb']} MB
            ‚Ä¢ –ü–æ–ø–∞–¥–∞–Ω–∏—è: {cache.stats['hits']}
            ‚Ä¢ –ü—Ä–æ–º–∞—Ö–∏: {cache.stats['misses']}
            ‚Ä¢ Hit Rate: {cache_info['hit_rate']}%
            """)
            
            if cache_info['categories']:
                st.markdown("**üìÅ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:**")
                for cat, info in cache_info['categories'].items():
                    st.text(f"‚Ä¢ {cat}: {info['count']} ({info['size_mb']} MB)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–π"):
                deleted = cache.clean_expired()
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π")
                st.rerun()
        
        with col2:
            if st.button("üí• –û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"):
                try:
                    cache.db_path.unlink(missing_ok=True)
                    st.success("–ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")
                    st.rerun()
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
        
        st.markdown("---")
        st.subheader("‚öôÔ∏è –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        
        show_advanced = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        if show_advanced:
            request_delay = st.slider(
                "–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)", 
                0.1, 2.0, REQUEST_DELAY, 0.1,
                help="–£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ API"
            )
            
            max_retries = st.slider(
                "–ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫", 
                1, 5, MAX_RETRIES,
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö API"
            )
            
            globals()['REQUEST_DELAY'] = request_delay
            globals()['MAX_RETRIES'] = max_retries
        
        st.markdown("---")
        st.subheader("üë®‚Äçüíª –ê–≤—Ç–æ—Ä")
        st.markdown("""
        **–°–≤—è–∑–∞—Ç—å—Å—è:**
        - üí¨ [Telegram](https://t.me/i_gma)
        - üì¢ [–ö–∞–Ω–∞–ª –æ AI](https://t.me/igm_a)
        - üîó [GitHub](https://github.com/yourusername)
        """)

    st.markdown("### üéØ –í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        keyword = st.text_input(
            "",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã...",
            help="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ YouTube –Ω–∏—à–∏"
        )
    
    with col2:
        analyze_button = st.button(
            "üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!", 
            type="primary", 
            use_container_width=True,
            disabled=not keyword
        )

    st.markdown("**üí° –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**")
    example_cols = st.columns(3)
    
    examples = [
        "python –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
        "–º–æ–Ω—Ç–∞–∂ –≤–∏–¥–µ–æ",
        "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –∞–∫—Ü–∏–∏"
    ]
    
    for i, example in enumerate(examples):
        if example_cols[i % 3].button(f"üìå {example}", key=f"example_{i}"):
            keyword = example
            analyze_button = True

    if analyze_button and keyword:
        try:
            analyzer = YouTubeAnalyzer(youtube_api_key, cache)
            trends_analyzer = AdvancedTrendsAnalyzer(cache)
            
            analyzer.test_connection()
            
            spinner_text = "üåä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é YouTube..."
            if use_openai and openai_api_key and validate_openai_api_key(openai_api_key):
                spinner_text += " –ü—Ä–∏–≤–ª–µ–∫–∞—é AI..."

            with st.spinner(spinner_text):
                published_after_date = None
                if days_limit:
                    published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z"
                
                videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
                
                if videos is None:
                    st.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ YouTube API")
                    st.stop()
                
                if not videos:
                    st.warning("üîç –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –ø–æ –¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                    st.markdown("""
                    - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                    - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
                    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                    """)
                    st.stop()
                
                comp_analysis, df = analyzer.analyze_competition(videos)
                trends_data = trends_analyzer.analyze_keyword_trends(keyword)
                
                strategist = ContentStrategist(
                    openai_api_key if use_openai and validate_openai_api_key(openai_api_key) else None,
                    openai_model if use_openai else None
                )
                strategy_output = strategist.get_strategy(keyword, comp_analysis, trends_data, df, cache)

            st.markdown("---")
            st.markdown(f"# üìä –ê–Ω–∞–ª–∏–∑ –Ω–∏—à–∏: **{keyword}**")
            
            st.markdown("### üéØ –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    "üìπ –í–∏–¥–µ–æ", 
                    f"{len(df)}",
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ"
                )
            
            with col2:
                competition_level = comp_analysis['competition_level']
                st.metric(
                    "üèÜ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", 
                    competition_level.split()[0],
                    help=f"–ü–æ–ª–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {competition_level}"
                )
            
            with col3:
                avg_views = comp_analysis['avg_views']
                st.metric(
                    "üëÄ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", 
                    safe_format_number(int(avg_views)),
                    help=f"–¢–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(avg_views):,}"
                )
            
            with col4:
                engagement = comp_analysis['engagement_rate']
                st.metric(
                    "üí¨ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", 
                    f"{engagement:.1f}%",
                    help="–ù–∞—Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω–æ –∑—Ä–∏—Ç–µ–ª–∏ —Å—Ç–∞–≤—è—Ç –ª–∞–π–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç"
                )
            
            with col5:
                channels = comp_analysis['unique_channels']
                st.metric(
                    "üì∫ –ö–∞–Ω–∞–ª–æ–≤", 
                    channels,
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ"
                )

            st.markdown("### üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üî• –¢–æ–ø-10 –≤–∏–¥–µ–æ",
                    safe_format_number(int(comp_analysis['top_10_avg_views'])),
                    help="–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã —É –ª—É—á—à–∏—Ö 10 –≤–∏–¥–µ–æ"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col2:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üì± –ö–æ—Ä–æ—Ç–∫–∏–µ –≤–∏–¥–µ–æ",
                    f"{comp_analysis['shorts_percentage']:.0f}%",
                    help="–ü—Ä–æ—Ü–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ (–¥–æ 1 –º–∏–Ω—É—Ç—ã)"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col3:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üóìÔ∏è –ó–∞ –Ω–µ–¥–µ–ª—é",
                    f"{comp_analysis['videos_last_week']} —à—Ç.",
                    help="–°–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ –≤—ã—à–ª–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col4:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                if 'avg_duration' in comp_analysis and comp_analysis['avg_duration'] > 0:
                    duration_str = f"{comp_analysis['avg_duration']:.1f} –º–∏–Ω"
                else:
                    duration_str = "N/A"
                st.metric(
                    "‚è±Ô∏è –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞",
                    duration_str,
                    help="–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–∏–¥–µ–æ (–±–µ–∑ –∫–æ—Ä–æ—Ç–∫–∏—Ö)"
                )
                st.markdown('</div>', unsafe_allow_html=True)

            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üéØ AI –°–æ–≤–µ—Ç—ã", 
                "üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤",
                "üìà –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å", 
                "üèÜ –¢–æ–ø –≤–∏–¥–µ–æ", 
                "üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
            ])

            with tab1:
                css_class = "openai-result" if strategist.use_openai else "custom-container"
                st.markdown(f'<div class="{css_class}">', unsafe_allow_html=True)
                st.markdown(strategy_output)
                st.markdown('</div>', unsafe_allow_html=True)
                
                if not df.empty:
                    st.markdown("### üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã")
                    
                    titles = df['title'].tolist()
                    popular_words = extract_keywords_from_titles(titles)
                    
                    if popular_words:
                        st.markdown("**üè∑Ô∏è –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö:**")
                        words_cols = st.columns(5)
                        for i, (word, count) in enumerate(popular_words[:5]):
                            words_cols[i].metric(word, count)
                    
                    top_channels = df.nlargest(20, 'views').groupby('channel').agg({
                        'views': 'mean',
                        'subscribers': 'first',
                        'video_id': 'count'
                    }).round(0).sort_values('views', ascending=False)
                    
                    if not top_channels.empty:
                        st.markdown("**üì∫ –í–µ–¥—É—â–∏–µ –∫–∞–Ω–∞–ª—ã –≤ –Ω–∏—à–µ:**")
                        st.dataframe(
                            top_channels.head(10).rename(columns={
                                'views': '–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã',
                                'subscribers': '–ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤',
                                'video_id': '–í–∏–¥–µ–æ –≤ –≤—ã–±–æ—Ä–∫–µ'
                            }),
                            use_container_width=True
                        )

            with tab2:
                st.markdown("### üè∑Ô∏è –ö–∞–∫–∏–µ —Ç–µ–≥–∏ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
                
                all_tags = []
                for video in videos:
                    if 'tags' in video and video['tags']:
                        all_tags.extend(video['tags'])
                
                title_words = []
                for video in videos:
                    words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', video['title'].lower())
                    title_words.extend(words)
                
                stop_words = {'–∫–∞–∫', '—á—Ç–æ', '–¥–ª—è', '—ç—Ç–æ', '–≤—Å–µ', '–µ—â–µ', '–≥–¥–µ', '—Ç–∞–∫', '–∏–ª–∏', '—É–∂–µ', '–ø—Ä–∏', '–µ–≥–æ', '–æ–Ω–∏', '–±—ã–ª', 'the', 'and', 'for', 'you', 'are', 'not', 'can', 'but', 'all', 'any', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'use', 'man', 'new', 'now', 'way', 'may'}
                
                potential_tags = list(set(all_tags + title_words))
                potential_tags = [tag for tag in potential_tags if len(tag) > 2 and tag.lower() not in stop_words]
                
                tag_popularity = Counter(all_tags)
                popular_tags = [tag for tag, count in tag_popularity.most_common(20) if count > 1]
                
                if popular_tags:
                    st.markdown("#### üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏ –≤ –Ω–∏—à–µ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üî• –¢–æ–ø —Ç–µ–≥–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:**")
                        selected_tags = []
                        for i, (tag, count) in enumerate(tag_popularity.most_common(10)):
                            if st.checkbox(f"{tag} ({count} —Ä–∞–∑)", key=f"tag_{i}"):
                                selected_tags.append(tag)
                    
                    with col2:
                        st.markdown("**‚ûï –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ —Ç–µ–≥–∏:**")
                        custom_tags = st.text_area(
                            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é:",
                            placeholder="–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, python",
                            help="–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                        )
                        
                        if custom_tags:
                            custom_list = [tag.strip() for tag in custom_tags.split(',') if tag.strip()]
                            selected_tags.extend(custom_list)
                    
                    if selected_tags and st.button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏", type="primary"):
                        tag_analyzer = YouTubeTagAnalyzer(
                            serpapi_key if use_serpapi and serpapi_key else None,
                            cache
                        )
                        
                        with st.spinner("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ–≥–æ–≤..."):
                            tag_results = tag_analyzer.analyze_multiple_keywords(selected_tags[:10])
                        
                        if tag_results:
                            st.markdown("#### üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤")
                            
                            results_data = []
                            for result in tag_results:
                                results_data.append({
                                    '–¢–µ–≥': result.keyword,
                                    '–ü–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º': safe_format_number(result.search_volume),
                                    '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è': f"{result.competition_score}/100",
                                    '–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞': f"{result.seo_score}/100",
                                    '–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞': f"{result.overall_score}/100",
                                    '–°–ª–æ–∂–Ω–æ—Å—Ç—å': result.difficulty
                                })
                            
                            results_df = pd.DataFrame(results_data)
                            st.dataframe(results_df, use_container_width=True, hide_index=True)
                            
                            st.markdown("#### üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–µ–≥–∏")
                            
                            top_3 = tag_results[:3]
                            cols = st.columns(3)
                            
                            for i, result in enumerate(top_3):
                                with cols[i]:
                                    st.markdown(f"""
                                    <div class="metric-card">
                                    <h4>üèÜ #{i+1}: {result.keyword}</h4>
                                    <p><strong>–û—Ü–µ–Ω–∫–∞:</strong> {result.overall_score}/100</p>
                                    <p><strong>–û–±—ä–µ–º:</strong> {safe_format_number(result.search_volume)}</p>
                                    <p><strong>–°–ª–æ–∂–Ω–æ—Å—Ç—å:</strong> {result.difficulty}</p>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            st.markdown("#### üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã")
                            
                            if tag_results:
                                avg_competition = sum(r.competition_score for r in tag_results) / len(tag_results)
                                avg_seo_score = sum(r.seo_score for r in tag_results) / len(tag_results)
                                
                                insights = []
                                
                                if avg_competition < 40:
                                    insights.append("‚úÖ **–ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è** - –æ—Ç–ª–∏—á–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ö–æ–¥–∞ –≤ –Ω–∏—à—É")
                                elif avg_competition > 70:
                                    insights.append("‚ö†Ô∏è **–í—ã—Å–æ–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è** - –Ω—É–∂–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ")
                                
                                if avg_seo_score > 60:
                                    insights.append("üéØ **–•–æ—Ä–æ—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞** - –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –ø–ª–æ—Ö–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç —Ç–µ–≥–∏")
                                
                                best_tag = max(tag_results, key=lambda x: x.overall_score)
                                insights.append(f"üèÜ **–õ—É—á—à–∏–π —Ç–µ–≥**: '{best_tag.keyword}' (–æ—Ü–µ–Ω–∫–∞ {best_tag.overall_score}/100)")
                                
                                for insight in insights:
                                    st.markdown(insight)
                            
                            csv_tags = pd.DataFrame([{
                                'Keyword': r.keyword,
                                'Search_Volume': r.search_volume,
                                'Competition_Score': r.competition_score,
                                'SEO_Score': r.seo_score,
                                'Overall_Score': r.overall_score,
                                'Difficulty': r.difficulty
                            } for r in tag_results])
                            
                            csv_tags_export = csv_tags.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                "üì• –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (CSV)",
                                csv_tags_export,
                                f'tag_analysis_{keyword.replace(" ", "_")}.csv',
                                'text/csv'
                            )
                    
                    if not use_serpapi:
                        st.info("üí° **–°–æ–≤–µ—Ç**: –í–∫–ª—é—á–∏—Ç–µ SerpAPI –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤!")
                
                else:
                    st.warning("üè∑Ô∏è –¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                    st.markdown("""
                    - –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    - –ò–∑–º–µ–Ω–∏—Ç—å —Ç–µ–º—É –Ω–∞ –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—É—é
                    - –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ —Ç–µ–≥–∏ –≤ –ø–æ–ª–µ –≤—ã—à–µ
                    """)

            with tab3:
                if trends_data and 'interest_df' in trends_data and not trends_data['interest_df'].empty:
                    st.markdown("### üìà –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º—ã")
                    
                    interest_df = trends_data['interest_df']
                    
                    fig_trends = go.Figure()
                    fig_trends.add_trace(go.Scatter(
                        x=interest_df.index,
                        y=interest_df[keyword],
                        mode='lines+markers',
                        name='–ò–Ω—Ç–µ—Ä–µ—Å',
                        line=dict(color='#1f77b4', width=3),
                        marker=dict(size=6),
                        hovertemplate='<b>%{x}</b><br>–ò–Ω—Ç–µ—Ä–µ—Å: %{y}<extra></extra>'
                    ))
                    
                    fig_trends.update_layout(
                        title=f'–ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—É–ª—è—Ä–Ω–∞ —Ç–µ–º–∞: "{keyword}"',
                        xaxis_title='–î–∞—Ç–∞',
                        yaxis_title='–£—Ä–æ–≤–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–∞',
                        hovermode='x unified',
                        template='plotly_dark'
                    )
                    
                    st.plotly_chart(fig_trends, use_container_width=True)
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(
                            "üìä –¢–µ–Ω–¥–µ–Ω—Ü–∏—è",
                            trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        )
                    
                    with col2:
                        current_interest = trends_data.get('current_interest', 0)
                        st.metric(
                            "üéØ –ò–Ω—Ç–µ—Ä–µ—Å —Å–µ–π—á–∞—Å",
                            f"{current_interest:.0f}/100"
                        )
                    
                    with col3:
                        trend_strength = trends_data.get('trend_strength', 0) * 100
                        st.metric(
                            "‚ö° –°–∏–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π",
                            f"{trend_strength:.1f}%"
                        )
                    
                    if 'top_queries' in trends_data and not trends_data['top_queries'].empty:
                        st.markdown("### üîç –ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**")
                            top_queries = trends_data['top_queries'].head(10)
                            for idx, row in top_queries.iterrows():
                                st.write(f"‚Ä¢ {row['query']} ({row['value']}%)")
                        
                        with col2:
                            if 'rising_queries' in trends_data and not trends_data['rising_queries'].empty:
                                st.markdown("**üöÄ –†–∞—Å—Ç—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã:**")
                                rising_queries = trends_data['rising_queries'].head(10)
                                for idx, row in rising_queries.iterrows():
                                    growth = row['value']
                                    if growth == 'Breakout':
                                        growth = 'üî• –í–∑—Ä—ã–≤'
                                    st.write(f"‚Ä¢ {row['query']} (+{growth})")
                
                else:
                    st.warning("üìà –î–∞–Ω–Ω—ã–µ Google Trends –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ —Ç–µ–º–∞ —Å–ª–∏—à–∫–æ–º —É–∑–∫–∞—è")
                    st.markdown("""
                    **–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:**
                    - –¢–µ–º–∞ —Å–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤
                    - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å Google Trends API
                    - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    
                    **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
                    - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ –æ–±—â—É—é —Ç–µ–º—É
                    - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    - –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ
                    """)

            with tab4:
                st.markdown("### üèÜ –¢–æ–ø –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º")
                
                if not df.empty:
                    top_videos = df.nlargest(20, 'views')
                    
                    for idx, video in top_videos.iterrows():
                        with st.container():
                            col1, col2 = st.columns([1, 3])
                            
                            with col1:
                                if video.get('thumbnail'):
                                    st.image(video['thumbnail'], width=120)
                                else:
                                    st.write("üé¨")
                            
                            with col2:
                                st.markdown(f"""
                                **[{video['title']}]({video['video_url']})**
                                
                                üì∫ **{video['channel']}** ({safe_format_number(video['subscribers'])} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)
                                
                                üëÄ **{video['views_formatted']} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤** ‚Ä¢ 
                                üëç **{video['likes_formatted']} –ª–∞–π–∫–æ–≤** ‚Ä¢ 
                                üí¨ **{safe_format_number(video['comments'])} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤** ‚Ä¢ 
                                ‚è±Ô∏è **{video['duration_formatted']}** ‚Ä¢ 
                                {video['short_indicator']}
                                
                                üìÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {video['published'][:10]}
                                """)
                            
                            st.markdown("---")
                    
                    st.markdown("### üìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø –≤–∏–¥–µ–æ")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        avg_views_top = top_videos['views'].mean()
                        st.metric(
                            "–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–ø-20",
                            safe_format_number(avg_views_top)
                        )
                    
                    with col2:
                        shorts_in_top = (top_videos['is_short'].sum() / len(top_videos)) * 100
                        st.metric(
                            "% Shorts –≤ —Ç–æ–ø–µ",
                            f"{shorts_in_top:.0f}%"
                        )
                    
                    with col3:
                        avg_engagement_top = ((top_videos['likes'] + top_videos['comments']) / top_videos['views']).mean() * 100
                        st.metric(
                            "–í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å —Ç–æ–ø-20",
                            f"{avg_engagement_top:.1f}%"
                        )
                
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ø –≤–∏–¥–µ–æ")

            with tab5:
                st.markdown("### üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∏")
                
                if not df.empty:
                    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                    st.markdown("#### üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤")
                    
                    fig_views = px.histogram(
                        df, 
                        x='views', 
                        nbins=30,
                        title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ',
                        labels={'views': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ'}
                    )
                    fig_views.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_views, use_container_width=True)
                    
                    # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã vs –ª–∞–π–∫–∏
                    st.markdown("#### üíù –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ª–∞–π–∫–æ–≤ –æ—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤")
                    
                    fig_scatter = px.scatter(
                        df, 
                        x='views', 
                        y='likes',
                        hover_data=['title', 'channel'],
                        title='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º–∏ –∏ –ª–∞–π–∫–∞–º–∏',
                        labels={'views': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 'likes': '–õ–∞–π–∫–∏'}
                    )
                    fig_scatter.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_scatter, use_container_width=True)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º
                    st.markdown("#### üì∫ –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤")
                    
                    channel_stats = df.groupby('channel').agg({
                        'views': ['count', 'mean', 'max'],
                        'subscribers': 'first',
                        'likes': 'mean'
                    }).round(0)
                    
                    channel_stats.columns = ['–í–∏–¥–µ–æ', '–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã', '–ú–∞–∫—Å –ø—Ä–æ—Å–º–æ—Ç—Ä—ã', '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏', '–°—Ä–µ–¥–Ω–∏–µ –ª–∞–π–∫–∏']
                    channel_stats = channel_stats.sort_values('–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã', ascending=False)
                    
                    st.dataframe(channel_stats.head(15), use_container_width=True)
                    
                    # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
                    st.markdown("#### ‚è∞ –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
                    
                    df_time = df.copy()
                    df_time['published_date'] = pd.to_datetime(df_time['published']).dt.date
                    daily_stats = df_time.groupby('published_date').agg({
                        'views': ['count', 'mean']
                    })
                    
                    daily_stats.columns = ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ', '–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã']
                    daily_stats = daily_stats.reset_index()
                    
                    fig_time = px.line(
                        daily_stats, 
                        x='published_date', 
                        y='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ',
                        title='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –ø–æ –¥–Ω—è–º'
                    )
                    fig_time.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_time, use_container_width=True)
                    
                    # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    st.markdown("#### ‚è±Ô∏è –ê–Ω–∞–ª–∏–∑ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        duration_ranges = pd.cut(
                            df['duration'], 
                            bins=[0, 1, 5, 15, 60, float('inf')],
                            labels=['< 1 –º–∏–Ω (Shorts)', '1-5 –º–∏–Ω', '5-15 –º–∏–Ω', '15-60 –º–∏–Ω', '> 60 –º–∏–Ω']
                        )
                        duration_counts = duration_ranges.value_counts()
                        
                        fig_duration = px.pie(
                            values=duration_counts.values,
                            names=duration_counts.index,
                            title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'
                        )
                        fig_duration.update_layout(template='plotly_dark')
                        st.plotly_chart(fig_duration, use_container_width=True)
                    
                    with col2:
                        st.markdown("**üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**")
                        for duration_range, count in duration_counts.items():
                            percentage = (count / len(df)) * 100
                            avg_views = df[duration_ranges == duration_range]['views'].mean()
                            st.write(f"**{duration_range}**: {count} –≤–∏–¥–µ–æ ({percentage:.1f}%)")
                            st.write(f"–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {safe_format_number(avg_views)}")
                            st.write("---")
                    
                    # –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö
                    st.markdown("#### üì• –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
                    export_df = df[[
                        'title', 'channel', 'views', 'likes', 'comments', 
                        'duration_formatted', 'published', 'video_url'
                    ]].copy()
                    
                    export_df.columns = [
                        '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–ö–∞–Ω–∞–ª', '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', '–õ–∞–π–∫–∏', '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',
                        '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', 'URL'
                    ]
                    
                    csv_data = export_df.to_csv(index=False).encode('utf-8')
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.download_button(
                            "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV)",
                            csv_data,
                            f'youtube_analysis_{keyword.replace(" ", "_")}.csv',
                            'text/csv'
                        )
                    
                    with col2:
                        summary_data = pd.DataFrame([{
                            '–ú–µ—Ç—Ä–∏–∫–∞': '–í—Å–µ–≥–æ –≤–∏–¥–µ–æ',
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': len(df)
                        }, {
                            '–ú–µ—Ç—Ä–∏–∫–∞': '–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã',
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': int(df['views'].mean())
                        }, {
                            '–ú–µ—Ç—Ä–∏–∫–∞': '–ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã', 
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': int(df['views'].median())
                        }, {
                            '–ú–µ—Ç—Ä–∏–∫–∞': '–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤',
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': df['channel'].nunique()
                        }, {
                            '–ú–µ—Ç—Ä–∏–∫–∞': '–ü—Ä–æ—Ü–µ–Ω—Ç Shorts',
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': f"{(df['is_short'].mean() * 100):.1f}%"
                        }])
                        
                        summary_csv = summary_data.to_csv(index=False).encode('utf-8')
                        st.download_button(
                            "üìä –°–∫–∞—á–∞—Ç—å —Å–≤–æ–¥–∫—É (CSV)",
                            summary_csv,
                            f'youtube_summary_{keyword.replace(" ", "_")}.csv',
                            'text/csv'
                        )
                
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")

        except Exception as e:
            st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
            st.info("üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            st.markdown("""
            - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API –∫–ª—é—á–µ–π
            - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            - –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            - –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
            """)

if __name__ == "__main__":
    main()
