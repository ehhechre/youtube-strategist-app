# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
import time
from pytrends.request import TrendReq
import openai
import numpy as np
from collections import Counter
import requests
import json
from dataclasses import dataclass
from urllib.parse import quote_plus


# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–¢–ò–õ–ò ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide"
)
warnings.filterwarnings('ignore')

st.markdown("""
<style>
    .main-header { 
        font-size: 2.8rem; 
        color: #FF0000; 
        text-align: center; 
        margin-bottom: 2rem; 
        font-weight: bold; 
        background: linear-gradient(90deg, #FF0000 0%, #FF6B6B 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .stButton>button { 
        border-radius: 8px; 
        font-weight: bold; 
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .custom-container {
        background: linear-gradient(135deg, rgba(42, 57, 62, 0.5), rgba(62, 77, 82, 0.3));
        padding: 1.5rem; 
        border-radius: 15px;
        border-left: 5px solid #00a0dc; 
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .openai-result {
        background: linear-gradient(135deg, rgba(26, 142, 95, 0.1), rgba(46, 162, 115, 0.05));
        padding: 1.5rem; 
        border-radius: 15px;
        border-left: 5px solid #1a8e5f; 
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .insight-box {
        background: linear-gradient(135deg, #262730, #3a3b45);
        padding: 1rem; 
        border-radius: 15px; 
        margin-top: 1rem;
        border: 1px solid #444;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #1e1e2e, #2a2a3e);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #444;
        margin: 0.5rem 0;
    }
    .success-alert {
        background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(34, 197, 94, 0.05));
        border: 1px solid rgba(34, 197, 94, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background: linear-gradient(135deg, rgba(251, 146, 60, 0.1), rgba(251, 146, 60, 0.05));
        border: 1px solid rgba(251, 146, 60, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. –£–¢–ò–õ–ò–¢–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø ---

def validate_youtube_api_key(api_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ YouTube API –∫–ª—é—á–∞"""
    if not api_key:
        return False
    
    # YouTube API –∫–ª—é—á–∏ –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å AIza –∏ –∏–º–µ—é—Ç –¥–ª–∏–Ω—É 39 —Å–∏–º–≤–æ–ª–æ–≤
    if api_key.startswith('AIza') and len(api_key) == 39:
        return True
    
    # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ –∫–ª—é—á –¥–ª–∏–Ω–Ω—ã–π –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    if len(api_key) > 30 and all(c.isalnum() or c in '-_' for c in api_key):
        return True
    
    return False

def validate_openai_api_key(api_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ OpenAI API –∫–ª—é—á–∞"""
    if not api_key:
        return False
    
    # OpenAI –∫–ª—é—á–∏ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å sk-
    return api_key.startswith('sk-')

def format_number(num):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏"""
    if num >= 1_000_000:
        return f"{num/1_000_000:.1f}M"
    elif num >= 1_000:
        return f"{num/1_000:.1f}K"
    return str(num)

def extract_keywords_from_titles(titles: list) -> list:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    all_words = []
    stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–∑–∞', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '—É', '–∂–µ', '–µ—â–µ', '—É–∂–µ', '–∏–ª–∏', '—Ç–∞–∫', '–Ω–æ', '–∞', '–∏—Ö', '–µ–≥–æ', '–µ—ë', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '–∫–æ–≥–¥–∞', '–≥–¥–µ', 'why', 'how', 'what', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    for title in titles:
        words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', title.lower())
        words = [word for word in words if word not in stop_words]
        all_words.extend(words)
    
    word_counts = Counter(all_words)
    return word_counts.most_common(10)

# --- 3. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ ---

class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {
            'search': 3600*4,       # 4 —á–∞—Å–∞
            'channels': 3600*24*7,  # 7 –¥–Ω–µ–π
            'trends': 3600*8,       # 8 —á–∞—Å–æ–≤
            'openai': 3600*24       # 1 –¥–µ–Ω—å
        }
        self.stats = {'hits': 0, 'misses': 0}

    def _init_sqlite(self):
        with self.lock:
            try:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY, 
                        value BLOB, 
                        expires_at TIMESTAMP,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏—Å—Ç–µ—á–µ–Ω–∏—è
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)')
                conn.commit()
                conn.close()
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")

    def get(self, key: str):
        with self.lock:
            try:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("SELECT value, expires_at FROM cache WHERE key = ?", (key,))
                result = cursor.fetchone()
                conn.close()
                
                if result and datetime.fromisoformat(result[1]) > datetime.now():
                    self.stats['hits'] += 1
                    return pickle.loads(result[0])
                elif result:
                    self.delete(key)
                
                self.stats['misses'] += 1
                return None
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")
                self.stats['misses'] += 1
                return None

    def set(self, key: str, value: any, category: str):
        with self.lock:
            try:
                ttl = self.ttl_map.get(category, 3600)
                expires_at = datetime.now() + timedelta(seconds=ttl)
                value_blob = pickle.dumps(value)
                
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute(
                    "INSERT OR REPLACE INTO cache (key, value, expires_at) VALUES (?, ?, ?)",
                    (key, value_blob, expires_at.isoformat())
                )
                conn.commit()
                conn.close()
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")

    def delete(self, key: str):
        with self.lock:
            try:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("DELETE FROM cache WHERE key = ?", (key,))
                conn.commit()
                conn.close()
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: {e}")

    def clean_expired(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–µ–∫—à–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        with self.lock:
            try:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("DELETE FROM cache WHERE expires_at < ?", (datetime.now().isoformat(),))
                deleted_count = cursor.rowcount
                conn.commit()
                conn.close()
                return deleted_count
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
                return 0

    def generate_key(self, *args) -> str:
        return hashlib.md5("".join(map(str, args)).encode('utf-8')).hexdigest()

class YouTubeAnalyzer:
    def __init__(self, api_key: str, cache: CacheManager):
        try:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
            self.cache = cache
            self.api_key = api_key
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ YouTube API: {e}")
            raise

    def test_connection(self) -> bool:
        """–ú—è–≥–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API"""
        try:
            # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±—ä–µ–∫—Ç —Å–æ–∑–¥–∞–ª—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
            return hasattr(self.youtube, 'search')
        except Exception as e:
            st.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ API: {e}")
            return True  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, —á—Ç–æ–±—ã –ø–æ–∑–≤–æ–ª–∏—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å

    def get_channel_stats(self, channel_ids: list):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ (–≤–∫–ª—é—á–∞—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤) –ø–∞—á–∫–∞–º–∏ –ø–æ 50."""
        if not channel_ids:
            return {}
            
        cache_key = self.cache.generate_key('channels', sorted(channel_ids))
        if cached_data := self.cache.get(cache_key):
            return cached_data
        
        channel_stats = {}
        try:
            for i in range(0, len(channel_ids), 50):
                chunk_ids = channel_ids[i:i+50]
                request = self.youtube.channels().list(
                    part="statistics,snippet", 
                    id=",".join(chunk_ids)
                )
                response = request.execute()
                
                for item in response.get('items', []):
                    stats = item.get('statistics', {})
                    snippet = item.get('snippet', {})
                    
                    channel_stats[item['id']] = {
                        'subscribers': int(stats.get('subscriberCount', 0)),
                        'total_views': int(stats.get('viewCount', 0)),
                        'video_count': int(stats.get('videoCount', 0)),
                        'title': snippet.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                        'description': snippet.get('description', ''),
                        'published_at': snippet.get('publishedAt', '')
                    }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.1)
            
            self.cache.set(cache_key, channel_stats, 'channels')
            return channel_stats
            
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –∫–∞–Ω–∞–ª–∞—Ö: {e}")
            return {}

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        cache_key = self.cache.generate_key('search', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_snippets = []
            next_page_token = None
            search_params = {
                'q': keyword,
                'part': 'snippet',
                'type': 'video',
                'order': 'relevance',
                'regionCode': 'RU'
            }
            
            if published_after:
                search_params['publishedAfter'] = published_after

            progress_bar = st.progress(0)
            status_text = st.empty()

            while len(video_snippets) < max_results:
                search_params['maxResults'] = min(50, max_results - len(video_snippets))
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                status_text.text(f"üîç –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(video_snippets)}/{max_results}")
                progress_bar.progress(len(video_snippets) / max_results)
                
                search_response = self.youtube.search().list(**search_params).execute()
                new_items = search_response.get('items', [])
                
                if not new_items:
                    break
                    
                video_snippets.extend(new_items)
                next_page_token = search_response.get('nextPageToken')
                
                if not next_page_token:
                    break
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.1)

            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(video_snippets)} –≤–∏–¥–µ–æ")

            if not video_snippets:
                return []

            video_ids = [item['id']['videoId'] for item in video_snippets]
            channel_ids = list(set([item['snippet']['channelId'] for item in video_snippets]))

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–¥–µ–æ –∏ –∫–∞–Ω–∞–ª–æ–≤
            status_text.text("üìä –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É...")
            channel_stats = self.get_channel_stats(channel_ids)
            
            videos = []
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                stats_response = self.youtube.videos().list(
                    part='statistics,contentDetails,snippet', 
                    id=','.join(chunk_ids)
                ).execute()
                
                video_details_map = {item['id']: item for item in stats_response.get('items', [])}
                
                for snippet in video_snippets[i:i+50]:
                    video_id = snippet['id']['videoId']
                    details = video_details_map.get(video_id)
                    
                    if not details:
                        continue
                    
                    stats = details.get('statistics', {})
                    content_details = details.get('contentDetails', {})
                    video_snippet = details.get('snippet', {})
                    
                    duration = self._parse_duration(content_details.get('duration', 'PT0S'))
                    channel_id = snippet['snippet']['channelId']
                    channel_info = channel_stats.get(channel_id, {})
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏
                    tags = video_snippet.get('tags', [])
                    
                    video_data = {
                        'video_id': video_id,
                        'title': snippet['snippet']['title'],
                        'channel': snippet['snippet']['channelTitle'],
                        'channel_id': channel_id,
                        'subscribers': channel_info.get('subscribers', 0),
                        'channel_total_views': channel_info.get('total_views', 0),
                        'channel_video_count': channel_info.get('video_count', 0),
                        'published': snippet['snippet']['publishedAt'],
                        'views': int(stats.get('viewCount', 0)),
                        'likes': int(stats.get('likeCount', 0)),
                        'comments': int(stats.get('commentCount', 0)),
                        'duration': duration,
                        'is_short': duration <= 1.05,
                        'tags': tags,
                        'description': video_snippet.get('description', ''),
                        'definition': content_details.get('definition', 'sd').upper(),
                        'category_id': video_snippet.get('categoryId', ''),
                        'language': video_snippet.get('defaultLanguage', 'ru')
                    }
                    videos.append(video_data)
                
                time.sleep(0.1)
            
            progress_bar.empty()
            status_text.empty()
            
            self.cache.set(cache_key, videos, 'search')
            return videos
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {e}")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ –≤ –º–∏–Ω—É—Ç–∞—Ö"""
        if not duration_str:
            return 0
            
        match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
        if not match:
            return 0
            
        h, m, s = (int(g or 0) for g in match.groups())
        return h * 60 + m + s / 60
    
    def analyze_competition(self, videos: list):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        if not videos:
            return {}, pd.DataFrame()
            
        df = pd.DataFrame(videos)
        df['published'] = pd.to_datetime(df['published'], errors='coerce').dt.tz_localize(None)
        df['views'] = df['views'].replace(0, 1)
        df['days_ago'] = (datetime.now() - df['published']).dt.days
        df['engagement_rate'] = ((df['likes'] + df['comments']) / df['views']) * 100
        df['views_per_subscriber'] = df['views'] / (df['subscribers'] + 1)

        # –ö–≤–∞—Ä—Ç–∏–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        view_quartiles = df['views'].quantile([0.25, 0.5, 0.75])
        
        analysis = {
            'total_videos': len(df),
            'avg_views': df['views'].mean(),
            'median_views': df['views'].median(),
            'top_10_avg_views': df.nlargest(10, 'views')['views'].mean(),
            'top_25_percent_views': view_quartiles[0.75],
            'engagement_rate': df['engagement_rate'].mean(),
            'videos_last_week': len(df[df['days_ago'] <= 7]),
            'videos_last_month': len(df[df['days_ago'] <= 30]),
            'shorts_percentage': df['is_short'].mean() * 100 if not df.empty else 0,
            'avg_days_to_top_10': df.nlargest(10, 'views')['days_ago'].mean() if not df.empty else 0,
            'unique_channels': df['channel'].nunique(),
            'avg_channel_subscribers': df['subscribers'].mean(),
            'avg_duration': df[~df['is_short']]['duration'].mean(),
            'hd_percentage': (df['definition'] == 'HD').mean() * 100 if 'definition' in df.columns else 0
        }

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ (—É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞)
        score = 0
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
        if analysis['top_10_avg_views'] < 30000:
            score += 3
        elif analysis['top_10_avg_views'] < 100000:
            score += 2
        elif analysis['top_10_avg_views'] < 500000:
            score += 1
        
        # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if analysis['videos_last_week'] < 3:
            score += 2
        elif analysis['videos_last_week'] < 10:
            score += 1
        
        # –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤
        if analysis['unique_channels'] < 20:
            score += 1
        
        # –ê–Ω–∞–ª–∏–∑ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        if analysis['engagement_rate'] < 2:
            score += 1
        
        competition_levels = {
            0: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
            1: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
            2: '–í—ã—Å–æ–∫–∞—è üü†',
            3: '–í—ã—Å–æ–∫–∞—è üü†',
            4: '–°—Ä–µ–¥–Ω—è—è üü°',
            5: '–°—Ä–µ–¥–Ω—è—è üü°',
            6: '–ù–∏–∑–∫–∞—è üü¢',
            7: '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢'
        }
        
        analysis['competition_level'] = competition_levels.get(score, '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥')
        analysis['competition_score'] = score
        
        return analysis, df

class AdvancedTrendsAnalyzer:
    def __init__(self, cache: CacheManager):
        self.cache = cache
        
    def _get_pytrends(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ pytrends —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return TrendReq(hl='ru-RU', tz=180, timeout=(10, 25), retries=2, backoff_factor=0.1)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Trends: {e}")
            return None

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
            
        pytrends = self._get_pytrends()
        if not pytrends:
            return None
            
        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–¥ –∑–∞ 12 –º–µ—Å—è—Ü–µ–≤
            pytrends.build_payload([keyword], timeframe='today 12-m', geo='RU')
            interest_12m = pytrends.interest_over_time()
            
            if interest_12m.empty:
                return None
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞ 5 –ª–µ—Ç –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞
            try:
                pytrends.build_payload([keyword], timeframe='today 5-y', geo='RU')
                interest_5y = pytrends.interest_over_time()
            except:
                interest_5y = pd.DataFrame()
            
            # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            try:
                related_queries = pytrends.related_queries()
                rising_queries = related_queries.get(keyword, {}).get('rising', pd.DataFrame())
                top_queries = related_queries.get(keyword, {}).get('top', pd.DataFrame())
            except:
                rising_queries = pd.DataFrame()
                top_queries = pd.DataFrame()
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
            series = interest_12m[keyword]
            recent_avg = series.tail(4).mean()  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 4 –Ω–µ–¥–µ–ª–∏
            previous_avg = series.iloc[-8:-4].mean()  # –ü—Ä–µ–¥—ã–¥—É—â–∏–µ 4 –Ω–µ–¥–µ–ª–∏
            overall_avg = series.mean()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
            if recent_avg > previous_avg * 1.2:
                trend_direction = "–ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—É—â–∏–π üöÄ"
            elif recent_avg > previous_avg * 1.1:
                trend_direction = "–†–∞—Å—Ç—É—â–∏–π üìà"
            elif recent_avg < previous_avg * 0.8:
                trend_direction = "–ü–∞–¥–∞—é—â–∏–π üìâ"
            elif recent_avg < previous_avg * 0.9:
                trend_direction = "–°–ª–∞–±–æ –ø–∞–¥–∞—é—â–∏–π üìâ"
            else:
                trend_direction = "–°—Ç–∞–±–∏–ª—å–Ω—ã–π ‚û°Ô∏è"
            
            # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑)
            monthly_avg = series.groupby(series.index.month).mean()
            peak_months = monthly_avg.nlargest(3).index.tolist()
            
            result = {
                'interest_df': interest_12m,
                'interest_5y_df': interest_5y,
                'trend_direction': trend_direction,
                'recent_avg': recent_avg,
                'overall_avg': overall_avg,
                'trend_strength': abs(recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0,
                'rising_queries': rising_queries,
                'top_queries': top_queries,
                'peak_months': peak_months,
                'current_interest': series.iloc[-1] if not series.empty else 0
            }
            
            self.cache.set(cache_key, result, 'trends')
            return result
            
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

class ContentStrategist:
    def __init__(self, openai_key=None, openai_model=None):
        self.use_openai = bool(openai_key and openai_model)
        if self.use_openai:
            self.client = openai.OpenAI(api_key=openai_key)
            self.model = openai_model

    def get_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame, cache: CacheManager):
        cache_key = None
        if self.use_openai:
            cache_key = cache.generate_key('openai_v3', keyword, self.model, str(comp_analysis)[:100])
            if cached_strategy := cache.get(cache_key):
                st.toast("ü§ñ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞!", icon="üß†")
                return cached_strategy
        
        if self.use_openai:
            strategy = self._get_ai_strategy(keyword, comp_analysis, trends_data, df)
        else:
            strategy = self._get_rule_based_strategy(keyword, comp_analysis, df)
        
        if self.use_openai and cache_key and "–û—à–∏–±–∫–∞" not in strategy:
            cache.set(cache_key, strategy, 'openai')
        
        return strategy

    def _get_rule_based_strategy(self, keyword: str, comp_analysis: dict, df: pd.DataFrame):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–µ–∑ AI"""
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        if not df.empty:
            titles = df['title'].tolist()
            popular_words = extract_keywords_from_titles(titles)
            top_words = [word for word, count in popular_words[:5]]
        else:
            top_words = []
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        competition_level = comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        avg_views = comp_analysis.get('avg_views', 0)
        shorts_percentage = comp_analysis.get('shorts_percentage', 0)
        
        strategy_parts = []
        
        # –í–µ—Ä–¥–∏–∫—Ç
        if '–Ω–∏–∑–∫–∞—è' in competition_level.lower():
            verdict = "üéØ **–û–¢–õ–ò–ß–ù–ê–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–¨!** –ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —à–∞–Ω—Å—ã –¥–ª—è —Ä–æ—Å—Ç–∞."
        elif '—Å—Ä–µ–¥–Ω—è—è' in competition_level.lower():
            verdict = "‚ö° **–•–û–†–û–®–ò–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–´** —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º. –ù—É–∂–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è."
        else:
            verdict = "üî• **–í–´–°–û–ö–ê–Ø –ö–û–ù–ö–£–†–ï–ù–¶–ò–Ø** - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
        
        strategy_parts.append(f"### üéØ –í–µ—Ä–¥–∏–∫—Ç\n{verdict}")
        
        # –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        insights = []
        if avg_views < 50000:
            insights.append("üí° –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ–≤—ã—Å–æ–∫–∏–µ - –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–æ–º")
        if shorts_percentage > 50:
            insights.append("üì± –ú–Ω–æ–≥–æ Shorts –≤ –Ω–∏—à–µ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç")
        if top_words:
            insights.append(f"üî§ –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {', '.join(top_words[:3])}")
        
        strategy_parts.append("### üîç –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã\n" + "\n".join(insights))
        
        # –ò–¥–µ–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_ideas = [
            f"**–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword}** - –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
            f"**–¢–æ–ø-5 –æ—à–∏–±–æ–∫ –≤ {keyword}** - —Ä–∞–∑–±–æ—Ä —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º",
            f"**{keyword}: –¥–æ –∏ –ø–æ—Å–ª–µ** - –∫–µ–π—Å—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
            f"**–ö–∞–∫ –Ω–∞—á–∞—Ç—å –≤ {keyword} –±–µ–∑ –æ–ø—ã—Ç–∞** - –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω",
            f"**–°–µ–∫—Ä–µ—Ç—ã {keyword}, –æ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –≥–æ–≤–æ—Ä—è—Ç** - –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        ]
        
        if shorts_percentage > 30:
            content_ideas.extend([
                f"**{keyword} –∑–∞ 60 —Å–µ–∫—É–Ω–¥** - –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±—É—á–∞—é—â–∏–µ –≤–∏–¥–µ–æ",
                f"**–ë—ã—Å—Ç—Ä—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ {keyword}** - —Å–µ—Ä–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–æ–ª–∏–∫–æ–≤"
            ])
        
        strategy_parts.append("### üí° –ò–¥–µ–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n" + "\n".join(content_ideas))
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimization_tips = [
            "üé® **–Ø—Ä–∫–∏–µ –ø—Ä–µ–≤—å—é** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Ü–≤–µ—Ç–∞ –∏ —á–µ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
            "‚è∞ **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏** - —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ 18:00-21:00 –ø–æ –ú–°–ö",
            "üéØ **–¶–µ–ø–ª—è—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–∏—Å–ª–∞, –≤–æ–ø—Ä–æ—Å—ã, –∏–Ω—Ç—Ä–∏–≥—É",
            "üìù **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è** - –¥–æ–±–∞–≤—å—Ç–µ —Ç–∞–π–º-–∫–æ–¥—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏",
            "üè∑Ô∏è **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–≥–∏** - –º–∏–∫—Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏ –Ω–∏—à–µ–≤—ã—Ö —Ç–µ–≥–æ–≤"
        ]
        
        strategy_parts.append("### üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n" + "\n".join(optimization_tips))
        
        return "\n\n".join(strategy_parts)
    
    def _get_ai_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame):
        st.toast("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ OpenAI...", icon="üß†")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        top_titles = []
        top_channels = []
        if not df.empty:
            top_videos = df.nlargest(10, 'views')
            top_titles = top_videos['title'].tolist()
            top_channels = top_videos['channel'].value_counts().head(5).to_dict()
        
        trends_info = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        if trends_data:
            trends_info = f"{trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
            if 'recent_avg' in trends_data:
                trends_info += f" (—Ç–µ–∫—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å: {trends_data['recent_avg']:.0f})"
        
        prompt = f"""
        –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π YouTube-—Å—Ç—Ä–∞—Ç–µ–≥ —Å –æ–ø—ã—Ç–æ–º –±–æ–ª–µ–µ 10 –ª–µ—Ç. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∏—à–∏ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è.

        **–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–ê–Ø –¢–ï–ú–ê:** "{keyword}"

        **–î–ê–ù–ù–´–ï –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:**
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ: {comp_analysis.get('total_videos', 0)}
        - –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
        - –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('avg_views', 0)):,}
        - –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('median_views', 0)):,}
        - –ü—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–ø-10: {int(comp_analysis.get('top_10_avg_views', 0)):,}
        - –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: {comp_analysis.get('engagement_rate', 0):.2f}%
        - –ü—Ä–æ—Ü–µ–Ω—Ç Shorts: {comp_analysis.get('shorts_percentage', 0):.1f}%
        - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {comp_analysis.get('unique_channels', 0)}
        - –í–∏–¥–µ–æ –∑–∞ –Ω–µ–¥–µ–ª—é: {comp_analysis.get('videos_last_week', 0)}

        **–¢–†–ï–ù–î–´ GOOGLE:**
        {trends_info}

        **–¢–û–ü-5 –ó–ê–ì–û–õ–û–í–ö–û–í –ö–û–ù–ö–£–†–ï–ù–¢–û–í:**
        {chr(10).join(f"‚Ä¢ {title}" for title in top_titles[:5])}

        **–í–ï–î–£–©–ò–ï –ö–ê–ù–ê–õ–´:**
        {chr(10).join(f"‚Ä¢ {channel}: {count} –≤–∏–¥–µ–æ" for channel, count in list(top_channels.items())[:3])}

        **–ó–ê–î–ê–ù–ò–ï:**
        –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:

        1. **üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–¥–∏–∫—Ç** - –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ –Ω–∏—à–∏ –∏ –≥–ª–∞–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        2. **üìä –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π** - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω –Ω–∏—à–∏

        3. **üé¨ –ö–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è** - 7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π –¥–ª—è –≤–∏–¥–µ–æ —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏:
           - –ó–∞–≥–æ–ª–æ–≤–æ–∫
           - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
           - –§–æ—Ä–º–∞—Ç (—Ç—É—Ç–æ—Ä–∏–∞–ª/–æ–±–∑–æ—Ä/–∫–µ–π—Å/–∏ —Ç.–¥.)
           - –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

        4. **üöÄ –¢–∞–∫—Ç–∏–∫–∞ —Ä–æ—Å—Ç–∞** - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 30 –¥–Ω–µ–π

        5. **üí∞ –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è** - 3 —Å–ø–æ—Å–æ–±–∞ –∑–∞—Ä–∞–±–æ—Ç–∫–∞ + –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏

        6. **üè∑Ô∏è SEO –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ–≥–∞–º, –ø—Ä–µ–≤—å—é, –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

        –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ actionable —Å–æ–≤–µ—Ç–∞—Ö.
        """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

# --- 4. –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–ï–ì–û–í ---

@dataclass
class TagScore:
    keyword: str
    search_volume: int
    competition_score: int  # 0-100 (0 = low, 100 = high)
    seo_score: int         # 0-100 
    overall_score: int     # 0-100
    difficulty: str        # "Very Low", "Low", "Medium", "High", "Very High"

class YouTubeTagAnalyzer:
    def __init__(self, serpapi_key: str = None, cache: CacheManager = None):
        self.serpapi_key = serpapi_key
        self.use_serpapi = bool(serpapi_key)
        self.cache = cache
        self.base_serpapi = "https://serpapi.com/search"
        
    def get_search_volume_serpapi(self, keyword: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._estimate_search_volume_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_volume', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
                'gl': 'us',
                'hl': 'en'
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            data = response.json()
            
            # –≠—Å—Ç–∏–º–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if 'video_results' in data:
                result_count = len(data['video_results'])
                volume = min(result_count * 150, 100000)  # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
                
                if cache_key and self.cache:
                    self.cache.set(cache_key, volume, 'search')
                return volume
                
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ SerpAPI –¥–ª—è '{keyword}': {e}")
            
        return self._estimate_search_volume_basic(keyword)
    
    def _estimate_search_volume_basic(self, keyword: str) -> int:
        """–ë–∞–∑–æ–≤–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        word_count = len(keyword.split())
        char_count = len(keyword)
        
        # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞
        base_volume = max(1000, 5000 - (word_count * 500) - (char_count * 10))
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
        popular_words = {
            '–∫–∞–∫', '—á—Ç–æ', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–æ–±–∑–æ—Ä', '—É—Ä–æ–∫', '—Ç—É—Ç–æ—Ä–∏–∞–ª',
            'guide', 'tutorial', 'how', 'what', 'review', 'tips'
        }
        
        bonus = sum(300 for word in keyword.lower().split() if word in popular_words)
        
        return min(base_volume + bonus, 50000)
    
    def analyze_competition_serpapi(self, keyword: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._analyze_competition_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_competition', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
                'gl': 'us',
                'hl': 'en'
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            data = response.json()
            
            if 'video_results' not in data:
                return self._analyze_competition_basic(keyword)
            
            videos = data['video_results'][:20]
            analysis = self._process_competition_data(videos, keyword)
            
            if cache_key and self.cache:
                self.cache.set(cache_key, analysis, 'search')
            
            return analysis
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
            return self._analyze_competition_basic(keyword)
    
    def _analyze_competition_basic(self, keyword: str) -> dict:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API)"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        word_count = len(keyword.split())
        
        if word_count == 1:
            competition_level = "High"
            optimized_ratio = 0.8
        elif word_count == 2:
            competition_level = "Medium"
            optimized_ratio = 0.6
        else:
            competition_level = "Low"
            optimized_ratio = 0.4
        
        return {
            'total_videos': 20,
            'optimized_titles': int(20 * optimized_ratio),
            'high_view_videos': max(1, int(20 * (1 - optimized_ratio))),
            'verified_channels': max(1, int(20 * 0.3)),
            'avg_views': 15000 if word_count == 1 else 8000,
            'keyword_in_title': int(20 * optimized_ratio),
            'recent_videos': 3,
            'competition_level': competition_level
        }
    
    def _process_competition_data(self, videos: list, keyword: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        analysis = {
            'total_videos': len(videos),
            'optimized_titles': 0,
            'high_view_videos': 0,
            'verified_channels': 0,
            'avg_views': 0,
            'keyword_in_title': 0,
            'recent_videos': 0
        }
        
        total_views = 0
        keyword_lower = keyword.lower()
        
        for video in videos:
            title = video.get('title', '').lower()
            if any(word in title for word in keyword_lower.split()):
                analysis['optimized_titles'] += 1
                analysis['keyword_in_title'] += 1
            
            views = self._extract_views(video.get('views', '0'))
            total_views += views
            
            if views > 100000:
                analysis['high_view_videos'] += 1
            
            channel = video.get('channel', {})
            if 'verified' in str(channel).lower():
                analysis['verified_channels'] += 1
            
            published = video.get('published_date', '')
            if self._is_recent(published):
                analysis['recent_videos'] += 1
        
        if analysis['total_videos'] > 0:
            analysis['avg_views'] = total_views // analysis['total_videos']
        
        return analysis
    
    def _extract_views(self, views_str: str) -> int:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"""
        if not views_str:
            return 0
        
        clean = ''.join(c for c in str(views_str) if c.isdigit() or c in [',', '.'])
        try:
            number_str = clean.replace(',', '').replace('.', '')
            return int(number_str) if number_str else 0
        except:
            return 0
    
    def _is_recent(self, date_str: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –≤–∏–¥–µ–æ"""
        if not date_str:
            return False
        
        recent_indicators = ['day', 'days', 'week', 'weeks', 'hour', 'hours']
        return any(indicator in date_str.lower() for indicator in recent_indicators)
    
    def calculate_scores(self, keyword: str, analysis: dict, search_volume: int) -> TagScore:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ç–µ–≥–∞"""
        # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
        total = analysis['total_videos']
        if total == 0:
            competition_score = 50
        else:
            optimized_ratio = analysis['optimized_titles'] / total
            high_views_ratio = analysis['high_view_videos'] / total
            verified_ratio = analysis['verified_channels'] / total
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–∏—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
            avg_views_factor = min(analysis['avg_views'] / 500000, 1.0)
            
            competition_score = min(int((
                optimized_ratio * 0.3 +
                high_views_ratio * 0.25 +
                verified_ratio * 0.2 +
                avg_views_factor * 0.25
            ) * 100), 100)
        
        # SEO –æ—Ü–µ–Ω–∫–∞ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤)
        if total > 0:
            keyword_optimization = analysis['keyword_in_title'] / total
            seo_score = max(int((1.0 - keyword_optimization) * 100), 10)
        else:
            seo_score = 50
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        import math
        volume_score = min(math.log10(max(search_volume, 1)) * 25, 100)
        competition_inverted = 100 - competition_score
        
        overall_score = min(int(
            volume_score * 0.4 +
            competition_inverted * 0.35 +
            seo_score * 0.25
        ), 100)
        
        # –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if competition_score <= 20:
            difficulty = "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 40:
            difficulty = "–ù–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 60:
            difficulty = "–°—Ä–µ–¥–Ω—è—è üü°"
        elif competition_score <= 80:
            difficulty = "–í—ã—Å–æ–∫–∞—è üü†"
        else:
            difficulty = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥"
        
        return TagScore(
            keyword=keyword,
            search_volume=search_volume,
            competition_score=competition_score,
            seo_score=seo_score,
            overall_score=overall_score,
            difficulty=difficulty
        )
    
    def analyze_keyword(self, keyword: str) -> TagScore:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        search_volume = self.get_search_volume_serpapi(keyword)
        competition_analysis = self.analyze_competition_serpapi(keyword)
        
        return self.calculate_scores(keyword, competition_analysis, search_volume)
    
    def analyze_multiple_keywords(self, keywords: list) -> list:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, keyword in enumerate(keywords):
            try:
                status_text.text(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–≥: {keyword}")
                progress_bar.progress((i + 1) / len(keywords))
                
                result = self.analyze_keyword(keyword)
                results.append(result)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if self.use_serpapi:
                    time.sleep(1)
                    
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ '{keyword}': {e}")
                continue
        
        progress_bar.empty()
        status_text.empty()
        
        return sorted(results, key=lambda x: x.overall_score, reverse=True)

def validate_serpapi_key(api_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ SerpAPI –∫–ª—é—á–∞"""
    if not api_key:
        return False
    
    # SerpAPI –∫–ª—é—á–∏ –æ–±—ã—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–µ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
    if len(api_key) > 30 and all(c.isalnum() for c in api_key):
        return True
    
    return False

# --- 5. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ---

def main():
    st.markdown('<h1 class="main-header">YouTube AI Strategist üß†</h1>', unsafe_allow_html=True)
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        
        # YouTube API
        st.subheader("üîë YouTube API")
        youtube_api_key = st.text_input(
            "YouTube API Key", 
            type="password",
            help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –≤ Google Cloud Console"
        )
        
        if youtube_api_key:
            if validate_youtube_api_key(youtube_api_key):
                st.success("‚úÖ YouTube API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
                st.info("üí° YouTube –∫–ª—é—á–∏ –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'AIza...' –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç 39 —Å–∏–º–≤–æ–ª–æ–≤")
        
        st.markdown("---")
        
        # SerpAPI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤
        st.subheader("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤")
        use_serpapi = st.toggle("–í–∫–ª—é—á–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (SerpAPI)", value=False)
        
        serpapi_key = ""
        if use_serpapi:
            serpapi_key = st.text_input(
                "SerpAPI Key", 
                type="password",
                help="–ö–ª—é—á –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"
            )
            
            if serpapi_key:
                if validate_serpapi_key(serpapi_key):
                    st.success("‚úÖ SerpAPI –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                else:
                    st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
            
            st.info("üí° SerpAPI –¥–∞–µ—Ç 100 –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü")
        
        st.markdown("---")
        
        # OpenAI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        st.subheader("ü§ñ AI-—Å—Ç—Ä–∞—Ç–µ–≥")
        use_openai = st.toggle("–í–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑ (OpenAI)", value=True)
        
        openai_api_key = ""
        openai_model = "gpt-4o-mini"
        
        if use_openai:
            openai_api_key = st.text_input(
                "OpenAI API Key", 
                type="password",
                help="–ö–ª—é—á –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å—Ç—Ä–∞—Ç–µ–≥–∏–π"
            )
            
            if openai_api_key:
                if validate_openai_api_key(openai_api_key):
                    st.success("‚úÖ OpenAI API –∫–ª—é—á –≤–∞–ª–∏–¥–µ–Ω")
                else:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π OpenAI API –∫–ª—é—á")
                    st.info("üí° –ö–ª—é—á –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'sk-'")
            
            openai_model = st.selectbox(
                "–ú–æ–¥–µ–ª—å OpenAI", 
                ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"], 
                index=1,
                help="gpt-4o-mini - –±—ã—Å—Ç—Ä–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ"
            )
        
        st.markdown("---")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
        st.subheader("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 20, 200, 100, 10)
        
        date_range_options = {
            "–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None,
            "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365,
            "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180,
            "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90,
            "–ó–∞ –º–µ—Å—è—Ü": 30
        }
        
        selected_date_range = st.selectbox(
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:", 
            list(date_range_options.keys()), 
            index=1
        )
        days_limit = date_range_options[selected_date_range]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–µ–π
        if not youtube_api_key:
            st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
            st.info("üìö [–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á](https://developers.google.com/youtube/v3/getting-started)")
            st.stop()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        cache = CacheManager()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        st.markdown("---")
        st.subheader("üíæ –ö—ç—à")
        st.info(f"**–ü–æ–ø–∞–¥–∞–Ω–∏—è:** {cache.stats['hits']} | **–ü—Ä–æ–º–∞—Ö–∏:** {cache.stats['misses']}")
        
        if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à"):
            deleted = cache.clean_expired()
            st.success(f"–£–¥–∞–ª–µ–Ω–æ {deleted} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π")
        
        # –ö–æ–Ω—Ç–∞–∫—Ç—ã
        st.markdown("---")
        st.subheader("üë®‚Äçüíª –ê–≤—Ç–æ—Ä")
        st.markdown("""
        **–°–≤—è–∑–∞—Ç—å—Å—è:**
        - üí¨ [Telegram](https://t.me/i_gma)
        - üì¢ [–ö–∞–Ω–∞–ª –æ AI](https://t.me/igm_a)
        - üîó [GitHub](https://github.com/yourusername)
        """)

    # –ì–ª–∞–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    st.markdown("### üéØ –í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        keyword = st.text_input(
            "",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã...",
            help="–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ YouTube –Ω–∏—à–∏"
        )
    
    with col2:
        analyze_button = st.button(
            "üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!", 
            type="primary", 
            use_container_width=True,
            disabled=not keyword
        )

    # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    st.markdown("**üí° –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**")
    example_cols = st.columns(3)
    
    examples = [
        "python –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
        "–º–æ–Ω—Ç–∞–∂ –≤–∏–¥–µ–æ",
        "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –∞–∫—Ü–∏–∏"
    ]
    
    for i, example in enumerate(examples):
        if example_cols[i % 3].button(f"üìå {example}", key=f"example_{i}"):
            keyword = example
            analyze_button = True

    # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
    if analyze_button and keyword:
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
            analyzer = YouTubeAnalyzer(youtube_api_key, cache)
            trends_analyzer = AdvancedTrendsAnalyzer(cache)
            
            # –ú—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–Ω–µ –±–ª–æ–∫–∏—Ä—É—é—â–∞—è)
            analyzer.test_connection()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–Ω–Ω–µ—Ä–∞
            spinner_text = "üåä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é YouTube..."
            if use_openai and openai_api_key and validate_openai_api_key(openai_api_key):
                spinner_text += " –ü—Ä–∏–≤–ª–µ–∫–∞—é AI..."

            with st.spinner(spinner_text):
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç—ã
                published_after_date = None
                if days_limit:
                    published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z"
                
                # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ
                videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
                
                if videos is None:
                    st.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ YouTube API")
                    st.stop()
                
                if not videos:
                    st.warning("üîç –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –ø–æ –¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                    st.markdown("""
                    - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                    - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
                    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                    """)
                    st.stop()
                
                # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
                comp_analysis, df = analyzer.analyze_competition(videos)
                
                # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
                trends_data = trends_analyzer.analyze_keyword_trends(keyword)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
                strategist = ContentStrategist(
                    openai_api_key if use_openai and validate_openai_api_key(openai_api_key) else None,
                    openai_model if use_openai else None
                )
                strategy_output = strategist.get_strategy(keyword, comp_analysis, trends_data, df, cache)

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.markdown("---")
            st.markdown(f"# üìä –ê–Ω–∞–ª–∏–∑ –Ω–∏—à–∏: **{keyword}**")
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            st.markdown("### üéØ –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    "üìπ –í–∏–¥–µ–æ", 
                    f"{len(df)}",
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ"
                )
            
            with col2:
                competition_level = comp_analysis['competition_level']
                st.metric(
                    "üèÜ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", 
                    competition_level.split()[0],
                    help=f"–ü–æ–ª–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {competition_level}"
                )
            
            with col3:
                avg_views = comp_analysis['avg_views']
                st.metric(
                    "üëÄ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", 
                    format_number(int(avg_views)),
                    help=f"–¢–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(avg_views):,}"
                )
            
            with col4:
                engagement = comp_analysis['engagement_rate']
                st.metric(
                    "üí¨ –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å", 
                    f"{engagement:.1f}%",
                    help="–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ª–∞–π–∫–æ–≤ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º"
                )
            
            with col5:
                channels = comp_analysis['unique_channels']
                st.metric(
                    "üì∫ –ö–∞–Ω–∞–ª–æ–≤", 
                    channels,
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ"
                )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            st.markdown("### üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üî• –¢–æ–ø-10 –≤–∏–¥–µ–æ",
                    format_number(int(comp_analysis['top_10_avg_views'])),
                    help="–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –ª—É—á—à–∏—Ö 10 –≤–∏–¥–µ–æ"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col2:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üì± Shorts",
                    f"{comp_analysis['shorts_percentage']:.0f}%",
                    help="–ü—Ä–æ—Ü–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ (–¥–æ 1 –º–∏–Ω—É—Ç—ã)"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col3:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üóìÔ∏è –ó–∞ –Ω–µ–¥–µ–ª—é",
                    f"{comp_analysis['videos_last_week']} —à—Ç.",
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col4:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                if 'avg_duration' in comp_analysis and comp_analysis['avg_duration'] > 0:
                    duration_str = f"{comp_analysis['avg_duration']:.1f} –º–∏–Ω"
                else:
                    duration_str = "N/A"
                st.metric(
                    "‚è±Ô∏è –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞",
                    duration_str,
                    help="–°—Ä–µ–¥–Ω—è—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ (–±–µ–∑ Shorts)"
                )
                st.markdown('</div>', unsafe_allow_html=True)

            # –¢–∞–±—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üéØ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è", 
                "üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤",
                "üìà –¢—Ä–µ–Ω–¥—ã", 
                "üèÜ –¢–æ–ø –≤–∏–¥–µ–æ", 
                "üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞"
            ])

            with tab1:
                css_class = "openai-result" if strategist.use_openai else "custom-container"
                st.markdown(f'<div class="{css_class}">', unsafe_allow_html=True)
                st.markdown(strategy_output)
                st.markdown('</div>', unsafe_allow_html=True)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
                if not df.empty:
                    st.markdown("### üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã")
                    
                    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å–ª–æ–≤
                    titles = df['title'].tolist()
                    popular_words = extract_keywords_from_titles(titles)
                    
                    if popular_words:
                        st.markdown("**üè∑Ô∏è –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö:**")
                        words_cols = st.columns(5)
                        for i, (word, count) in enumerate(popular_words[:5]):
                            words_cols[i].metric(word, count)
                    
                    # –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤
                    top_channels = df.nlargest(20, 'views').groupby('channel').agg({
                        'views': 'mean',
                        'subscribers': 'first',
                        'video_id': 'count'
                    }).round(0).sort_values('views', ascending=False)
                    
                    if not top_channels.empty:
                        st.markdown("**üì∫ –í–µ–¥—É—â–∏–µ –∫–∞–Ω–∞–ª—ã –≤ –Ω–∏—à–µ:**")
                        st.dataframe(
                            top_channels.head(10).rename(columns={
                                'views': '–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã',
                                'subscribers': '–ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤',
                                'video_id': '–í–∏–¥–µ–æ –≤ –≤—ã–±–æ—Ä–∫–µ'
                            }),
                            use_container_width=True
                        )

            with tab2:
                st.markdown("### üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ
                all_tags = []
                for video in videos:
                    if 'tags' in video and video['tags']:
                        all_tags.extend(video['tags'])
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏
                title_words = []
                for video in videos:
                    words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', video['title'].lower())
                    title_words.extend(words)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                stop_words = {'–∫–∞–∫', '—á—Ç–æ', '–¥–ª—è', '—ç—Ç–æ', '–≤—Å–µ', '–µ—â–µ', '–≥–¥–µ', '—Ç–∞–∫', '–∏–ª–∏', '—É–∂–µ', '–ø—Ä–∏', '–µ–≥–æ', '–æ–Ω–∏', '–±—ã–ª', 'the', 'and', 'for', 'you', 'are', 'not', 'can', 'but', 'all', 'any', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'use', 'man', 'new', 'now', 'way', 'may'}
                
                potential_tags = list(set(all_tags + title_words))
                potential_tags = [tag for tag in potential_tags if len(tag) > 2 and tag.lower() not in stop_words]
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤
                tag_popularity = Counter(all_tags)
                popular_tags = [tag for tag, count in tag_popularity.most_common(20) if count > 1]
                
                if popular_tags:
                    st.markdown("#### üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏ –≤ –Ω–∏—à–µ")
                    
                    # –í—ã–±–æ—Ä —Ç–µ–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üî• –¢–æ–ø —Ç–µ–≥–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:**")
                        selected_tags = []
                        for i, (tag, count) in enumerate(tag_popularity.most_common(10)):
                            if st.checkbox(f"{tag} ({count} —Ä–∞–∑)", key=f"tag_{i}"):
                                selected_tags.append(tag)
                    
                    with col2:
                        st.markdown("**‚ûï –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ —Ç–µ–≥–∏:**")
                        custom_tags = st.text_area(
                            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é:",
                            placeholder="–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, python",
                            help="–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                        )
                        
                        if custom_tags:
                            custom_list = [tag.strip() for tag in custom_tags.split(',') if tag.strip()]
                            selected_tags.extend(custom_list)
                    
                    # –ö–Ω–æ–ø–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
                    if selected_tags and st.button("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–µ–≥–∏", type="primary"):
                        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–≥–æ–≤
                        tag_analyzer = YouTubeTagAnalyzer(
                            serpapi_key if use_serpapi and serpapi_key else None,
                            cache
                        )
                        
                        with st.spinner("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ–≥–æ–≤..."):
                            tag_results = tag_analyzer.analyze_multiple_keywords(selected_tags[:10])  # –õ–∏–º–∏—Ç 10 —Ç–µ–≥–æ–≤
                        
                        if tag_results:
                            st.markdown("#### üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤")
                            
                            # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                            results_data = []
                            for result in tag_results:
                                results_data.append({
                                    '–¢–µ–≥': result.keyword,
                                    '–ü–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º': f"{result.search_volume:,}",
                                    '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è': f"{result.competition_score}/100",
                                    'SEO –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏': f"{result.seo_score}/100",
                                    '–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞': f"{result.overall_score}/100",
                                    '–°–ª–æ–∂–Ω–æ—Å—Ç—å': result.difficulty
                                })
                            
                            results_df = pd.DataFrame(results_data)
                            st.dataframe(results_df, use_container_width=True, hide_index=True)
                            
                            # –¢–æ–ø-3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                            st.markdown("#### üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–µ–≥–∏")
                            
                            top_3 = tag_results[:3]
                            cols = st.columns(3)
                            
                            for i, result in enumerate(top_3):
                                with cols[i]:
                                    st.markdown(f"""
                                    <div class="metric-card">
                                    <h4>üèÜ #{i+1}: {result.keyword}</h4>
                                    <p><strong>–û—Ü–µ–Ω–∫–∞:</strong> {result.overall_score}/100</p>
                                    <p><strong>–û–±—ä–µ–º:</strong> {result.search_volume:,}</p>
                                    <p><strong>–°–ª–æ–∂–Ω–æ—Å—Ç—å:</strong> {result.difficulty}</p>
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            # –ò–Ω—Å–∞–π—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                            st.markdown("#### üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã")
                            
                            if tag_results:
                                avg_competition = sum(r.competition_score for r in tag_results) / len(tag_results)
                                avg_seo_score = sum(r.seo_score for r in tag_results) / len(tag_results)
                                
                                insights = []
                                
                                if avg_competition < 40:
                                    insights.append("‚úÖ **–ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è** - –æ—Ç–ª–∏—á–Ω–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ö–æ–¥–∞ –≤ –Ω–∏—à—É")
                                elif avg_competition > 70:
                                    insights.append("‚ö†Ô∏è **–í—ã—Å–æ–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è** - –Ω—É–∂–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ")
                                
                                if avg_seo_score > 60:
                                    insights.append("üéØ **–•–æ—Ä–æ—à–∏–µ SEO –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏** - –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã —Å–ª–∞–±–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç —Ç–µ–≥–∏")
                                
                                best_tag = max(tag_results, key=lambda x: x.overall_score)
                                insights.append(f"üèÜ **–õ—É—á—à–∏–π —Ç–µ–≥**: '{best_tag.keyword}' (–æ—Ü–µ–Ω–∫–∞ {best_tag.overall_score}/100)")
                                
                                for insight in insights:
                                    st.markdown(insight)
                            
                            # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                            csv_tags = pd.DataFrame([{
                                'Keyword': r.keyword,
                                'Search_Volume': r.search_volume,
                                'Competition_Score': r.competition_score,
                                'SEO_Score': r.seo_score,
                                'Overall_Score': r.overall_score,
                                'Difficulty': r.difficulty
                            } for r in tag_results])
                            
                            csv_tags_export = csv_tags.to_csv(index=False).encode('utf-8')
                            st.download_button(
                                "üì• –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (CSV)",
                                csv_tags_export,
                                f'tag_analysis_{keyword.replace(" ", "_")}.csv',
                                'text/csv'
                            )
                    
                    # –ï—Å–ª–∏ SerpAPI –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É
                    if not use_serpapi:
                        st.info("üí° **–°–æ–≤–µ—Ç**: –í–∫–ª—é—á–∏—Ç–µ SerpAPI –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–∏—Å–∫–æ–≤–æ–º –æ–±—ä–µ–º–µ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏!")
                
                else:
                    st.warning("üè∑Ô∏è –¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                    st.markdown("""
                    - –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–µ
                    - –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ —Ç–µ–≥–∏ –≤ –ø–æ–ª–µ –≤—ã—à–µ
                    """)

            with tab3:
                if trends_data and 'interest_df' in trends_data and not trends_data['interest_df'].empty:
                    st.markdown("### üìà –î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∞ (Google Trends)")
                    
                    # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
                    interest_df = trends_data['interest_df']
                    
                    fig_trends = go.Figure()
                    fig_trends.add_trace(go.Scatter(
                        x=interest_df.index,
                        y=interest_df[keyword],
                        mode='lines+markers',
                        name='–ò–Ω—Ç–µ—Ä–µ—Å',
                        line=dict(color='#1f77b4', width=3),
                        marker=dict(size=6),
                        hovertemplate='<b>%{x}</b><br>–ò–Ω—Ç–µ—Ä–µ—Å: %{y}<extra></extra>'
                    ))
                    
                    fig_trends.update_layout(
                        title=f'–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞: "{keyword}"',
                        xaxis_title='–î–∞—Ç–∞',
                        yaxis_title='–ò–Ω–¥–µ–∫—Å –∏–Ω—Ç–µ—Ä–µ—Å–∞',
                        hovermode='x unified',
                        template='plotly_dark'
                    )
                    
                    st.plotly_chart(fig_trends, use_container_width=True)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(
                            "üìä –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞",
                            trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        )
                    
                    with col2:
                        current_interest = trends_data.get('current_interest', 0)
                        st.metric(
                            "üéØ –¢–µ–∫—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å",
                            f"{current_interest:.0f}/100"
                        )
                    
                    with col3:
                        trend_strength = trends_data.get('trend_strength', 0) * 100
                        st.metric(
                            "‚ö° –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞",
                            f"{trend_strength:.1f}%"
                        )
                    
                    # –°–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                    if 'top_queries' in trends_data and not trends_data['top_queries'].empty:
                        st.markdown("### üîç –°–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**üìä –¢–æ–ø –∑–∞–ø—Ä–æ—Å—ã:**")
                            top_queries = trends_data['top_queries'].head(10)
                            for idx, row in top_queries.iterrows():
                                st.write(f"‚Ä¢ {row['query']} ({row['value']}%)")
                        
                        with col2:
                            if 'rising_queries' in trends_data and not trends_data['rising_queries'].empty:
                                st.markdown("**üöÄ –†–∞—Å—Ç—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã:**")
                                rising_queries = trends_data['rising_queries'].head(10)
                                for idx, row in rising_queries.iterrows():
                                    growth = row['value']
                                    if growth == 'Breakout':
                                        growth = 'üî• –í–∑—Ä—ã–≤'
                                    st.write(f"‚Ä¢ {row['query']} (+{growth})")
                
                else:
                    st.markdown('<div class="warning-alert">', unsafe_allow_html=True)
                    st.warning("üìà –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ Google Trends. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                    st.markdown("""
                    - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è API
                    - –°–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                    - –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º
                    """)
                    st.markdown('</div>', unsafe_allow_html=True)

            with tab3:
                st.markdown(f"### üèÜ –¢–æ–ø-50 –≤–∏–¥–µ–æ –ø–æ —Ç–µ–º–µ '{keyword}'")
                
                # –§–∏–ª—å—Ç—Ä—ã
                filter_col1, filter_col2, filter_col3 = st.columns(3)
                
                with filter_col1:
                    channels = ['–í—Å–µ –∫–∞–Ω–∞–ª—ã'] + sorted(df['channel'].unique().tolist())
                    selected_channel = st.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞–Ω–∞–ª—É:", channels)
                
                with filter_col2:
                    title_keyword = st.text_input("–ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö:")
                
                with filter_col3:
                    min_views = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤:", min_value=0, value=0, step=1000)
                
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                df_filtered = df.copy()
                
                if selected_channel != '–í—Å–µ –∫–∞–Ω–∞–ª—ã':
                    df_filtered = df_filtered[df_filtered['channel'] == selected_channel]
                
                if title_keyword:
                    df_filtered = df_filtered[
                        df_filtered['title'].str.contains(title_keyword, case=False, na=False)
                    ]
                
                if min_views > 0:
                    df_filtered = df_filtered[df_filtered['views'] >= min_views]
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                df_display = df_filtered.copy()
                if not df_display.empty:
                    df_display['published'] = pd.to_datetime(df_display['published']).dt.strftime('%Y-%m-%d')
                    df_display['views_formatted'] = df_display['views'].apply(format_number)
                    df_display['likes_formatted'] = df_display['likes'].apply(format_number)
                    df_display['duration_formatted'] = df_display['duration'].apply(
                        lambda x: f"{int(x)}:{int((x % 1) * 60):02d}" if x >= 1 else f"0:{int(x * 60):02d}"
                    )
                    
                    # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    display_columns = {
                        'title': '–ó–∞–≥–æ–ª–æ–≤–æ–∫',
                        'channel': '–ö–∞–Ω–∞–ª',
                        'views_formatted': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã',
                        'likes_formatted': '–õ–∞–π–∫–∏',
                        'comments': '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',
                        'duration_formatted': '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                        'published': '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏'
                    }
                    
                    df_show = df_display[list(display_columns.keys())].rename(columns=display_columns)
                    df_show = df_show.sort_values('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', key=lambda x: df_filtered['views'], ascending=False)
                    
                    st.dataframe(
                        df_show.head(50),
                        use_container_width=True,
                        hide_index=True
                    )
                    
                    # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    csv = df_filtered.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "üì• –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (CSV)",
                        csv,
                        f'youtube_analysis_{keyword.replace(" ", "_")}.csv',
                        'text/csv'
                    )
                else:
                    st.info("üîç –í–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å —Ç–µ–∫—É—â–∏–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")

            with tab4:
                st.markdown("### üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞")
                
                if not df.empty:
                    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                    fig_views = px.histogram(
                        df, 
                        x='views', 
                        nbins=30,
                        title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤',
                        labels={'views': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ'}
                    )
                    fig_views.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_views, use_container_width=True)
                    
                    # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –ø–æ –¥–Ω—è–º
                    if len(df) > 10:
                        df_time = df.copy()
                        df_time['published'] = pd.to_datetime(df_time['published'])
                        df_time['week'] = df_time['published'].dt.to_period('W')
                        
                        weekly_stats = df_time.groupby('week').agg({
                            'views': 'mean',
                            'video_id': 'count'
                        }).reset_index()
                        
                        weekly_stats['week_str'] = weekly_stats['week'].astype(str)
                        
                        fig_weekly = go.Figure()
                        
                        fig_weekly.add_trace(go.Scatter(
                            x=weekly_stats['week_str'],
                            y=weekly_stats['views'],
                            mode='lines+markers',
                            name='–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã',
                            yaxis='y',
                            line=dict(color='#1f77b4', width=2)
                        ))
                        
                        fig_weekly.add_trace(go.Bar(
                            x=weekly_stats['week_str'],
                            y=weekly_stats['video_id'],
                            name='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ',
                            yaxis='y2',
                            opacity=0.6,
                            marker_color='#ff7f0e'
                        ))
                        
                        fig_weekly.update_layout(
                            title='–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –Ω–µ–¥–µ–ª—è–º',
                            xaxis_title='–ù–µ–¥–µ–ª—è',
                            yaxis=dict(title='–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã', side='left'),
                            yaxis2=dict(title='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ', side='right', overlaying='y'),
                            template='plotly_dark'
                        )
                        
                        st.plotly_chart(fig_weekly, use_container_width=True)
                    
                    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
                    st.markdown("### üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                    
                    numeric_columns = ['views', 'likes', 'comments', 'duration', 'subscribers']
                    correlation_data = df[numeric_columns].corr()
                    
                    fig_corr = px.imshow(
                        correlation_data,
                        title='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏',
                        color_continuous_scale='RdBu',
                        aspect='auto'
                    )
                    fig_corr.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_corr, use_container_width=True)
                    
                    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
                    st.markdown("**üí° –ö–ª—é—á–µ–≤—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:**")
                    
                    views_likes_corr = correlation_data.loc['views', 'likes']
                    views_subs_corr = correlation_data.loc['views', 'subscribers']
                    
                    st.write(f"‚Ä¢ –ü—Ä–æ—Å–º–æ—Ç—Ä—ã ‚Üî –õ–∞–π–∫–∏: {views_likes_corr:.2f}")
                    st.write(f"‚Ä¢ –ü—Ä–æ—Å–º–æ—Ç—Ä—ã ‚Üî –ü–æ–¥–ø–∏—Å—á–∏–∫–∏ –∫–∞–Ω–∞–ª–∞: {views_subs_corr:.2f}")
                    
                    if views_likes_corr > 0.7:
                        st.success("‚úÖ –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏ –ª–∞–π–∫–æ–≤ - –∞–∫—Ç–∏–≤–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è")
                    elif views_likes_corr < 0.3:
                        st.warning("‚ö†Ô∏è –ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏ –ª–∞–π–∫–æ–≤ - –ø–∞—Å—Å–∏–≤–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è")
                
                else:
                    st.info("üìä –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")

        except Exception as e:
            st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
            st.info("üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            st.markdown("""
            - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å API –∫–ª—é—á–µ–π
            - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            - –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            - –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è
            """)

if __name__ == "__main__":
    main()
