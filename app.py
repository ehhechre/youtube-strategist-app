# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
from pytrends.request import TrendReq
from prophet import Prophet
import numpy as np
import openai

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –°–¢–ò–õ–ò ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide"
)
warnings.filterwarnings('ignore')

st.markdown("""
<style>
    .main-header { font-size: 2.8rem; color: #FF0000; text-align: center; margin-bottom: 2rem; font-weight: bold; }
    .stButton>button { border-radius: 8px; font-weight: bold; }
    .custom-container {
        background-color: rgba(42, 57, 62, 0.5); padding: 1.5rem; border-radius: 10px;
        border-left: 5px solid #00a0dc; margin-top: 1rem;
    }
    .openai-result {
        background-color: rgba(26, 142, 95, 0.1); padding: 1.5rem; border-radius: 10px;
        border-left: 5px solid #1a8e5f; margin-top: 1rem;
    }
    .insight-box {
        background-color: #262730; padding: 1rem; border-radius: 10px; margin-top: 1rem;
        border: 1px solid #444;
    }
</style>
""", unsafe_allow_html=True)


# --- 2. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ (–í–°–Ø –õ–û–ì–ò–ö–ê) ---

class CacheManager:
    # ... (–∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {'search': 3600*4, 'trends': 3600*8, 'openai': 3600*24}
        self.stats = {'hits': 0, 'misses': 0}

    def _init_sqlite(self):
        with self.lock:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = conn.cursor()
            cursor.execute('CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value BLOB, expires_at TIMESTAMP)')
            conn.commit()
            conn.close()

    def get(self, key: str):
        with self.lock:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = conn.cursor()
            cursor.execute("SELECT value, expires_at FROM cache WHERE key = ?", (key,))
            result = cursor.fetchone()
            conn.close()
            if result and datetime.fromisoformat(result[1]) > datetime.now():
                self.stats['hits'] += 1
                return pickle.loads(result[0])
            elif result: self.delete(key)
            self.stats['misses'] += 1
            return None

    def set(self, key: str, value: any, category: str):
        with self.lock:
            ttl = self.ttl_map.get(category, 3600)
            expires_at = datetime.now() + timedelta(seconds=ttl)
            value_blob = pickle.dumps(value)
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = conn.cursor()
            cursor.execute("INSERT OR REPLACE INTO cache (key, value, expires_at) VALUES (?, ?, ?)",
                           (key, value_blob, expires_at.isoformat()))
            conn.commit()
            conn.close()

    def delete(self, key: str):
        with self.lock:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = conn.cursor()
            cursor.execute("DELETE FROM cache WHERE key = ?", (key,))
            conn.commit()
            conn.close()

    def generate_key(self, *args) -> str:
        return hashlib.md5("".join(map(str, args)).encode('utf-8')).hexdigest()

class YouTubeAnalyzer:
    # ... (–∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫—Ä–æ–º–µ publishedAfter –≤ search_videos)
    def __init__(self, api_key: str, cache: CacheManager):
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.cache = cache

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        cache_key = self.cache.generate_key('search', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_ids = []
            next_page_token = None
            search_params = {
                'q': keyword, 'part': 'id', 'type': 'video', 'order': 'relevance', 'regionCode': 'RU',
            }
            if published_after:
                search_params['publishedAfter'] = published_after

            while len(video_ids) < max_results:
                search_params['maxResults'] = min(50, max_results - len(video_ids))
                search_params['pageToken'] = next_page_token
                search_request = self.youtube.search().list(**search_params)
                search_response = search_request.execute()
                video_ids.extend([item['id']['videoId'] for item in search_response.get('items', []) if item.get('id', {}).get('kind') == 'youtube#video'])
                next_page_token = search_response.get('nextPageToken')
                if not next_page_token: break

            if not video_ids: return []

            videos = []
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                stats_request = self.youtube.videos().list(part='statistics,snippet,contentDetails', id=','.join(chunk_ids))
                stats_response = stats_request.execute()
                for item in stats_response.get('items', []):
                    stats = item.get('statistics', {})
                    duration = self._parse_duration(item['contentDetails']['duration'])
                    videos.append({
                        'title': item['snippet']['title'], 'channel': item['snippet']['channelTitle'],
                        'published': item['snippet']['publishedAt'], 'views': int(stats.get('viewCount', 0)),
                        'likes': int(stats.get('likeCount', 0)), 'comments': int(stats.get('commentCount', 0)),
                        'duration': duration, 'is_short': duration <= 1.05
                    })
            self.cache.set(cache_key, videos, 'search')
            return videos
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {str(e)}")
            if "quotaExceeded" in str(e): st.error("üö® –í–∞—à–∞ –¥–Ω–µ–≤–Ω–∞—è –∫–≤–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ YouTube API –∏—Å—á–µ—Ä–ø–∞–Ω–∞!")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
        if not match: return 0
        h, m, s = (int(g or 0) for g in match.groups())
        return h * 60 + m + s / 60
    
    def analyze_competition(self, videos: list):
        if not videos: return {}, pd.DataFrame()
        df = pd.DataFrame(videos)
        df['published'] = pd.to_datetime(df['published'], errors='coerce').dt.tz_localize(None)
        df['views'] = df['views'].replace(0, 1)
        df['days_ago'] = (datetime.now() - df['published']).dt.days

        analysis = {
            'avg_views': df['views'].mean(),
            'top_10_avg_views': df.nlargest(10, 'views')['views'].mean(),
            'engagement_rate': ((df['likes'] + df['comments']) / df['views']).mean() * 100,
            'videos_last_week': len(df[df['days_ago'] <= 7]),
            'shorts_percentage': df['is_short'].mean() * 100 if not df.empty else 0,
            'avg_days_to_top_10': df.nlargest(10, 'views')['days_ago'].mean() if not df.empty else 0
        }
        score = 0
        if analysis['top_10_avg_views'] < 50000: score += 2
        elif analysis['top_10_avg_views'] < 250000: score += 1
        if analysis['videos_last_week'] < 5: score += 1
        
        level_map = {0: '–í—ã—Å–æ–∫–∞—è', 1: '–í—ã—Å–æ–∫–∞—è', 2: '–°—Ä–µ–¥–Ω—è—è', 3: '–ù–∏–∑–∫–∞—è'}
        analysis['competition_level'] = level_map.get(score, '–í—ã—Å–æ–∫–∞—è')
        return analysis, df

class AdvancedTrendsAnalyzer:
    # ... (–∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    def __init__(self, cache: CacheManager):
        self.pytrends = TrendReq(hl='ru-RU', tz=180, timeout=(10, 25))
        self.cache = cache

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        try:
            self.pytrends.build_payload([keyword], timeframe='today 12-m', geo='RU')
            interest = self.pytrends.interest_over_time()
            if interest.empty: return None
            
            series = interest[keyword]
            fh_avg, sh_avg = series.iloc[:len(series)//2].mean(), series.iloc[len(series)//2:].mean()
            trend = "–†–∞—Å—Ç—É—â–∏–π üìà" if sh_avg > fh_avg * 1.15 else ("–ü–∞–¥–∞—é—â–∏–π üìâ" if fh_avg > sh_avg * 1.15 else "–°—Ç–∞–±–∏–ª—å–Ω—ã–π ‚û°Ô∏è")
            
            result = {'interest_df': interest, 'trend_direction': trend}
            self.cache.set(cache_key, result, 'trends')
            return result
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

class ContentStrategist:
    def __init__(self, openai_key=None, openai_model=None):
        self.use_openai = bool(openai_key and openai_model)
        if self.use_openai:
            # –í–∞–∂–Ω–æ: —Å–∞–º –∫–ª—é—á —Ç–µ–ø–µ—Ä—å –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞
            self.api_key = openai_key 
            self.model = openai_model

    def get_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame):
        cache_key = None
        if self.use_openai:
            cache_key = cache.generate_key('openai', keyword, self.model, str(comp_analysis), str(trends_data))
            if cached_strategy := cache.get(cache_key):
                st.toast("ü§ñ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞!", icon="üß†")
                return cached_strategy
        
        strategy = self._get_ai_strategy(keyword, comp_analysis, trends_data, df) if self.use_openai else self._get_rule_based_strategy(keyword, comp_analysis, df)
        
        if self.use_openai and cache_key and "–û—à–∏–±–∫–∞" not in strategy:
            cache.set(cache_key, strategy, 'openai')
        
        return strategy

    def _get_rule_based_strategy(self, keyword, comp_analysis, df):
        patterns = self._find_viral_patterns(df)
        ideas = [f"–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword} –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –≤ {datetime.now().year}", f"–¢–æ–ø-5 –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã—Ö —Ñ–∏—à–µ–∫ –≤ {keyword}"]
        return f"""
        ### üí° –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª)
        
        **–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –Ω–∏—à–µ:**
        {patterns}
        ---
        **–ò–¥–µ–∏ –¥–ª—è –≤–∏–¥–µ–æ:**
        {chr(10).join(f'- {i}' for i in ideas)}
        """

    def _find_viral_patterns(self, df: pd.DataFrame):
        if len(df) < 20: return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."
        df['engagement'] = (df['likes'] + df['comments']) / df['views'].replace(0, 1)
        viral = df[df['views'] > df['views'].quantile(0.8)]
        patterns = []
        viral_dur, regular_dur = viral['duration'].mean(), df['duration'].mean()
        if abs(viral_dur - regular_dur) > 1:
            patterns.append(f"- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –í–∏—Ä—É—Å–Ω—ã–µ –≤–∏–¥–µ–æ –≤ —Å—Ä–µ–¥–Ω–µ–º {'–¥–ª–∏–Ω–Ω–µ–µ' if viral_dur > regular_dur else '–∫–æ—Ä–æ—á–µ'} ({viral_dur:.0f} –º–∏–Ω).")
        return "\n".join(patterns) if patterns else "–ß–µ—Ç–∫–∏—Ö –≤–∏—Ä—É—Å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

    def _get_ai_strategy(self, keyword, comp_analysis, trends_data, df):
        st.toast("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ OpenAI...", icon="üß†")
        top_titles = "\n".join([f"- {title}" for title in df.nlargest(5, 'views')['title']])
        prompt = f"""
        –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π YouTube-—Å—Ç—Ä–∞—Ç–µ–≥. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∏—à–∏ –ø–æ –¥–∞–Ω–Ω—ã–º. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –¥–µ—Ä–∑–∫–∏–º –∏ –¥–∞–≤–∞–π –¥–µ–π—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–µ—Ç—ã.

        –ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–ê–Ø –¢–ï–ú–ê: "{keyword}"
        –î–ê–ù–ù–´–ï:
        - –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {comp_analysis['competition_level']}
        - –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis['avg_views']):,}
        - –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: {comp_analysis['engagement_rate']:.2f}%
        - –¢—Ä–µ–Ω–¥ –≤ Google: {trends_data['trend_direction'] if trends_data else '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}
        - –¢–æ–ø-5 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:
        {top_titles}

        –ó–ê–î–ê–ù–ò–ï (–≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown):
        1.  **üöÄ –í–µ—Ä–¥–∏–∫—Ç –∏ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª:** –û—Ü–µ–Ω–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –Ω–∏—à–∏ –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º.
        2.  **üí° 3 –ö—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ –ò–¥–µ–∏ –¥–ª—è –í–∏–¥–µ–æ:** –ü—Ä–∏–¥—É–º–∞–π 3 –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ, —Ü–µ–ø–ª—è—é—â–∏–µ –∏–¥–µ–∏ —Å –±—Ä–æ—Å–∫–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
        3.  **üé£ –£–Ω–∏–∫–∞–ª—å–Ω—ã–π "–ö—Ä—é—á–æ–∫":** –ö–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –ø–æ–º–æ–∂–µ—Ç "–≤–∑–ª–æ–º–∞—Ç—å" –∞–ª–≥–æ—Ä–∏—Ç–º –≤ —ç—Ç–æ–π –Ω–∏—à–µ?
        4.  **üí£ –°–∫—Ä—ã—Ç—ã–π –†–∏—Å–∫:** –ù–∞–∑–æ–≤–∏ –æ–¥–∏–Ω –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–π —Ä–∏—Å–∫ –¥–ª—è –∞–≤—Ç–æ—Ä–∞ –≤ —ç—Ç–æ–π —Ç–µ–º–µ.
        """
        try:
            # --- –ò–ó–ú–ï–ù–ï–ù–ù–´–ô –ë–õ–û–ö ---
            client = openai.OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=800
            )
            return response.choices[0].message.content
            # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ù–û–ì–û –ë–õ–û–ö–ê ---
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

# --- 3. UI –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø (STREAMLIT) ---

st.markdown('<h1 class="main-header">YouTube AI Strategist üß†</h1>', unsafe_allow_html=True)

with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    youtube_api_key = st.text_input("YouTube API Key", type="password")
    
    st.markdown("---")
    use_openai = st.toggle("ü§ñ –í–∫–ª—é—á–∏—Ç—å AI-—Å—Ç—Ä–∞—Ç–µ–≥–∞ (OpenAI)", value=True)
    openai_api_key, openai_model = "", ""
    if use_openai:
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        openai_model = st.selectbox("–ú–æ–¥–µ–ª—å OpenAI", ["gpt-4o", "gpt-4o-mini"], index=1)

    st.markdown("---")
    st.header("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ê–Ω–∞–ª–∏–∑–∞")
    max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 10, 100, 80, 10)
    
    # --- –ù–û–í–´–ô –§–ò–õ–¨–¢–† –ü–û –î–ê–¢–ï ---
    date_range_options = {"–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None, "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365, "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180, "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90}
    selected_date_range = st.selectbox("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ –∑–∞:", list(date_range_options.keys()), index=1)
    days_limit = date_range_options[selected_date_range]
    
    if not youtube_api_key: st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á."); st.stop()

    cache = CacheManager()
    st.markdown("---")
    st.info(f"**–ö—ç—à:** {cache.stats['hits']} –ø–æ–ø–∞–¥–∞–Ω–∏–π / {cache.stats['misses']} –ø—Ä–æ–º–∞—Ö–æ–≤")

keyword = st.text_input("üîç –í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è")
if st.button("üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!", type="primary", use_container_width=True) and keyword:
    analyzer = YouTubeAnalyzer(youtube_api_key, cache)
    trends_analyzer = AdvancedTrendsAnalyzer(cache)
    
    spinner_text = "üåä –ü–æ–≥—Ä—É–∂–∞—é—Å—å –≤ –∞–Ω–∞–ª–∏–∑..."
    if use_openai and openai_api_key: spinner_text += " –ü—Ä–∏–≤–ª–µ–∫–∞—é AI..."

    with st.spinner(spinner_text):
        published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z" if days_limit else None
        videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
        if videos is None or not videos: 
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –≤–∏–¥–µ–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ.")
            st.stop()
        
        comp_analysis, df = analyzer.analyze_competition(videos)
        trends_data = trends_analyzer.analyze_keyword_trends(keyword)
        
        strategist = ContentStrategist(openai_api_key if use_openai else None, openai_model if use_openai else None)
        strategy_output = strategist.get_strategy(keyword, comp_analysis, trends_data, df)

    st.markdown("---")
    st.header(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞: **{keyword}**")
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤–∏–¥–µ–æ", f"{len(df)} —à—Ç.")
    col2.metric("–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", comp_analysis['competition_level'])
    col3.metric("–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", f"{int(comp_analysis['avg_views']):,}")
    col4.metric("–í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å", f"{comp_analysis['engagement_rate']:.2f}%")

    # --- –ù–û–í–´–ô –ë–õ–û–ö "–ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´" ---
    with st.container(border=True):
        st.markdown("#### üîë –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã")
        c1, c2, c3 = st.columns(3)
        c1.metric("–ù–æ–≤—ã—Ö –≤–∏–¥–µ–æ –∑–∞ –Ω–µ–¥–µ–ª—é", f"{comp_analysis['videos_last_week']} —à—Ç.")
        c2.metric("–ü—Ä–æ—Ü–µ–Ω—Ç Shorts", f"{comp_analysis['shorts_percentage']:.0f}%")
        c3.metric("–°—Ä–µ–¥–Ω–∏–π '–≤–æ–∑—Ä–∞—Å—Ç' —Ç–æ–ø-–≤–∏–¥–µ–æ", f"{int(comp_analysis['avg_days_to_top_10'])} –¥–Ω–µ–π")


    tab1, tab2, tab3 = st.tabs(["üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è", "üìà –¢—Ä–µ–Ω–¥—ã", "üèÜ –¢–æ–ø –í–∏–¥–µ–æ –∏ –î–∞–Ω–Ω—ã–µ"])

    with tab1:
        css_class = "openai-result" if strategist.use_openai else "custom-container"
        with st.container():
            st.markdown(f'<div class="{css_class}">{strategy_output}</div>', unsafe_allow_html=True)
    
    with tab2:
        if trends_data and 'interest_df' in trends_data:
            st.subheader("üìà –î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –≤ Google Trends")
            fig_trends = px.line(trends_data['interest_df'], y=keyword, title=f'–ò–Ω—Ç–µ—Ä–µ—Å –∫ "{keyword}" –∑–∞ 12 –º–µ—Å.')
            st.plotly_chart(fig_trends, use_container_width=True)
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends.")

    with tab3:
        st.subheader(f"üèÜ –¢–æ–ø-50 –≤–∏–¥–µ–æ –ø–æ —Ç–µ–º–µ '{keyword}'")
        
        # --- –ù–û–í–´–ï –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ï –§–ò–õ–¨–¢–†–´ ---
        df_display = df.copy()
        df_display['published'] = pd.to_datetime(df_display['published']).dt.strftime('%Y-%m-%d')
        
        filter1, filter2 = st.columns(2)
        with filter1:
            # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞–Ω–∞–ª—É
            channels = ['–í—Å–µ –∫–∞–Ω–∞–ª—ã'] + list(df_display['channel'].unique())
            selected_channel = st.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞–Ω–∞–ª—É:", channels)
            if selected_channel != '–í—Å–µ –∫–∞–Ω–∞–ª—ã':
                df_display = df_display[df_display['channel'] == selected_channel]
        with filter2:
             # –§–∏–ª—å—Ç—Ä –ø–æ —Å–ª–æ–≤—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            title_keyword = st.text_input("–§–∏–ª—å—Ç—Ä –ø–æ —Å–ª–æ–≤—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ:")
            if title_keyword:
                df_display = df_display[df_display['title'].str.contains(title_keyword, case=False, na=False)]

        st.dataframe(df_display.nlargest(50, 'views'), use_container_width=True)
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("üì• –°–∫–∞—á–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ (CSV)", csv, f'youtube_data_{keyword}.csv', 'text/csv')
