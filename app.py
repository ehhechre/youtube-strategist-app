–û—Ç–ª–∏—á–Ω—ã–π –∫–æ–¥! –≠—Ç–æ –æ—á–µ–Ω—å –∞–º–±–∏—Ü–∏–æ–∑–Ω—ã–π –∏ –º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç. –Ø –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏–ª –≤–∞—à —Å–∫—Ä–∏–ø—Çapp.py–∏ –Ω–∞—à–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –∏ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏—á–∏–Ω–æ–π —Ç–æ–≥–æ, —á—Ç–æ "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç" –∏

–ü—Ä–æ–±–ª–µ–º–∞ –Ω–µ –≤ —Ç–æ–º, —á—Ç–æ –∫–æ–¥ —Å–ª–æ–º–∞–Ω, –∞ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö –∏ –Ω–µ –æ—á–µ–Ω—å –Ω–∞–¥–µ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å–±–æ—è–º –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –Ø –Ω–µ —Å—Ç–∞–ª –Ω–∏—á–µ–≥–æ —É–¥–∞–ª—è—Ç—å, –∞ –∏—Å–ø—Ä–∞–≤–∏–ª –∏ –∑–∞—â–∏—Ç–∏–ª –æ–ø–∞—Å–Ω—ã–µ —É—á–∞—Å—Ç–∫–∏.

–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è:

–°–∞–º–∞—è –≤–µ—Ä–æ—è—Ç–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ "–Ω–µ—Ç –≤–∏–¥–µ–æ": –ñ–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–π —Ä–µ–≥–∏–æ–Ω.

–ü—Ä–æ–±–ª–µ–º–∞:–í —Ñ—É–Ω–∫—Ü–∏–∏–ø–æ–∏—Å–∫_–≤–∏–¥–µ–æ—É –≤–∞—Å –±—ã–ª–∏ –∂–µ—Å—Ç–∫–æ –ø—Ä–æ–ø–∏—Å–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã'regionCode': 'RU'–∏'relevanceLanguage': 'ru'. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ API –∏—Å–∫–∞–ª –≤–∏–¥–µ–æ —Ç–æ–ª—å–∫–æ –≤ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö –∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ï—Å–ª–∏ –≤—ã –≤–≤–µ–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏–ª–∏ –∏—Å–∫–∞–ª–∏ —Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –ø–æ–ø—É–ª—è—Ä–Ω–∞ –≤ RU-—Å–µ–≥–º–µ–Ω—Ç–µ, YouTube API –º–æ–∂–µ—Ç –ø—Ä–æ—Å—Ç–æ –Ω–µ –≤–µ—Ä–Ω—É—Ç—å –Ω–∏–∫–∞–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

–†–µ—à–µ–Ω–∏–µ:–Ø —É–±—Ä–∞–ª —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –¢–µ–ø–µ—Ä—å –ø–æ–∏—Å–∫ –±—É–¥–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ –ª—é–±–æ–º—É –∑–∞–ø—Ä–æ—Å—É. –≠—Ç–æ —Å–∞–º–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.

–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤ (Tag Analyzer).

–ü—Ä–æ–±–ª–µ–º–∞:–í –∫–ª–∞—Å—Å–µ `YouTubeTaYouTubeTagAnalyzer, —Ñ—É–Ω–∫—Ü–∏—è_–æ–±—Ä–∞–±–æ—Ç–∫–∞_–∫–æ–Ω–∫—É—Ä—Å–Ω—ã—Ö_–¥–∞–Ω–Ω—ã—Ö—Å–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–æ –ø—Ä–æ–≤–µ—Ä—è–ª–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ:–ª—é–±–æ–π(—Å–ª–æ–≤–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –¥–ª—è —Å–ª–æ–≤–∞ –≤ keyword_lower.split()). –ï—Å–ª–∏ —Ç–≤–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –±—ã–ª–æ "–∫–∞–∫ —Å

–†–µ—à–µ–Ω–∏–µ:–Ø –∏–∑–º–µ–Ω–∏–ª –ª–æ–≥–∏–∫—É –Ω–∞ –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É–µ—Å–ª–∏ keyword_lower –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ:, –∫–æ—Ç–æ—Ä–∞—è –∏

–ù–µ–Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –≤ Tag Analyzer.

–ü—Ä–æ–±–ª–µ–º–∞:–§—É–Ω–∫—Ü–∏—è_extract_views–±—ã–ª–∞ –æ—á–µ–Ω—å —Ö—Ä—É–ø–∫–æ–π. –û–Ω–∞ –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–∏–ª–∞ –≤—Å–µ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã. –ï—Å–ª–∏ –±—ã API –≤–µ—Ä–Ω—É–ª –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´1,2 –º–ª–Ω –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤¬ª, –≤–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥.12, —á—Ç–æ –Ω–µ–≤–µ—Ä–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ.

–†–µ—à–µ–Ω–∏–µ:–Ø –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–ª —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é. –¢–µ–ø–µ—Ä

–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –∫–ª—é—á–∞–º–∏ –∫—ç—à–∞.

–ü—Ä–æ–±–ª–µ–º–∞:–§—É–Ω–∫—Ü–∏—è—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å_–∫–ª—é—á–≤CacheManager–ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤str(arg). –ï—Å–ª–∏ –æ–¥–Ω–∏–º –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –±—ã–ª –±—ã —Å–ª–æ–∂–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å, —ç—Ç–æ –º–æ–≥–ª–æ –±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º—É –∫–ª—é—á—É.

–†–µ—à–µ–Ω–∏–µ:–Ø –±–ª–æ–∫–∏—Ä—É—é —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é. –¢–µ–ø–µ—Ä—å –æ–Ω–∞ —Å–Ω–∞—á–∞–ª–∞ —Ç–∏–ø –∞—Ä–≥json.dumps–¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∫—ç—à–∏

–ú–µ–ª–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.

–Ø –¥–æ–±–∞–≤–∏–ª –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æyoutube_strategist.log–±—ã–ª–æ –ª–µ–≥—á–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å, –Ω–∞ –∫–∞–∫–æ–º —ç—Ç–∞–ø–µ –≤–æ–∑–Ω

–í–Ω–µ—Å –Ω–µ–±–æ–ª—å—à–∏–µ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–Ω–µ—Å–∏–º–ø–æ—Ä—Ç –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏–≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞.

–£–ª—É—á—à–∏–ª –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—É—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—á–∫–∞—Ö, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–∞–¥–µ–Ω–∏–π, –µ—Å–ª–∏ API –≤–µ—Ä–Ω–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ

–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤–∞—à –ø–æ–ª–Ω—ã–π, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–¥. –ü—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –µ–≥–æ –∏ –∑–∞–º–µ–Ω–∏—Ç–µapp.py. –Ø –¥–æ–±–∞–≤–∏–ª –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏# –ò–°–ü–†–ê–í–õ–ï–ù–û:–∏–ª–∏# –£–õ–£–ß–®–ï–ù–û:–≤ —Ç–µ—Ö –º–µ—Å—Ç–∞—Ö, –≥–¥–µ –±—ã–ª

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥app.py
code
–ü–∏—Ç–æ–Ω
download
content_copy
expand_less

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
import time
import logging
from pytrends.request import TrendReq
import openai
import numpy as np
from collections import Counter
import requests
import json
from dataclasses import dataclass
from urllib.parse import quote_plus
import concurrent.futures
import unicodedata
import math # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–µ—Ä–µ–Ω–æ—Å –∏–º–ø–æ—Ä—Ç–∞ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('youtube_strategist.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è API –ª–∏–º–∏—Ç–æ–≤
YOUTUBE_API_DAILY_QUOTA = 10000
REQUEST_DELAY = 0.1
MAX_RETRIES = 3
REQUEST_TIMEOUT = 30

st.markdown("""
<style>
    .main-header {
        font-size: 2.8rem;
        color: #FF0000;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
        background: linear-gradient(90deg, #FF0000 0%, #FF6B6B 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .stButton>button {
        border-radius: 8px;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .custom-container {
        background: linear-gradient(135deg, rgba(42, 57, 62, 0.5), rgba(62, 77, 82, 0.3));
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #00a0dc;
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .openai-result {
        background: linear-gradient(135deg, rgba(26, 142, 95, 0.1), rgba(46, 162, 115, 0.05));
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #1a8e5f;
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .insight-box {
        background: linear-gradient(135deg, #262730, #3a3b45);
        padding: 1rem;
        border-radius: 15px;
        margin-top: 1rem;
        border: 1px solid #444;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #1e1e2e, #2a2a3e);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #444;
        margin: 0.5rem 0;
    }
    .success-alert {
        background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(34, 197, 94, 0.05));
        border: 1px solid rgba(34, 197, 94, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background: linear-gradient(135deg, rgba(251, 146, 60, 0.1), rgba(251, 146, 60, 0.05));
        border: 1px solid rgba(251, 146, 60, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. –£–¢–ò–õ–ò–¢–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø (–£–õ–£–ß–®–ï–ù–ù–´–ï) ---

def validate_youtube_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ YouTube API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    
    if api_key.startswith('AIza') and len(api_key) == 39:
        return True
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –∫–ª—é—á–µ–π, –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∞—è.
    if len(api_key) > 30 and re.match(r'^[A-Za-z0-9_-]+$', api_key):
        return True
    
    return False

def validate_openai_api_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ OpenAI API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return api_key.startswith('sk-') and len(api_key) > 40

def validate_serpapi_key(api_key: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ SerpAPI –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    
    api_key = api_key.strip()
    return len(api_key) > 30 and all(c.isalnum() for c in api_key)

def safe_format_number(num) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        if pd.isna(num) or num is None:
            return "0"
        
        num = float(num)
        if num >= 1_000_000_000:
            return f"{num/1_000_000_000:.1f}B"
        elif num >= 1_000_000:
            return f"{num/1_000_000:.1f}M"
        elif num >= 1_000:
            return f"{num/1_000:.1f}K"
        return str(int(num))
    except (ValueError, TypeError, OverflowError):
        return "0"

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not isinstance(text, str):
        return ""
    
    text = unicodedata.normalize('NFKD', text)
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    return text.strip()

def safe_int_conversion(value, default=0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    try:
        if pd.isna(value) or value is None:
            return default
        return int(float(value))
    except (ValueError, TypeError, OverflowError):
        return default

def safe_float_conversion(value, default=0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
    try:
        if pd.isna(value) or value is None:
            return default
        return float(value)
    except (ValueError, TypeError, OverflowError):
        return default

def validate_keyword(keyword: str) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    if not keyword or not isinstance(keyword, str):
        return False
    
    keyword = keyword.strip()
    
    if len(keyword) < 2 or len(keyword) > 100:
        return False
    
    if keyword.count(' ') > 10:
        return False
    
    if re.search(r'[<>"\'\[\]{}|\\`]', keyword):
        return False
    
    return True

def extract_keywords_from_titles(titles: list, min_length=3, max_keywords=15) -> list:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    if not titles:
        return []
    
    all_words = []
    stop_words = {
        '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–∑–∞', '–æ—Ç', '–¥–æ',
        '–∏–∑', '–∫', '–æ', '—É', '–∂–µ', '–µ—â–µ', '—É–∂–µ', '–∏–ª–∏', '—Ç–∞–∫', '–Ω–æ', '–∞', '–∏—Ö', '–µ–≥–æ',
        '–µ—ë', '–º–æ–π', '—Ç–≤–æ–π', '–Ω–∞—à', '–≤–∞—à', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–µ—Å–ª–∏',
        '—á—Ç–æ–±—ã', '–∫–æ–≥–¥–∞', '–≥–¥–µ', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
        'for', 'of', 'with', 'by', 'you', 'are', 'can', 'all', 'any', 'how', 'what',
        'when', 'where', 'why', 'this', 'that', 'have', 'had', 'will', 'been', 'were',
        'was', 'are', 'is', 'am', 'be', 'do', 'did', 'does', 'has', 'get', 'got'
    }
    
    try:
        for title in titles:
            if not title:
                continue
            
            title_clean = clean_text(str(title).lower())
            words = re.findall(r'\b[–∞-—è—ë]{3,}|[a-z]{3,}\b', title_clean)
            filtered_words = [
                word for word in words
                if len(word) >= min_length and word not in stop_words
            ]
            all_words.extend(filtered_words)
        
        word_counts = Counter(all_words)
        return word_counts.most_common(max_keywords)
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return []

def retry_api_call(func, max_retries=MAX_RETRIES, delay=REQUEST_DELAY):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ API –≤—ã–∑–æ–≤–æ–≤"""
    def wrapper(*args, **kwargs):
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    logger.info(f"API –≤—ã–∑–æ–≤ —É—Å–ø–µ—à–µ–Ω —Å {attempt + 1} –ø–æ–ø—ã—Ç–∫–∏")
                return result
            
            except HttpError as e:
                last_exception = e
                status_code = e.resp.status
                
                if status_code == 403:
                    st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á –∏ –µ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ Google Cloud Console.")
                    logger.error(f"–û—à–∏–±–∫–∞ 403 (Forbidden). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code == 400:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
                    logger.error(f"–û—à–∏–±–∫–∞ 400 (Bad Request). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code in [500, 502, 503, 504]:
                    logger.warning(f"–°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))
                        continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {status_code}: {e}")
                    break
            
            except Exception as e:
                last_exception = e
                logger.warning(f"–û—à–∏–±–∫–∞ API –≤—ã–∑–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay)
                    continue
                break
        
        logger.error(f"API –≤—ã–∑–æ–≤ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_exception}")
        raise last_exception
    
    return wrapper

# --- 3. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ ---

class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {
            'search': 3600*4,
            'channels': 3600*24*7,
            'trends': 3600*8,
            'openai': 3600*24,
            'serpapi': 3600*6
        }
        self.stats = {'hits': 0, 'misses': 0, 'errors': 0, 'size_mb': 0}
        self._update_cache_stats()

    def _init_sqlite(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                with self.lock:
                    conn = sqlite3.connect(
                        self.db_path,
                        check_same_thread=False,
                        timeout=10.0
                    )
                    cursor = conn.cursor()
                    
                    cursor.execute('''
                        CREATE TABLE IF NOT EXISTS cache (
                            key TEXT PRIMARY KEY,
                            value BLOB,
                            expires_at TIMESTAMP,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            access_count INTEGER DEFAULT 1,
                            category TEXT,
                            size_bytes INTEGER
                        )
                    ''')
                    
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON cache(category)')
                    cursor.execute('CREATE INDEX IF NOT EXISTS idx_access_count ON cache(access_count)')
                    
                    cursor.execute('PRAGMA journal_mode=WAL')
                    cursor.execute('PRAGMA synchronous=NORMAL')
                    cursor.execute('PRAGMA cache_size=10000')
                    
                    conn.commit()
                    conn.close()
                    logger.info("–ö—ç—à –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    return
            
            except sqlite3.Error as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_attempts - 1:
                    time.sleep(1)
                    continue
                else:
                    st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")

    def _update_cache_stats(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
                result = cursor.fetchone()
                if result:
                    self.stats['size_mb'] = round(result[0] / (1024 * 1024), 2)
                
                conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞: {e}")

    def get(self, key: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "SELECT value, expires_at, access_count FROM cache WHERE key = ?",
                    (key,)
                )
                result = cursor.fetchone()
                
                if result:
                    value_blob, expires_at, access_count = result
                    
                    if datetime.fromisoformat(expires_at) > datetime.now():
                        cursor.execute(
                            "UPDATE cache SET access_count = ? WHERE key = ?",
                            (access_count + 1, key)
                        )
                        conn.commit()
                        conn.close()
                        
                        self.stats['hits'] += 1
                        return pickle.loads(value_blob)
                    else:
                        cursor.execute("DELETE FROM cache WHERE key = ?", (key,))
                        conn.commit()
                
                conn.close()
                self.stats['misses'] += 1
                return None
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: {e}")
            return None

    def set(self, key: str, value: any, category: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            with self.lock:
                ttl = self.ttl_map.get(category, 3600)
                expires_at = datetime.now() + timedelta(seconds=ttl)
                value_blob = pickle.dumps(value)
                size_bytes = len(value_blob)
                
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("""
                    INSERT OR REPLACE INTO cache
                    (key, value, expires_at, category, size_bytes, created_at, access_count)
                    VALUES (?, ?, ?, ?, ?, ?, 1)
                """, (key, value_blob, expires_at.isoformat(), category, size_bytes, datetime.now().isoformat()))
                
                conn.commit()
                conn.close()
                
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")

    def clean_expired(self) -> int:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute(
                    "DELETE FROM cache WHERE expires_at < ?",
                    (datetime.now().isoformat(),)
                )
                expired_count = cursor.rowcount
                
                cursor.execute("SELECT COUNT(*) FROM cache")
                total_records = cursor.fetchone()[0]
                
                if total_records > 1000:
                    cursor.execute("""
                        DELETE FROM cache WHERE key IN (
                            SELECT key FROM cache
                            ORDER BY access_count ASC, created_at ASC
                            LIMIT ?
                        )
                    """, (total_records // 10,))
                    
                    old_records = cursor.rowcount
                    logger.info(f"–£–¥–∞–ª–µ–Ω–æ {old_records} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
                
                cursor.execute("VACUUM")
                
                conn.commit()
                conn.close()
                
                self._update_cache_stats()
                return expired_count
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
            return 0

    def get_cache_info(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                
                cursor.execute("SELECT COUNT(*), SUM(size_bytes) FROM cache")
                count, total_size = cursor.fetchone()
                
                cursor.execute("""
                    SELECT category, COUNT(*), SUM(size_bytes), AVG(access_count)
                    FROM cache GROUP BY category
                """)
                categories = cursor.fetchall()
                
                conn.close()
                
                return {
                    'total_records': count or 0,
                    'total_size_mb': round((total_size or 0) / (1024 * 1024), 2),
                    'categories': {cat: {'count': cnt, 'size_mb': round((size or 0) / (1024 * 1024), 2), 'avg_access': round(avg or 0, 1)} for cat, cnt, size, avg in categories},
                    'hit_rate': round(self.stats['hits'] / max(self.stats['hits'] + self.stats['misses'], 1) * 100, 1)
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ: {e}")
            return {'error': str(e)}

    def generate_key(self, *args) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–π –¥–ª—è –∫—ç—à–∞."""
        try:
            clean_args = []
            for arg in args:
                if isinstance(arg, (dict, list)):
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º JSON –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤–∞—Ä–µ–π/—Å–ø–∏—Å–∫–æ–≤
                    s = json.dumps(arg, sort_keys=True)
                elif arg is None:
                    s = 'None'
                else:
                    s = str(arg)
                clean_args.append(s.strip()[:200]) # –£–≤–µ–ª–∏—á–∏–º –ª–∏–º–∏—Ç –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∫–ª—é—á–µ–π
            
            combined = "|".join(clean_args)
            return hashlib.md5(combined.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞ –∫—ç—à–∞: {e}")
            return hashlib.md5(f"error_{time.time()}_{str(args)}".encode()).hexdigest()

class YouTubeAnalyzer:
    def __init__(self, api_key: str, cache: CacheManager):
        try:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
            self.cache = cache
            self.api_key = api_key
            self.quota_used = 0
            logger.info("YouTube API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ YouTube API: {e}")
            raise

    def test_connection(self) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            # –î–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏ –¥–µ—à–µ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–∞
            self.youtube.i18nLanguages().list(part='snippet', hl='en').execute()
            logger.info("YouTube API —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ.")
            return True
        except HttpError as e:
            logger.error(f"–¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ YouTube: {e.resp.status} - {e.error_details[0]['reason']}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à API –∫–ª—é—á.")
            return False
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API: {e}")
            return False

    def _make_api_request(self, request_func, *args, **kwargs):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –∫–≤–æ—Ç"""
        try:
            if self.quota_used > YOUTUBE_API_DAILY_QUOTA * 0.9:
                st.warning("‚ö†Ô∏è –ü—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ –ª–∏–º–∏—Ç—É YouTube API –∫–≤–æ—Ç—ã")
            
            response = retry_api_call(request_func)(*args, **kwargs).execute() # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω .execute()
            self.quota_used += 1 # –≠—Ç–æ –Ω–µ—Ç–æ—á–Ω–æ, —Ç.–∫. –∑–∞–ø—Ä–æ—Å—ã —Å—Ç–æ—è—Ç –ø–æ-—Ä–∞–∑–Ω–æ–º—É, –Ω–æ –∫–∞–∫ –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ–π–¥–µ—Ç
            return response
        
        except HttpError as e:
            if e.resp.status == 403:
                st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
            elif e.resp.status == 400:
                st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API")
            else:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ YouTube API: {e}")
            raise
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ API: {e}")
            raise

    def get_channel_stats(self, channel_ids: list):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤"""
        if not channel_ids:
            return {}
        
        unique_ids = list(set(filter(None, channel_ids)))
        if not unique_ids:
            return {}
            
        cache_key = self.cache.generate_key('channels', sorted(unique_ids))
        if cached_data := self.cache.get(cache_key):
            return cached_data
        
        channel_stats = {}
        try:
            for i in range(0, len(unique_ids), 50):
                chunk_ids = unique_ids[i:i+50]
                
                request = self.youtube.channels().list(
                    part="statistics,snippet,brandingSettings",
                    id=",".join(chunk_ids)
                )
                response = self._make_api_request(lambda: request) # –ò–°–ü–†–ê–í–õ–ï–ù–û: –æ–±–µ—Ä—Ç–∫–∞ –≤ –ª—è–º–±–¥—É
                
                for item in response.get('items', []):
                    stats = item.get('statistics', {})
                    snippet = item.get('snippet', {})
                    branding = item.get('brandingSettings', {}).get('channel', {})
                    
                    channel_stats[item['id']] = {
                        'subscribers': safe_int_conversion(stats.get('subscriberCount', 0)),
                        'total_views': safe_int_conversion(stats.get('viewCount', 0)),
                        'video_count': safe_int_conversion(stats.get('videoCount', 0)),
                        'title': clean_text(snippet.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
                        'description': clean_text(snippet.get('description', ''))[:500],
                        'published_at': snippet.get('publishedAt', ''),
                        'country': snippet.get('country', ''),
                        'verified': 'verified' in str(snippet.get('thumbnails', {})), # –£–õ–£–ß–®–ï–ù–û: –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±
                        'keywords': branding.get('keywords', '').split(',')[:10] if branding.get('keywords') else []
                    }
                
                if i + 50 < len(unique_ids):
                    time.sleep(REQUEST_DELAY)
            
            self.cache.set(cache_key, channel_stats, 'channels')
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {len(channel_stats)} –∫–∞–Ω–∞–ª–æ–≤")
            return channel_stats
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤: {e}")
            return {}

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        
        if not validate_keyword(keyword):
            st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            return None
        
        if max_results > 500:
            max_results = 500
            st.warning("‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 500")
        
        cache_key = self.cache.generate_key('search_v3', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_snippets = []
            next_page_token = None
            search_params = {
                'q': clean_text(keyword),
                'part': 'snippet',
                'type': 'video',
                'order': 'relevance',
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω—ã –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω –∏ —è–∑—ã–∫ –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                # 'regionCode': 'RU',
                # 'relevanceLanguage': 'ru'
            }
            
            if published_after:
                search_params['publishedAfter'] = published_after

            progress_bar = st.progress(0)
            status_text = st.empty()
            
            while len(video_snippets) < max_results:
                search_params['maxResults'] = min(50, max_results - len(video_snippets))
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                status_text.text(f"üîç –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ: {len(video_snippets)}/{max_results}")
                progress_bar.progress(len(video_snippets) / max_results)
                
                request = self.youtube.search().list(**search_params)
                search_response = self._make_api_request(lambda: request) # –ò–°–ü–†–ê–í–õ–ï–ù–û: –æ–±–µ—Ä—Ç–∫–∞ –≤ –ª—è–º–±–¥—É
                new_items = search_response.get('items', [])
                
                if not new_items:
                    logger.warning(f"–ü–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{keyword}' –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
                    break
                    
                video_snippets.extend(new_items)
                next_page_token = search_response.get('nextPageToken')
                
                if not next_page_token:
                    break
                
                time.sleep(REQUEST_DELAY)

            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(video_snippets)} –≤–∏–¥–µ–æ. –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª–∏...")
            
            if not video_snippets:
                return []

            video_ids = [item['id']['videoId'] for item in video_snippets if 'videoId' in item.get('id', {})]
            channel_ids = list(set([item['snippet']['channelId'] for item in video_snippets]))

            status_text.text("üìä –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤...")
            channel_stats = self.get_channel_stats(channel_ids)
            
            videos = []
            
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                status_text.text(f"üìä –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –≤–∏–¥–µ–æ ({i+len(chunk_ids)}/{len(video_ids)})...")
                
                request = self.youtube.videos().list(
                    part='statistics,contentDetails,snippet,topicDetails',
                    id=','.join(chunk_ids)
                )
                stats_response = self._make_api_request(lambda: request) # –ò–°–ü–†–ê–í–õ–ï–ù–û: –æ–±–µ—Ä—Ç–∫–∞ –≤ –ª—è–º–±–¥—É
                
                video_details_map = {item['id']: item for item in stats_response.get('items', [])}
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ video_snippets, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                for snippet_item in video_snippets:
                    video_id = snippet_item['id'].get('videoId')
                    if not video_id or video_id not in video_details_map:
                        continue
                        
                    details = video_details_map.get(video_id)
                    if not details:
                        continue
                    
                    stats = details.get('statistics', {})
                    content_details = details.get('contentDetails', {})
                    video_snippet = details.get('snippet', {})
                    topic_details = details.get('topicDetails', {})
                    
                    duration = self._parse_duration(content_details.get('duration', 'PT0S'))
                    channel_id = video_snippet['channelId']
                    channel_info = channel_stats.get(channel_id, {})
                    
                    category_id = safe_int_conversion(video_snippet.get('categoryId', 0))
                    tags = video_snippet.get('tags', [])[:20]
                    
                    video_data = {
                        'video_id': video_id,
                        'title': clean_text(video_snippet['title']),
                        'channel': clean_text(video_snippet['channelTitle']),
                        'channel_id': channel_id,
                        'subscribers': channel_info.get('subscribers', 0),
                        'subscribers_formatted': safe_format_number(channel_info.get('subscribers', 0)),
                        'channel_total_views': channel_info.get('total_views', 0),
                        'channel_video_count': channel_info.get('video_count', 0),
                        'channel_verified': channel_info.get('verified', False),
                        'published': video_snippet['publishedAt'],
                        'views': safe_int_conversion(stats.get('viewCount', 0)),
                        'views_formatted': safe_format_number(safe_int_conversion(stats.get('viewCount', 0))),
                        'likes': safe_int_conversion(stats.get('likeCount', 0)),
                        'likes_formatted': safe_format_number(safe_int_conversion(stats.get('likeCount', 0))),
                        'comments': safe_int_conversion(stats.get('commentCount', 0)),
                        'duration': duration,
                        'duration_formatted': self._format_duration(duration),
                        'is_short': duration <= 1.05,
                        'short_indicator': "ü©≥ Shorts" if duration <= 1.05 else "üìπ –í–∏–¥–µ–æ",
                        'tags': tags,
                        'description': clean_text(video_snippet.get('description', ''))[:1000],
                        'definition': content_details.get('definition', 'sd').upper(),
                        'category_id': category_id,
                        'language': video_snippet.get('defaultLanguage', 'ru'),
                        'topics': topic_details.get('topicCategories', [])[:5],
                        'thumbnail': snippet_item['snippet'].get('thumbnails', {}).get('medium', {}).get('url', ''),
                        'video_url': f"https://www.youtube.com/watch?v={video_id}"
                    }
                    videos.append(video_data)
                
                if i + 50 < len(video_ids):
                    time.sleep(REQUEST_DELAY)
            
            progress_bar.empty()
            status_text.empty()
            
            self.cache.set(cache_key, videos, 'search')
            logger.info(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ –¥–ª—è '{keyword}'")
            return videos
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ search_videos –¥–ª—è '{keyword}': {e}", exc_info=True)
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {e}")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not duration_str:
            return 0
            
        try:
            match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
            if not match:
                return 0
                
            h, m, s = (safe_int_conversion(g) for g in match.groups())
            return h * 60 + m + s / 60
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ '{duration_str}': {e}")
            return 0
    
    def _format_duration(self, duration_minutes: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        try:
            if duration_minutes < 1:
                return f"0:{int(duration_minutes * 60):02d}"
            
            hours = int(duration_minutes // 60)
            minutes = int(duration_minutes % 60)
            seconds = int((duration_minutes - (hours * 60) - minutes) * 60) # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–µ–∫—É–Ω–¥
            
            if hours > 0:
                return f"{hours}:{minutes:02d}:{seconds:02d}"
            else:
                return f"{minutes}:{seconds:02d}"
        except Exception:
            return "0:00"
    
    def analyze_competition(self, videos: list):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        if not videos:
            return {}, pd.DataFrame()
        
        try:
            df = pd.DataFrame(videos)
            
            df['published'] = pd.to_datetime(df['published'], errors='coerce', utc=True).dt.tz_localize(None)
            df = df.dropna(subset=['published', 'views'])
            
            if df.empty:
                logger.warning("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö DataFrame –ø—É—Å—Ç")
                return {}, pd.DataFrame()
            
            df['views'] = df['views'].apply(lambda x: max(safe_int_conversion(x, 1), 1))
            df['days_ago'] = (datetime.now() - df['published']).dt.days.fillna(0)
            # –£–õ–£–ß–®–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å, –µ—Å–ª–∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ 0
            df['engagement_rate'] = np.where(df['views'] > 0, ((df['likes'] + df['comments']) / df['views']) * 100, 0)
            df['views_per_subscriber'] = np.where(df['subscribers'] > 0, df['views'] / df['subscribers'], 0)
            
            view_quartiles = df['views'].quantile([0.25, 0.5, 0.75, 0.9])
            
            analysis = {
                'total_videos': len(df),
                'avg_views': safe_float_conversion(df['views'].mean()),
                'median_views': safe_float_conversion(df['views'].median()),
                'top_10_avg_views': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['views'].mean()),
                'top_25_percent_views': safe_float_conversion(view_quartiles[0.75]),
                'engagement_rate': safe_float_conversion(df['engagement_rate'].mean()),
                'videos_last_week': len(df[df['days_ago'] <= 7]),
                'videos_last_month': len(df[df['days_ago'] <= 30]),
                'shorts_percentage': safe_float_conversion(df['is_short'].mean() * 100),
                'avg_days_to_top_10': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['days_ago'].mean()),
                'unique_channels': df['channel'].nunique(),
                'avg_channel_subscribers': safe_float_conversion(df.drop_duplicates(subset=['channel_id'])['subscribers'].mean()), # –£–õ–£–ß–®–ï–ù–û: —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∫–∞–Ω–∞–ª–∞–º
                'avg_duration': safe_float_conversion(df[~df['is_short']]['duration'].mean()),
                'hd_percentage': safe_float_conversion((df['definition'] == 'HD').mean() * 100),
                'verified_channels_count': safe_int_conversion(df['channel_verified'].sum()),
                'avg_likes_per_view': safe_float_conversion(np.where(df['views'] > 0, (df['likes'] / df['views']), 0).mean() * 100),
                'avg_comments_per_view': safe_float_conversion(np.where(df['views'] > 0, (df['comments'] / df['views']), 0).mean() * 100)
            }

            score = 0
            
            if analysis['top_10_avg_views'] < 20000:
                score += 4
            elif analysis['top_10_avg_views'] < 50000:
                score += 3
            elif analysis['top_10_avg_views'] < 200000:
                score += 2
            elif analysis['top_10_avg_views'] < 500000:
                score += 1
            
            if analysis['videos_last_week'] < 2:
                score += 3
            elif analysis['videos_last_week'] < 5:
                score += 2
            elif analysis['videos_last_week'] < 15:
                score += 1
            
            if analysis['unique_channels'] < 15:
                score += 2
            elif analysis['unique_channels'] < 30:
                score += 1
            
            if analysis['engagement_rate'] < 1.5:
                score += 2
            elif analysis['engagement_rate'] < 3:
                score += 1
            
            competition_levels = {
                0: '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥',
                1: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
                2: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
                3: '–í—ã—Å–æ–∫–∞—è üü†',
                4: '–í—ã—Å–æ–∫–∞—è üü†',
                5: '–°—Ä–µ–¥–Ω—è—è üü°',
                6: '–°—Ä–µ–¥–Ω—è—è üü°',
                7: '–ù–∏–∑–∫–∞—è üü¢',
                8: '–ù–∏–∑–∫–∞—è üü¢',
                9: '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢',
                10: '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è üü¢'
            }
            
            analysis['competition_level'] = competition_levels.get(score, '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥')
            analysis['competition_score'] = score
            analysis['opportunity_rating'] = min(score * 10, 100)
            
            return analysis, df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}", exc_info=True)
            return {}, pd.DataFrame()

class AdvancedTrendsAnalyzer:
    def __init__(self, cache: CacheManager):
        self.cache = cache
        
    def _get_pytrends(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ pytrends —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            return TrendReq(hl='ru-RU', tz=180, timeout=(10, 25), retries=2, backoff_factor=0.1)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Trends: {e}")
            return None

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
            
        pytrends = self._get_pytrends()
        if not pytrends:
            return None
            
        try:
            pytrends.build_payload([keyword], timeframe='today 12-m', geo='') # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
            interest_12m = pytrends.interest_over_time()
            
            if interest_12m.empty or keyword not in interest_12m.columns:
                 st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è '{keyword}'.")
                 return None
            
            try:
                pytrends.build_payload([keyword], timeframe='today 5-y', geo='') # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
                interest_5y = pytrends.interest_over_time()
            except:
                interest_5y = pd.DataFrame()
            
            try:
                related_queries = pytrends.related_queries()
                rising_queries = related_queries.get(keyword, {}).get('rising', pd.DataFrame())
                top_queries = related_queries.get(keyword, {}).get('top', pd.DataFrame())
            except:
                rising_queries = pd.DataFrame()
                top_queries = pd.DataFrame()
            
            series = interest_12m[keyword]
            recent_avg = series.tail(4).mean()
            previous_avg = series.iloc[-8:-4].mean()
            overall_avg = series.mean()
            
            if recent_avg > previous_avg * 1.2:
                trend_direction = "–ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—É—â–∏–π üöÄ"
            elif recent_avg > previous_avg * 1.1:
                trend_direction = "–†–∞—Å—Ç—É—â–∏–π üìà"
            elif recent_avg < previous_avg * 0.8:
                trend_direction = "–ü–∞–¥–∞—é—â–∏–π üìâ"
            elif recent_avg < previous_avg * 0.9:
                trend_direction = "–°–ª–∞–±–æ –ø–∞–¥–∞—é—â–∏–π üìâ"
            else:
                trend_direction = "–°—Ç–∞–±–∏–ª—å–Ω—ã–π ‚û°Ô∏è"
            
            monthly_avg = series.groupby(series.index.month).mean()
            peak_months = monthly_avg.nlargest(3).index.tolist()
            
            result = {
                'interest_df': interest_12m,
                'interest_5y_df': interest_5y,
                'trend_direction': trend_direction,
                'recent_avg': recent_avg,
                'overall_avg': overall_avg,
                'trend_strength': abs(recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0,
                'rising_queries': rising_queries,
                'top_queries': top_queries,
                'peak_months': peak_months,
                'current_interest': series.iloc[-1] if not series.empty else 0
            }
            
            self.cache.set(cache_key, result, 'trends')
            return result
            
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

# --- 4. –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–ï–ì–û–í ---

@dataclass
class TagScore:
    keyword: str
    search_volume: int
    competition_score: int
    seo_score: int
    overall_score: int
    difficulty: str

class YouTubeTagAnalyzer:
    def __init__(self, serpapi_key: str = None, cache: CacheManager = None):
        self.serpapi_key = serpapi_key
        self.use_serpapi = bool(serpapi_key)
        self.cache = cache
        self.base_serpapi = "https://serpapi.com/search"
        
    def get_search_volume_serpapi(self, keyword: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –æ–±—ä–µ–º —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._estimate_search_volume_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_volume', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∏—Å–∫–µ
            search_info = data.get('search_information', {})
            if 'total_results' in search_info:
                volume = safe_int_conversion(search_info['total_results'])
            elif 'video_results' in data:
                volume = len(data['video_results']) * 150 # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞
            else:
                volume = self._estimate_search_volume_basic(keyword)

            if cache_key and self.cache:
                self.cache.set(cache_key, volume, 'serpapi')
            return volume
                
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ SerpAPI –¥–ª—è '{keyword}': {e}")
            
        return self._estimate_search_volume_basic(keyword)
    
    def _estimate_search_volume_basic(self, keyword: str) -> int:
        """–ë–∞–∑–æ–≤–∞—è —ç—Å—Ç–∏–º–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API"""
        word_count = len(keyword.split())
        char_count = len(keyword)
        
        base_volume = max(1000, 5000 - (word_count * 500) - (char_count * 10))
        
        popular_words = {
            '–∫–∞–∫', '—á—Ç–æ', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–æ–±–∑–æ—Ä', '—É—Ä–æ–∫', '—Ç—É—Ç–æ—Ä–∏–∞–ª',
            'guide', 'tutorial', 'how', 'what', 'review', 'tips'
        }
        
        bonus = sum(300 for word in keyword.lower().split() if word in popular_words)
        
        return min(base_volume + bonus, 50000)
    
    def analyze_competition_serpapi(self, keyword: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ —á–µ—Ä–µ–∑ SerpAPI"""
        if not self.use_serpapi:
            return self._analyze_competition_basic(keyword)
            
        cache_key = self.cache.generate_key('serpapi_competition', keyword) if self.cache else None
        if cache_key and self.cache:
            if cached_data := self.cache.get(cache_key):
                return cached_data
        
        try:
            params = {
                'api_key': self.serpapi_key,
                'engine': 'youtube',
                'search_query': keyword,
            }
            
            response = requests.get(self.base_serpapi, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if 'video_results' not in data:
                return self._analyze_competition_basic(keyword)
            
            videos = data['video_results'][:20]
            analysis = self._process_competition_data(videos, keyword)
            
            if cache_key and self.cache:
                self.cache.set(cache_key, analysis, 'serpapi') # –£–õ–£–ß–®–ï–ù–û: –∫–∞—Ç–µ–≥–æ—Ä–∏—è serpapi
            
            return analysis
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
            return self._analyze_competition_basic(keyword)
    
    def _analyze_competition_basic(self, keyword: str) -> dict:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API)"""
        word_count = len(keyword.split())
        
        if word_count == 1:
            competition_level = "High"
            optimized_ratio = 0.8
        elif word_count == 2:
            competition_level = "Medium"
            optimized_ratio = 0.6
        else:
            competition_level = "Low"
            optimized_ratio = 0.4
        
        return {
            'total_videos': 20,
            'optimized_titles': int(20 * optimized_ratio),
            'high_view_videos': max(1, int(20 * (1 - optimized_ratio))),
            'verified_channels': max(1, int(20 * 0.3)),
            'avg_views': 15000 if word_count == 1 else 8000,
            'keyword_in_title': int(20 * optimized_ratio),
            'recent_videos': 3,
            'competition_level': competition_level
        }
    
    def _process_competition_data(self, videos: list, keyword: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        analysis = {
            'total_videos': len(videos),
            'optimized_titles': 0,
            'high_view_videos': 0,
            'verified_channels': 0,
            'avg_views': 0,
            'keyword_in_title': 0,
            'recent_videos': 0
        }
        
        total_views = 0
        keyword_lower = keyword.lower()
        
        for video in videos:
            title = video.get('title', '').lower()
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π —Ñ—Ä–∞–∑—ã
            if keyword_lower in title:
                analysis['optimized_titles'] += 1
                analysis['keyword_in_title'] += 1
            
            views = self._extract_views(video.get('view_count_text', '0'))
            total_views += views
            
            if views > 100000:
                analysis['high_view_videos'] += 1
            
            channel = video.get('channel', {})
            if channel.get('verified', False):
                analysis['verified_channels'] += 1
            
            published_date = video.get('published_date', '')
            if self._is_recent(published_date):
                analysis['recent_videos'] += 1
        
        if analysis['total_videos'] > 0:
            analysis['avg_views'] = total_views // analysis['total_videos']
        
        return analysis
    
    def _extract_views(self, views_str: str) -> int:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞."""
        if not views_str or not isinstance(views_str, str):
            return 0
        
        views_str = views_str.lower().replace(',', '').replace(' views', '').strip()
        num_part = re.match(r'[\d.]+', views_str)
        if not num_part:
            return 0
        
        num = float(num_part.group(0))
        
        if 'k' in views_str:
            return int(num * 1000)
        if 'm' in views_str:
            return int(num * 1000000)
        if 'b' in views_str:
            return int(num * 1000000000)
            
        return int(num)
    
    def _is_recent(self, date_str: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏ –≤–∏–¥–µ–æ"""
        if not date_str:
            return False
        
        recent_indicators = ['day', 'days', 'week', 'weeks', 'hour', 'hours', '—á–∞—Å', '–¥–µ–Ω—å', '–Ω–µ–¥–µ–ª']
        return any(indicator in date_str.lower() for indicator in recent_indicators)
    
    def calculate_scores(self, keyword: str, analysis: dict, search_volume: int) -> TagScore:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ç–µ–≥–∞"""
        total = analysis.get('total_videos', 0)
        if total == 0:
            competition_score = 50
        else:
            optimized_ratio = analysis.get('optimized_titles', 0) / total
            high_views_ratio = analysis.get('high_view_videos', 0) / total
            verified_ratio = analysis.get('verified_channels', 0) / total
            
            avg_views_factor = min(analysis.get('avg_views', 0) / 500000, 1.0)
            
            competition_score = min(int((
                optimized_ratio * 0.3 +
                high_views_ratio * 0.25 +
                verified_ratio * 0.2 +
                avg_views_factor * 0.25
            ) * 100), 100)
        
        if total > 0:
            keyword_optimization = analysis.get('keyword_in_title', 0) / total
            seo_score = max(int((1.0 - keyword_optimization) * 100), 10)
        else:
            seo_score = 50
        
        volume_score = min(math.log10(max(search_volume, 1)) * 20, 100) # –£–õ–£–ß–®–ï–ù–û: —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –≤–µ—Å
        competition_inverted = 100 - competition_score
        
        overall_score = min(int(
            volume_score * 0.4 +
            competition_inverted * 0.35 +
            seo_score * 0.25
        ), 100)
        
        if competition_score <= 20:
            difficulty = "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 40:
            difficulty = "–ù–∏–∑–∫–∞—è üü¢"
        elif competition_score <= 60:
            difficulty = "–°—Ä–µ–¥–Ω—è—è üü°"
        elif competition_score <= 80:
            difficulty = "–í—ã—Å–æ–∫–∞—è üü†"
        else:
            difficulty = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥"
        
        return TagScore(
            keyword=keyword,
            search_volume=search_volume,
            competition_score=competition_score,
            seo_score=seo_score,
            overall_score=overall_score,
            difficulty=difficulty
        )
    
    def analyze_keyword(self, keyword: str) -> TagScore:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        search_volume = self.get_search_volume_serpapi(keyword)
        competition_analysis = self.analyze_competition_serpapi(keyword)
        
        return self.calculate_scores(keyword, competition_analysis, search_volume)
    
    def analyze_multiple_keywords(self, keywords: list) -> list:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        results = []
        if not keywords:
             return []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, keyword in enumerate(keywords):
            try:
                status_text.text(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–≥: {keyword} ({i+1}/{len(keywords)})")
                progress_bar.progress((i + 1) / len(keywords))
                
                result = self.analyze_keyword(keyword)
                results.append(result)
                
                if self.use_serpapi:
                    time.sleep(0.7) # –£–º–µ–Ω—å—à–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞
                    
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ '{keyword}': {e}")
                continue
        
        progress_bar.empty()
        status_text.empty()
        
        return sorted(results, key=lambda x: x.overall_score, reverse=True)

class ContentStrategist:
    def __init__(self, openai_key=None, openai_model=None):
        self.use_openai = bool(openai_key and openai_model)
        if self.use_openai:
            try:
                self.client = openai.OpenAI(api_key=openai_key)
                self.model = openai_model
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
                self.use_openai = False


    def get_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame, cache: CacheManager):
        if not comp_analysis: # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑
            return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."

        cache_key = None
        if self.use_openai:
            # –£–õ–£–ß–®–ï–ù–û: –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –∫–ª—é—á –∫—ç—à–∞
            cache_key = cache.generate_key('openai_v4', keyword, self.model, comp_analysis, trends_data)
            if cached_strategy := cache.get(cache_key):
                st.toast("ü§ñ AI –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –∫—ç—à–∞!", icon="üß†")
                return cached_strategy
        
        if self.use_openai:
            strategy = self._get_ai_strategy(keyword, comp_analysis, trends_data, df)
        else:
            strategy = self._get_rule_based_strategy(keyword, comp_analysis, df)
        
        if self.use_openai and cache_key and "–û—à–∏–±–∫–∞" not in strategy:
            cache.set(cache_key, strategy, 'openai')
        
        return strategy

    def _get_rule_based_strategy(self, keyword: str, comp_analysis: dict, df: pd.DataFrame):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–µ–∑ AI"""
        
        if not df.empty:
            titles = df['title'].tolist()
            popular_words = extract_keywords_from_titles(titles)
            top_words = [word for word, count in popular_words[:5]]
        else:
            top_words = []
        
        competition_level = comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        avg_views = comp_analysis.get('avg_views', 0)
        shorts_percentage = comp_analysis.get('shorts_percentage', 0)
        
        strategy_parts = []
        
        if '–Ω–∏–∑–∫–∞—è' in competition_level.lower():
            verdict = "üéØ **–û–¢–õ–ò–ß–ù–ê–Ø –í–û–ó–ú–û–ñ–ù–û–°–¢–¨!** –ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —à–∞–Ω—Å—ã –¥–ª—è —Ä–æ—Å—Ç–∞."
        elif '—Å—Ä–µ–¥–Ω—è—è' in competition_level.lower():
            verdict = "‚ö° **–•–û–†–û–®–ò–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–´** —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º. –ù—É–∂–Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è."
        else:
            verdict = "üî• **–í–´–°–û–ö–ê–Ø –ö–û–ù–ö–£–†–ï–ù–¶–ò–Ø** - —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
        
        strategy_parts.append(f"### üéØ –í–µ—Ä–¥–∏–∫—Ç\n{verdict}")
        
        insights = []
        if avg_views < 50000:
            insights.append("üí° –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ–≤—ã—Å–æ–∫–∏–µ - –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–¥–µ–ª–∏—Ç—å—Å—è –∫–∞—á–µ—Å—Ç–≤–æ–º")
        if shorts_percentage > 50:
            insights.append("üì± –ú–Ω–æ–≥–æ Shorts –≤ –Ω–∏—à–µ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç")
        if top_words:
            insights.append(f"üî§ –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {', '.join(top_words[:3])}")
        
        if insights:
             strategy_parts.append("### üîç –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã\n" + "\n".join(insights))

        content_ideas = [
            f"**–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {keyword}** - –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
            f"**–¢–æ–ø-5 –æ—à–∏–±–æ–∫ –≤ {keyword}** - —Ä–∞–∑–±–æ—Ä —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º",
            f"**{keyword}: –¥–æ –∏ –ø–æ—Å–ª–µ** - –∫–µ–π—Å—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
            f"**–ö–∞–∫ –Ω–∞—á–∞—Ç—å –≤ {keyword} –±–µ–∑ –æ–ø—ã—Ç–∞** - –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω",
            f"**–°–µ–∫—Ä–µ—Ç—ã {keyword}, –æ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –≥–æ–≤–æ—Ä—è—Ç** - –∏–Ω—Å–∞–π–¥–µ—Ä—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        ]
        
        if shorts_percentage > 30:
            content_ideas.extend([
                f"**{keyword} –∑–∞ 60 —Å–µ–∫—É–Ω–¥** - –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±—É—á–∞—é—â–∏–µ –≤–∏–¥–µ–æ",
                f"**–ë—ã—Å—Ç—Ä—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ {keyword}** - —Å–µ—Ä–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–æ–ª–∏–∫–æ–≤"
            ])
        
        strategy_parts.append("### üí° –ò–¥–µ–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n- " + "\n- ".join(content_ideas))
        
        optimization_tips = [
            "üé® **–Ø—Ä–∫–∏–µ –ø—Ä–µ–≤—å—é** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Ü–≤–µ—Ç–∞ –∏ —á–µ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç",
            "‚è∞ **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏** - —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ 18:00-21:00 –ø–æ –ú–°–ö",
            "üéØ **–¶–µ–ø–ª—è—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–∏—Å–ª–∞, –≤–æ–ø—Ä–æ—Å—ã, –∏–Ω—Ç—Ä–∏–≥—É",
            "üìù **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è** - –¥–æ–±–∞–≤—å—Ç–µ —Ç–∞–π–º-–∫–æ–¥—ã –∏ –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏",
            "üè∑Ô∏è **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–≥–∏** - –º–∏–∫—Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏ –Ω–∏—à–µ–≤—ã—Ö —Ç–µ–≥–æ–≤"
        ]
        
        strategy_parts.append("### üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n- " + "\n- ".join(optimization_tips))
        
        return "\n\n".join(strategy_parts)
    
    def _get_ai_strategy(self, keyword: str, comp_analysis: dict, trends_data: dict, df: pd.DataFrame):
        st.toast("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∞–Ω–∞–ª–∏–∑ –≤ OpenAI...", icon="üß†")
        
        top_titles = []
        top_channels = []
        if not df.empty:
            top_videos = df.nlargest(10, 'views')
            top_titles = top_videos['title'].tolist()
            top_channels = top_videos['channel'].value_counts().head(5).to_dict()
        
        trends_info = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        if trends_data:
            trends_info = f"{trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
            if 'recent_avg' in trends_data:
                trends_info += f" (—Ç–µ–∫—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å: {trends_data['recent_avg']:.0f})"
        
        prompt = f"""
        –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π YouTube-—Å—Ç—Ä–∞—Ç–µ–≥ —Å –æ–ø—ã—Ç–æ–º –±–æ–ª–µ–µ 10 –ª–µ—Ç. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∏—à–∏ –∏ —Å–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è.

        **–ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–ê–Ø –¢–ï–ú–ê:** "{keyword}"

        **–î–ê–ù–ù–´–ï –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:**
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ: {comp_analysis.get('total_videos', 0)}
        - –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {comp_analysis.get('competition_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
        - –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('avg_views', 0)):,}
        - –ú–µ–¥–∏–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã: {int(comp_analysis.get('median_views', 0)):,}
        - –ü—Ä–æ—Å–º–æ—Ç—Ä—ã —Ç–æ–ø-10: {int(comp_analysis.get('top_10_avg_views', 0)):,}
        - –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å: {comp_analysis.get('engagement_rate', 0):.2f}%
        - –ü—Ä–æ—Ü–µ–Ω—Ç Shorts: {comp_analysis.get('shorts_percentage', 0):.1f}%
        - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {comp_analysis.get('unique_channels', 0)}
        - –í–∏–¥–µ–æ –∑–∞ –Ω–µ–¥–µ–ª—é: {comp_analysis.get('videos_last_week', 0)}

        **–¢–†–ï–ù–î–´ GOOGLE:**
        {trends_info}

        **–¢–û–ü-5 –ó–ê–ì–û–õ–û–í–ö–û–í –ö–û–ù–ö–£–†–ï–ù–¢–û–í:**
        {chr(10).join(f"‚Ä¢ {title}" for title in top_titles[:5])}

        **–í–ï–î–£–©–ò–ï –ö–ê–ù–ê–õ–´:**
        {chr(10).join(f"‚Ä¢ {channel}: {count} –≤–∏–¥–µ–æ" for channel, count in list(top_channels.items())[:3])}

        **–ó–ê–î–ê–ù–ò–ï:**
        –°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:

        1. **üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–¥–∏–∫—Ç** - –æ—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ –Ω–∏—à–∏ –∏ –≥–ª–∞–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

        2. **üìä –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π** - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω –Ω–∏—à–∏

        3. **üé¨ –ö–æ–Ω—Ç–µ–Ω—Ç-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è** - 7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–¥–µ–π –¥–ª—è –≤–∏–¥–µ–æ —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏:
           - –ó–∞–≥–æ–ª–æ–≤–æ–∫
           - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
           - –§–æ—Ä–º–∞—Ç (—Ç—É—Ç–æ—Ä–∏–∞–ª/–æ–±–∑–æ—Ä/–∫–µ–π—Å/–∏ —Ç.–¥.)
           - –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

        4. **üöÄ –¢–∞–∫—Ç–∏–∫–∞ —Ä–æ—Å—Ç–∞** - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 30 –¥–Ω–µ–π

        5. **üí∞ –ú–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏—è** - 3 —Å–ø–æ—Å–æ–±–∞ –∑–∞—Ä–∞–±–æ—Ç–∫–∞ + –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏

        6. **üè∑Ô∏è SEO –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ–≥–∞–º, –ø—Ä–µ–≤—å—é, –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

        –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ actionable —Å–æ–≤–µ—Ç–∞—Ö.
        """

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI: {e}", exc_info=True)
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}"

# --- 5. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ---

def main():
    st.markdown('<h1 class="main-header">YouTube AI Strategist üß†</h1>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        
        st.subheader("üîë YouTube API")
        youtube_api_key = st.text_input(
            "YouTube API Key",
            type="password",
            help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –≤ Google Cloud Console"
        )
        
        if youtube_api_key:
            if validate_youtube_api_key(youtube_api_key):
                st.success("‚úÖ YouTube API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
                st.info("üí° YouTube –∫–ª—é—á–∏ –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'AIza...' –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç 39 —Å–∏–º–≤–æ–ª–æ–≤")
        
        st.markdown("---")
        
        st.subheader("ü§ñ AI-—Å—Ç—Ä–∞—Ç–µ–≥")
        use_openai = st.toggle("–í–∫–ª—é—á–∏—Ç—å AI-–∞–Ω–∞–ª–∏–∑ (OpenAI)", value=True)
        
        openai_api_key = ""
        openai_model = "gpt-4o-mini"
        
        if use_openai:
            openai_api_key = st.text_input(
                "OpenAI API Key",
                type="password",
                help="–ö–ª—é—á –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å—Ç—Ä–∞—Ç–µ–≥–∏–π"
            )
            
            if openai_api_key:
                if validate_openai_api_key(openai_api_key):
                    st.success("‚úÖ OpenAI API –∫–ª—é—á –≤–∞–ª–∏–¥–µ–Ω")
                else:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π OpenAI API –∫–ª—é—á")
                    st.info("üí° –ö–ª—é—á –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'sk-'")
            
            openai_model = st.selectbox(
                "–ú–æ–¥–µ–ª—å OpenAI",
                ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo"],
                index=1,
                help="gpt-4o-mini - –±—ã—Å—Ç—Ä–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ"
            )
        
        st.markdown("---")
        
        st.subheader("üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤")
        use_serpapi = st.toggle("–í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (SerpAPI)", value=False)
        
        serpapi_key = ""
        if use_serpapi:
            serpapi_key = st.text_input(
                "SerpAPI Key",
                type="password",
                help="–ö–ª—é—á –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"
            )
            
            if serpapi_key:
                if validate_serpapi_key(serpapi_key):
                    st.success("‚úÖ SerpAPI –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                else:
                    st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
            
            st.info("üí° SerpAPI –¥–∞–µ—Ç 100 –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü")
        
        st.markdown("---")
        
        st.subheader("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 20, 200, 100, 10)
        
        date_range_options = {
            "–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None,
            "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365,
            "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180,
            "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90,
            "–ó–∞ –º–µ—Å—è—Ü": 30
        }
        
        selected_date_range = st.selectbox(
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:",
            list(date_range_options.keys()),
            index=1
        )
        days_limit = date_range_options[selected_date_range]
        
        if not youtube_api_key:
            st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
            st.info("üìö [–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á](https://developers.google.com/youtube/v3/getting-started)")
            st.stop()
        
        cache = CacheManager()
        
        st.markdown("---")
        st.subheader("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
        
        cache_info = cache.get_cache_info()
        if 'error' not in cache_info:
            st.info(f"""
            **üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
            ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {cache_info['total_records']}
            ‚Ä¢ –†–∞–∑–º–µ—Ä: {cache_info['total_size_mb']} MB
            ‚Ä¢ –ü–æ–ø–∞–¥–∞–Ω–∏—è: {cache.stats['hits']}
            ‚Ä¢ –ü—Ä–æ–º–∞—Ö–∏: {cache.stats['misses']}
            ‚Ä¢ Hit Rate: {cache_info['hit_rate']}%
            """)
            
            if cache_info.get('categories'):
                with st.expander("üìÅ –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"):
                    for cat, info in cache_info['categories'].items():
                        st.text(f"‚Ä¢ {cat}: {info['count']} ({info['size_mb']} MB)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–π"):
                deleted = cache.clean_expired()
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {deleted} –∑–∞–ø–∏—Å–µ–π")
                st.rerun()
        
        with col2:
            if st.button("üí• –û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"):
                try:
                    if cache.db_path.exists():
                        cache.db_path.unlink()
                    st.success("–ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω")
                    st.rerun()
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
        
        st.markdown("---")
        st.subheader("‚öôÔ∏è –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        
        show_advanced = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        if show_advanced:
            request_delay = st.slider(
                "–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)",
                0.1, 2.0, REQUEST_DELAY, 0.1,
                help="–£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ API"
            )
            
            max_retries = st.slider(
                "–ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫",
                1, 5, MAX_RETRIES,
                help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö API"
            )
            
            globals()['REQUEST_DELAY'] = request_delay
            globals()['MAX_RETRIES'] = max_retries
        
        st.markdown("---")
        st.subheader("üë®‚Äçüíª –ê–≤—Ç–æ—Ä")
        st.markdown("""
        **–°–≤—è–∑–∞—Ç—å—Å—è:**
        - üí¨ [Telegram](https://t.me/i_gma)
        - üì¢ [–ö–∞–Ω–∞–ª –æ AI](https://t.me/igm_a)
        - üîó [GitHub](https://github.com/yourusername)
        """)

    st.markdown("### üéØ –í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        keyword = st.text_input(
            "",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã...",
            help="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ YouTube –Ω–∏—à–∏"
        )
    
    with col2:
        analyze_button = st.button(
            "üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!",
            type="primary",
            use_container_width=True,
            disabled=not keyword
        )

    st.markdown("**üí° –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**")
    example_cols = st.columns(3)
    
    examples = [
        "python –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
        "–º–æ–Ω—Ç–∞–∂ –≤–∏–¥–µ–æ",
        "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –∞–∫—Ü–∏–∏"
    ]
    
    for i, example in enumerate(examples):
        if example_cols[i % 3].button(f"üìå {example}", key=f"example_{i}"):
            keyword = example
            st.session_state.keyword = example # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ state –¥–ª—è rerun
            st.rerun()

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑, –µ—Å–ª–∏ keyword –µ—Å—Ç—å (–≤ —Ç–æ–º —á–∏—Å–ª–µ –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ –ø—Ä–∏–º–µ—Ä)
    if 'keyword' in st.session_state and st.session_state.keyword:
        keyword = st.session_state.keyword
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª–æ—Å—å –ø–æ–≤—Ç–æ—Ä–Ω–æ –±–µ–∑ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏
        del st.session_state.keyword 
        analyze_button = True


    if analyze_button and keyword:
        try:
            analyzer = YouTubeAnalyzer(youtube_api_key, cache)
            trends_analyzer = AdvancedTrendsAnalyzer(cache)
            
            # –£–õ–£–ß–®–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –¥–æ–ª–≥–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
            if not analyzer.test_connection():
                st.stop()
            
            spinner_text = "üåä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é YouTube..."
            if use_openai and openai_api_key and validate_openai_api_key(openai_api_key):
                spinner_text += " –ü—Ä–∏–≤–ª–µ–∫–∞—é AI..."

            with st.spinner(spinner_text):
                published_after_date = None
                if days_limit:
                    published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z"
                
                videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
                
                if videos is None:
                    st.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ YouTube API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Å–æ–ª—å –∏ –ª–æ–≥-—Ñ–∞–π–ª.")
                    st.stop()
                
                if not videos:
                    st.warning(f"üîç –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{keyword}'. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
                    st.markdown("""
                    - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (—Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –±–æ–ª–µ–µ –æ–±—â–∏–º –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º)
                    - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ '–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è'
                    - –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ
                    """)
                    st.stop()
                
                comp_analysis, df = analyzer.analyze_competition(videos)
                trends_data = trends_analyzer.analyze_keyword_trends(keyword)
                
                strategist = ContentStrategist(
                    openai_api_key if use_openai and validate_openai_api_key(openai_api_key) else None,
                    openai_model if use_openai else None
                )
                strategy_output = strategist.get_strategy(keyword, comp_analysis, trends_data, df, cache)

            st.markdown("---")
            st.markdown(f"# üìä –ê–Ω–∞–ª–∏–∑ –Ω–∏—à–∏: **{keyword}**")
            
            st.markdown("### üéØ –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric(
                    "üìπ –í–∏–¥–µ–æ",
                    f"{len(df)}",
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ"
                )
            
            with col2:
                competition_level = comp_analysis.get('competition_level', 'N/A')
                st.metric(
                    "üèÜ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è",
                    competition_level.split()[0],
                    help=f"–ü–æ–ª–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {competition_level}"
                )
            
            with col3:
                avg_views = comp_analysis.get('avg_views', 0)
                st.metric(
                    "üëÄ –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã",
                    safe_format_number(int(avg_views)),
                    help=f"–¢–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(avg_views):,}"
                )
            
            with col4:
                engagement = comp_analysis.get('engagement_rate', 0)
                st.metric(
                    "üí¨ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
                    f"{engagement:.1f}%",
                    help="–ù–∞—Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω–æ –∑—Ä–∏—Ç–µ–ª–∏ —Å—Ç–∞–≤—è—Ç –ª–∞–π–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç"
                )
            
            with col5:
                channels = comp_analysis.get('unique_channels', 0)
                st.metric(
                    "üì∫ –ö–∞–Ω–∞–ª–æ–≤",
                    channels,
                    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ"
                )

            st.markdown("### üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üî• –¢–æ–ø-10 –≤–∏–¥–µ–æ",
                    safe_format_number(int(comp_analysis.get('top_10_avg_views', 0))),
                    help="–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã —É –ª—É—á—à–∏—Ö 10 –≤–∏–¥–µ–æ"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col2:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üì± –ö–æ—Ä–æ—Ç–∫–∏–µ –≤–∏–¥–µ–æ",
                    f"{comp_analysis.get('shorts_percentage', 0):.0f}%",
                    help="–ü—Ä–æ—Ü–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ (–¥–æ 1 –º–∏–Ω—É—Ç—ã)"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col3:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                st.metric(
                    "üóìÔ∏è –ó–∞ –Ω–µ–¥–µ–ª—é",
                    f"{comp_analysis.get('videos_last_week', 0)} —à—Ç.",
                    help="–°–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ –≤—ã—à–ª–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é"
                )
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col4:
                st.markdown('<div class="metric-card">', unsafe_allow_html=True)
                avg_duration = comp_analysis.get('avg_duration', 0)
                if avg_duration > 0:
                    duration_str = f"{avg_duration:.1f} –º–∏–Ω"
                else:
                    duration_str = "N/A"
                st.metric(
                    "‚è±Ô∏è –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞",
                    duration_str,
                    help="–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–∏–¥–µ–æ (–±–µ–∑ –∫–æ—Ä–æ—Ç–∫–∏—Ö)"
                )
                st.markdown('</div>', unsafe_allow_html=True)

            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "üéØ AI –°–æ–≤–µ—Ç—ã",
                "üè∑Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤",
                "üìà –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å",
                "üèÜ –¢–æ–ø –≤–∏–¥–µ–æ",
                "üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"
            ])

            with tab1:
                css_class = "openai-result" if strategist.use_openai else "custom-container"
                st.markdown(f'<div class="{css_class}">', unsafe_allow_html=True)
                st.markdown(strategy_output)
                st.markdown('</div>', unsafe_allow_html=True)
                
                if not df.empty:
                    st.markdown("### üîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã")
                    
                    titles = df['title'].tolist()
                    popular_words = extract_keywords_from_titles(titles)
                    
                    if popular_words:
                        st.markdown("**üè∑Ô∏è –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö:**")
                        words_cols = st.columns(5)
                        for i, (word, count) in enumerate(popular_words[:5]):
                            words_cols[i].metric(word, count)
                    
                    top_channels_df = df.drop_duplicates(subset=['channel_id']).nlargest(10, 'subscribers')
                    
                    if not top_channels_df.empty:
                        st.markdown("**üì∫ –í–µ–¥—É—â–∏–µ –∫–∞–Ω–∞–ª—ã –≤ –Ω–∏—à–µ (–ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º):**")
                        st.dataframe(
                            top_channels_df[['channel', 'subscribers_formatted', 'channel_video_count', 'channel_total_views']].rename(columns={
                                'channel': '–ö–∞–Ω–∞–ª',
                                'subscribers_formatted': '–ü–æ–¥–ø–∏—Å—á–∏–∫–æ–≤',
                                'channel_video_count': '–í—Å–µ–≥–æ –≤–∏–¥–µ–æ',
                                'channel_total_views': '–í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤'
                            }),
                            use_container_width=True
                        )

            with tab2:
                st.markdown("### üè∑Ô∏è –ö–∞–∫–∏–µ —Ç–µ–≥–∏ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
                
                all_tags = [tag.lower() for video in videos if 'tags' in video and video['tags'] for tag in video['tags']]
                
                if all_tags:
                    tag_popularity = Counter(all_tags)
                    
                    st.markdown("#### üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏ –≤ –Ω–∏—à–µ")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üî• –¢–æ–ø —Ç–µ–≥–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:**")
                        # –£–õ–£–ß–®–ï–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ session_state –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–±–æ—Ä–∞
                        if 'selected_tags' not in st.session_state:
                            st.session_state.selected_tags = []
                        
                        for i, (tag, count) in enumerate(tag_popularity.most_common(10)):
                            if st.checkbox(f"{tag} ({count} —Ä–∞–∑)", key=f"tag_{i}", value=(tag in st.session_state.selected_tags)):
                                if tag not in st.session_state.selected_tags:
                                    st.session_state.selected_tags.append(tag)
                            elif tag in st.session_state.selected_tags:
                                st.session_state.selected_tags.remove(tag)

                    with col2:
                        st.markdown("**‚ûï –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏ —Ç–µ–≥–∏:**")
                        custom_tags = st.text_area(
                            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é:",
                            placeholder="–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, python",
                            help="–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                            key="custom_tags_input"
                        )
                        
                    current_selection = list(st.session_state.selected_tags)
                    if custom_tags:
                        custom_list = [tag.strip().lower() for tag in custom_tags.split(',') if tag.strip()]
                        current_selection.extend(custom_list)
                    
                    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    current_selection = list(dict.fromkeys(current_selection))

                    if current_selection:
                        if st.button(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å {len(current_selection)} —Ç–µ–≥–æ–≤", type="primary"):
                            tag_analyzer = YouTubeTagAnalyzer(
                                serpapi_key if use_serpapi and validate_serpapi_key(serpapi_key) else None,
                                cache
                            )
                            
                            with st.spinner(f"üè∑Ô∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å {len(current_selection)} —Ç–µ–≥–æ–≤..."):
                                tag_results = tag_analyzer.analyze_multiple_keywords(current_selection[:20]) # –õ–∏–º–∏—Ç –Ω–∞ 20 —Ç–µ–≥–æ–≤ –∑–∞ —Ä–∞–∑
                            
                            if tag_results:
                                st.markdown("#### üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–≥–æ–≤")
                                
                                results_data = []
                                for result in tag_results:
                                    results_data.append({
                                        '–¢–µ–≥': result.keyword,
                                        '–û–±—ä–µ–º': safe_format_number(result.search_volume),
                                        '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è': f"{result.competition_score}/100",
                                        'SEO': f"{result.seo_score}/100",
                                        '–û—Ü–µ–Ω–∫–∞': f"{result.overall_score}/100",
                                        '–°–ª–æ–∂–Ω–æ—Å—Ç—å': result.difficulty
                                    })
                                
                                results_df = pd.DataFrame(results_data)
                                st.dataframe(results_df, use_container_width=True, hide_index=True)
                                
                                st.markdown("#### üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–µ–≥–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
                                
                                recommended_tags = [r.keyword for r in tag_results if r.overall_score >= 50]
                                if recommended_tags:
                                    st.code(", ".join(recommended_tags))
                                else:
                                    st.info("–ù–µ—Ç —Ç–µ–≥–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –º–µ–Ω–µ–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.")

                                csv_tags = pd.DataFrame([{
                                    'Keyword': r.keyword,
                                    'Search_Volume': r.search_volume,
                                    'Competition_Score': r.competition_score,
                                    'SEO_Score': r.seo_score,
                                    'Overall_Score': r.overall_score,
                                    'Difficulty': r.difficulty
                                } for r in tag_results])
                                
                                csv_tags_export = csv_tags.to_csv(index=False).encode('utf-8')
                                st.download_button(
                                    "üì• –°–∫–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ (CSV)",
                                    csv_tags_export,
                                    f'tag_analysis_{keyword.replace(" ", "_")}.csv',
                                    'text/csv'
                                )
                        
                        if not use_serpapi:
                            st.info("üí° **–°–æ–≤–µ—Ç**: –í–∫–ª—é—á–∏—Ç–µ SerpAPI –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–≥–æ–≤!")
                
                else:
                    st.warning("üè∑Ô∏è –¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Ç–µ–≥–∏ –≤ –ø–æ–ª–µ –≤—ã—à–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

            # ... (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
            with tab3:
                if trends_data and 'interest_df' in trends_data and not trends_data['interest_df'].empty:
                    st.markdown("### üìà –ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º—ã")
                    
                    interest_df = trends_data['interest_df']
                    
                    fig_trends = go.Figure()
                    fig_trends.add_trace(go.Scatter(
                        x=interest_df.index,
                        y=interest_df[keyword],
                        mode='lines+markers',
                        name='–ò–Ω—Ç–µ—Ä–µ—Å',
                        line=dict(color='#1f77b4', width=3),
                        marker=dict(size=6),
                        hovertemplate='<b>%{x}</b><br>–ò–Ω—Ç–µ—Ä–µ—Å: %{y}<extra></extra>'
                    ))
                    
                    fig_trends.update_layout(
                        title=f'–ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—É–ª—è—Ä–Ω–∞ —Ç–µ–º–∞: "{keyword}"',
                        xaxis_title='–î–∞—Ç–∞',
                        yaxis_title='–£—Ä–æ–≤–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–∞',
                        hovermode='x unified',
                        template='plotly_dark'
                    )
                    
                    st.plotly_chart(fig_trends, use_container_width=True)
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric(
                            "üìä –¢–µ–Ω–¥–µ–Ω—Ü–∏—è",
                            trends_data.get('trend_direction', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        )
                    
                    with col2:
                        current_interest = trends_data.get('current_interest', 0)
                        st.metric(
                            "üéØ –ò–Ω—Ç–µ—Ä–µ—Å —Å–µ–π—á–∞—Å",
                            f"{current_interest:.0f}/100"
                        )
                    
                    with col3:
                        trend_strength = trends_data.get('trend_strength', 0) * 100
                        st.metric(
                            "‚ö° –°–∏–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π",
                            f"{trend_strength:.1f}%"
                        )
                    
                    if 'top_queries' in trends_data and not trends_data['top_queries'].empty:
                        st.markdown("### üîç –ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**üìä –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**")
                            top_queries = trends_data['top_queries'].head(10)
                            for idx, row in top_queries.iterrows():
                                st.write(f"‚Ä¢ {row['query']} ({row['value']}%)")
                        
                        with col2:
                            if 'rising_queries' in trends_data and not trends_data['rising_queries'].empty:
                                st.markdown("**üöÄ –†–∞—Å—Ç—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã:**")
                                rising_queries = trends_data['rising_queries'].head(10)
                                for idx, row in rising_queries.iterrows():
                                    growth = row['value']
                                    if growth == 'Breakout':
                                        growth = 'üî• –í–∑—Ä—ã–≤'
                                    st.write(f"‚Ä¢ {row['query']} (+{growth})")
                
                else:
                    st.warning("üìà –î–∞–Ω–Ω—ã–µ Google Trends –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ —Ç–µ–º–∞ —Å–ª–∏—à–∫–æ–º —É–∑–∫–∞—è")
                    st.markdown("""
                    **–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:**
                    - –¢–µ–º–∞ —Å–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤
                    - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å Google Trends API
                    - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    
                    **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
                    - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ –æ–±—â—É—é —Ç–µ–º—É
                    - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    - –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ
                    """)

            with tab4:
                st.markdown("### üèÜ –¢–æ–ø –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º")
                
                if not df.empty:
                    top_videos = df.nlargest(20, 'views')
                    
                    for idx, video in top_videos.iterrows():
                        with st.container():
                            col1, col2 = st.columns([1, 4])
                            
                            with col1:
                                if video.get('thumbnail'):
                                    st.image(video['thumbnail'])
                                else:
                                    st.write("üé¨")
                            
                            with col2:
                                st.markdown(f"""
                                **[{video['title']}]({video['video_url']})**
                                
                                üì∫ **{video['channel']}** ({video['subscribers_formatted']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)
                                
                                üëÄ **{video['views_formatted']} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤** ‚Ä¢ 
                                üëç **{video['likes_formatted']} –ª–∞–π–∫–æ–≤** ‚Ä¢ 
                                üí¨ **{safe_format_number(video['comments'])} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤** ‚Ä¢ 
                                ‚è±Ô∏è **{video['duration_formatted']}** ‚Ä¢ 
                                {video['short_indicator']}
                                
                                üìÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {video['published'][:10]}
                                """)
                            
                            st.markdown("---")
                    
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ø –≤–∏–¥–µ–æ")

            with tab5:
                st.markdown("### üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∏")
                
                if not df.empty:
                    st.markdown("#### üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤")
                    fig_views = px.histogram(
                        df, x='views', nbins=30, title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ',
                        labels={'views': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ'}
                    )
                    fig_views.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_views, use_container_width=True)
                    
                    st.markdown("#### üíù –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ª–∞–π–∫–æ–≤ –æ—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤")
                    fig_scatter = px.scatter(
                        df, x='views', y='likes', hover_data=['title', 'channel'],
                        title='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º–∏ –∏ –ª–∞–π–∫–∞–º–∏',
                        labels={'views': '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 'likes': '–õ–∞–π–∫–∏'}
                    )
                    fig_scatter.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_scatter, use_container_width=True)
                    
                    st.markdown("#### üì∫ –ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–æ–≤ (—Ç–æ–ø-15 –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º –≤ –≤—ã–±–æ—Ä–∫–µ)")
                    channel_stats = df.groupby('channel').agg(
                        –í–∏–¥–µ–æ=('video_id', 'count'),
                        –°—Ä–µ–¥–Ω–∏–µ_–ø—Ä–æ—Å–º–æ—Ç—Ä—ã=('views', 'mean'),
                        –ú–∞–∫—Å_–ø—Ä–æ—Å–º–æ—Ç—Ä—ã=('views', 'max'),
                        –ü–æ–¥–ø–∏—Å—á–∏–∫–∏=('subscribers', 'first'),
                        –°—Ä–µ–¥–Ω–∏–µ_–ª–∞–π–∫–∏=('likes', 'mean')
                    ).astype(int).sort_values('–°—Ä–µ–¥–Ω–∏–µ_–ø—Ä–æ—Å–º–æ—Ç—Ä—ã', ascending=False)
                    st.dataframe(channel_stats.head(15), use_container_width=True)
                    
                    st.markdown("#### ‚è∞ –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
                    df_time = df.copy()
                    df_time['published_date'] = pd.to_datetime(df_time['published']).dt.date
                    daily_stats = df_time.groupby('published_date').agg(–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–≤–∏–¥–µ–æ=('video_id', 'count')).reset_index()
                    fig_time = px.line(
                        daily_stats, x='published_date', y='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–≤–∏–¥–µ–æ',
                        title='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –ø–æ –¥–Ω—è–º'
                    )
                    fig_time.update_layout(template='plotly_dark')
                    st.plotly_chart(fig_time, use_container_width=True)
                    
                    st.markdown("#### üì• –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
                    export_df = df[[
                        'title', 'channel', 'views', 'likes', 'comments', 
                        'duration_formatted', 'published', 'video_url'
                    ]].copy()
                    
                    export_df.columns = [
                        '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–ö–∞–Ω–∞–ª', '–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', '–õ–∞–π–∫–∏', '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏',
                        '–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏', 'URL'
                    ]
                    
                    csv_data = export_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV)", csv_data,
                        f'youtube_analysis_{keyword.replace(" ", "_")}.csv', 'text/csv'
                    )
                else:
                    st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")

        except Exception as e:
            st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}", exc_info=True)
            st.info("üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
            st.markdown("""
            - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å API –∫–ª—é—á–µ–π
            - –ò–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            - –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            - –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É (Ctrl+F5)
            """)

if __name__ == "__main__":
    main()