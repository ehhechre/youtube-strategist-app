# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from datetime import datetime, timedelta
import plotly.express as px
import re
import hashlib
import pickle
from pathlib import Path
import sqlite3
import threading
import warnings
import time
import logging
from pytrends.request import TrendReq
import numpy as np
from collections import Counter
import unicodedata

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´ –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
st.set_page_config(
    page_title="YouTube AI Strategist üß†",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('youtube_strategist.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è API –ª–∏–º–∏—Ç–æ–≤
YOUTUBE_API_DAILY_QUOTA = 10000
REQUEST_DELAY = 0.1
MAX_RETRIES = 3
REQUEST_TIMEOUT = 30

st.markdown("""
<style>
    .main-header {
        font-size: 2.8rem;
        color: #FF0000;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
        background: linear-gradient(90deg, #FF0000 0%, #FF6B6B 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .stButton>button {
        border-radius: 8px;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .custom-container {
        background: linear-gradient(135deg, rgba(42, 57, 62, 0.5), rgba(62, 77, 82, 0.3));
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #00a0dc;
        margin-top: 1rem;
        backdrop-filter: blur(10px);
    }
    .insight-box {
        background: linear-gradient(135deg, #262730, #3a3b45);
        padding: 1rem;
        border-radius: 15px;
        margin-top: 1rem;
        border: 1px solid #444;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #1e1e2e, #2a2a3e);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #444;
        margin: 0.5rem 0;
    }
    .success-alert {
        background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(34, 197, 94, 0.05));
        border: 1px solid rgba(34, 197, 94, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-alert {
        background: linear-gradient(135deg, rgba(251, 146, 60, 0.1), rgba(251, 146, 60, 0.05));
        border: 1px solid rgba(251, 146, 60, 0.3);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# --- 2. –£–¢–ò–õ–ò–¢–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø ---

def validate_youtube_api_key(api_key: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ YouTube API –∫–ª—é—á–∞"""
    if not api_key or not isinstance(api_key, str):
        return False
    api_key = api_key.strip()
    if api_key.startswith('AIza') and len(api_key) == 39:
        return True
    if len(api_key) > 30 and re.match(r'^[A-Za-z0-9_-]+$', api_key):
        return True
    return False

def safe_format_number(num) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª"""
    try:
        if pd.isna(num) or num is None:
            return "0"
        num = float(num)
        if num >= 1_000_000_000:
            return f"{num/1_000_000_000:.1f}B"
        elif num >= 1_000_000:
            return f"{num/1_000_000:.1f}M"
        elif num >= 1_000:
            return f"{num/1_000:.1f}K"
        return str(int(num))
    except (ValueError, TypeError, OverflowError):
        return "0"

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not isinstance(text, str):
        return ""
    text = unicodedata.normalize('NFKD', text)
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    return text.strip()

def safe_int_conversion(value, default=0) -> int:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    try:
        if pd.isna(value) or value is None:
            return default
        return int(float(value))
    except (ValueError, TypeError, OverflowError):
        return default

def safe_float_conversion(value, default=0.0) -> float:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
    try:
        if pd.isna(value) or value is None:
            return default
        return float(value)
    except (ValueError, TypeError, OverflowError):
        return default

def validate_keyword(keyword: str) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    if not keyword or not isinstance(keyword, str):
        return False
    keyword = keyword.strip()
    if len(keyword) < 2 or len(keyword) > 100:
        return False
    if keyword.count(' ') > 10:
        return False
    if re.search(r'[<>"\'\[\]{}|\\`]', keyword):
        return False
    return True

def retry_api_call(func, max_retries=MAX_RETRIES, delay=REQUEST_DELAY):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ API –≤—ã–∑–æ–≤–æ–≤"""
    def wrapper(*args, **kwargs):
        last_exception = None
        for attempt in range(max_retries):
            try:
                result = func(*args, **kwargs)
                if attempt > 0:
                    logger.info(f"API –≤—ã–∑–æ–≤ —É—Å–ø–µ—à–µ–Ω —Å {attempt + 1} –ø–æ–ø—ã—Ç–∫–∏")
                return result
            except HttpError as e:
                last_exception = e
                status_code = e.resp.status
                if status_code == 403:
                    st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á –∏ –µ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ Google Cloud Console.")
                    logger.error(f"–û—à–∏–±–∫–∞ 403 (Forbidden). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code == 400:
                    st.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
                    logger.error(f"–û—à–∏–±–∫–∞ 400 (Bad Request). –î–µ—Ç–∞–ª–∏: {e.content}")
                    break
                elif status_code in [500, 502, 503, 504]:
                    logger.warning(f"–°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {status_code}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))
                        continue
                else:
                    logger.error(f"HTTP –æ—à–∏–±–∫–∞ {status_code}: {e}")
                    break
            except Exception as e:
                last_exception = e
                logger.warning(f"–û—à–∏–±–∫–∞ API –≤—ã–∑–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay)
                    continue
                break
        logger.error(f"API –≤—ã–∑–æ–≤ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {last_exception}")
        raise last_exception
    return wrapper

# --- 3. –ö–õ–ê–°–°–´-–ê–ù–ê–õ–ò–ó–ê–¢–û–†–´ ---

class CacheManager:
    def __init__(self, cache_dir: str = "data/cache"):
        self.db_path = Path(cache_dir) / "youtube_ai_cache.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        self._init_sqlite()
        self.ttl_map = {
            'search': 3600 * 4,
            'channels': 3600 * 24 * 7,
            'trends': 3600 * 8,
        }
        self.stats = {'hits': 0, 'misses': 0, 'errors': 0}

    def _init_sqlite(self):
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False, timeout=10.0)
                cursor = conn.cursor()
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS cache (
                        key TEXT PRIMARY KEY, value BLOB, expires_at TIMESTAMP
                    )
                ''')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_expires_at ON cache(expires_at)')
                conn.commit()
                conn.close()
        except sqlite3.Error as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")
            st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∞: {e}")

    def get(self, key: str):
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("SELECT value FROM cache WHERE key = ? AND expires_at > ?", (key, datetime.now()))
                result = cursor.fetchone()
                conn.close()
                if result:
                    self.stats['hits'] += 1
                    return pickle.loads(result[0])
                self.stats['misses'] += 1
                return None
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞: {e}")
            return None

    def set(self, key: str, value: any, category: str):
        try:
            with self.lock:
                ttl = self.ttl_map.get(category, 3600)
                expires_at = datetime.now() + timedelta(seconds=ttl)
                value_blob = pickle.dumps(value)
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("INSERT OR REPLACE INTO cache (key, value, expires_at) VALUES (?, ?, ?)", (key, value_blob, expires_at))
                conn.commit()
                conn.close()
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à: {e}")

    def clean_expired(self) -> int:
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("DELETE FROM cache WHERE expires_at < ?", (datetime.now(),))
                expired_count = cursor.rowcount
                conn.commit()
                conn.close()
                return expired_count
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
            return 0

    def get_cache_info(self) -> dict:
        try:
            with self.lock:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM cache")
                count = cursor.fetchone()[0]
                conn.close()
                size_mb = self.db_path.stat().st_size / (1024 * 1024) if self.db_path.exists() else 0
                return {
                    'total_records': count or 0,
                    'total_size_mb': round(size_mb, 2),
                    'hit_rate': round(self.stats['hits'] / max(self.stats['hits'] + self.stats['misses'], 1) * 100, 1)
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ: {e}")
            return {'error': str(e)}

    def generate_key(self, *args) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–π –¥–ª—è –∫—ç—à–∞."""
        combined = "|".join(map(str, args))
        return hashlib.md5(combined.encode('utf-8')).hexdigest()

class YouTubeAnalyzer:
    def __init__(self, api_key: str, cache: CacheManager):
        try:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
            self.cache = cache
            self.api_key = api_key
            self.quota_used = 0
            logger.info("YouTube API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ YouTube API: {e}")
            raise

    def test_connection(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API"""
        try:
            self.youtube.i18nLanguages().list(part='snippet', hl='en').execute()
            logger.info("YouTube API —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ.")
            return True
        except HttpError as e:
            logger.error(f"–¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            details = e.error_details[0] if hasattr(e, 'error_details') and e.error_details else {}
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ YouTube: {e.resp.status} - {details.get('reason', 'Unknown')}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à API –∫–ª—é—á.")
            return False
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å YouTube API: {e}")
            return False

    def _make_api_request(self, request_func, *args, **kwargs):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –∫–≤–æ—Ç"""
        try:
            if self.quota_used > YOUTUBE_API_DAILY_QUOTA * 0.9:
                st.warning("‚ö†Ô∏è –ü—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ –ª–∏–º–∏—Ç—É YouTube API –∫–≤–æ—Ç—ã")
            response = retry_api_call(request_func)(*args, **kwargs).execute()
            return response
        except HttpError as e:
            if e.resp.status == 403:
                st.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ YouTube API –∏–ª–∏ –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
            elif e.resp.status == 400:
                st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ YouTube API")
            else:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ YouTube API: {e}")
            raise
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ API: {e}")
            raise

    def get_channel_stats(self, channel_ids: list):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤"""
        if not channel_ids: return {}
        unique_ids = list(set(filter(None, channel_ids)))
        if not unique_ids: return {}
        cache_key = self.cache.generate_key('channels', sorted(unique_ids))
        if cached_data := self.cache.get(cache_key):
            return cached_data
        
        channel_stats = {}
        try:
            for i in range(0, len(unique_ids), 50):
                chunk_ids = unique_ids[i:i+50]
                request = self.youtube.channels().list(part="statistics,snippet,brandingSettings", id=",".join(chunk_ids))
                response = self._make_api_request(lambda: request)
                self.quota_used += 1

                for item in response.get('items', []):
                    stats = item.get('statistics', {})
                    snippet = item.get('snippet', {})
                    
                    channel_stats[item['id']] = {
                        'subscribers': safe_int_conversion(stats.get('subscriberCount', 0)),
                        'total_views': safe_int_conversion(stats.get('viewCount', 0)),
                        'video_count': safe_int_conversion(stats.get('videoCount', 0)),
                        'title': clean_text(snippet.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')),
                        'verified': 'verified' in str(snippet.get('thumbnails', {})),
                    }
                if i + 50 < len(unique_ids):
                    time.sleep(REQUEST_DELAY)
            
            self.cache.set(cache_key, channel_stats, 'channels')
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {len(channel_stats)} –∫–∞–Ω–∞–ª–æ–≤")
            return channel_stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤: {e}")
            return {}

    def search_videos(self, keyword: str, max_results: int = 100, published_after=None):
        """–ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not validate_keyword(keyword):
            st.error("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
            return None
        max_results = min(max_results, 500)
        
        cache_key = self.cache.generate_key('search_v5_simplified', keyword, max_results, published_after)
        if cached_data := self.cache.get(cache_key):
            st.toast("üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
        
        try:
            video_snippets = []
            next_page_token = None
            search_params = {'q': clean_text(keyword), 'part': 'snippet', 'type': 'video', 'order': 'relevance'}
            if published_after:
                search_params['publishedAfter'] = published_after

            progress_bar = st.progress(0)
            status_text = st.empty()
            
            fetched_count = 0
            while fetched_count < max_results:
                search_params['maxResults'] = min(50, max_results - fetched_count)
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                status_text.text(f"üîç –ò—â–µ–º –≤–∏–¥–µ–æ: {fetched_count}/{max_results}")
                progress_bar.progress(fetched_count / max_results)
                
                request = self.youtube.search().list(**search_params)
                search_response = self._make_api_request(lambda: request)
                self.quota_used += 100 # Search operation is costly
                new_items = search_response.get('items', [])
                
                if not new_items: break
                video_snippets.extend(new_items)
                fetched_count = len(video_snippets)
                next_page_token = search_response.get('nextPageToken')
                if not next_page_token: break
                time.sleep(REQUEST_DELAY)

            progress_bar.progress(1.0)
            status_text.text(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(video_snippets)} –≤–∏–¥–µ–æ. –°–æ–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª–∏...")
            
            if not video_snippets: return []

            video_ids = [item['id']['videoId'] for item in video_snippets if 'videoId' in item.get('id', {})]
            channel_ids = list(set([item['snippet']['channelId'] for item in video_snippets]))

            status_text.text("üìä –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤...")
            channel_stats = self.get_channel_stats(channel_ids)
            
            videos = []
            all_video_details = []
            for i in range(0, len(video_ids), 50):
                chunk_ids = video_ids[i:i+50]
                status_text.text(f"üìä –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –≤–∏–¥–µ–æ ({i+len(chunk_ids)}/{len(video_ids)})...")
                
                request = self.youtube.videos().list(part='statistics,contentDetails,snippet', id=','.join(chunk_ids))
                stats_response = self._make_api_request(lambda: request)
                self.quota_used += 1
                all_video_details.extend(stats_response.get('items', []))
                if i + 50 < len(video_ids):
                    time.sleep(REQUEST_DELAY)

            video_details_map = {item['id']: item for item in all_video_details}
            
            for snippet_item in video_snippets:
                video_id = snippet_item['id'].get('videoId')
                if not video_id or video_id not in video_details_map:
                    continue
                
                details = video_details_map[video_id]
                stats = details.get('statistics', {})
                content_details = details.get('contentDetails', {})
                video_snippet = details.get('snippet', {})
                
                duration = self._parse_duration(content_details.get('duration', 'PT0S'))
                channel_id = video_snippet.get('channelId')
                channel_info = channel_stats.get(channel_id, {})
                
                video_data = {
                    'video_id': video_id,
                    'title': clean_text(video_snippet.get('title', '')),
                    'channel': clean_text(video_snippet.get('channelTitle', '')),
                    'channel_id': channel_id,
                    'subscribers': channel_info.get('subscribers', 0),
                    'subscribers_formatted': safe_format_number(channel_info.get('subscribers', 0)),
                    'published': video_snippet.get('publishedAt', ''),
                    'views': safe_int_conversion(stats.get('viewCount', 0)),
                    'views_formatted': safe_format_number(safe_int_conversion(stats.get('viewCount', 0))),
                    'likes': safe_int_conversion(stats.get('likeCount', 0)),
                    'likes_formatted': safe_format_number(safe_int_conversion(stats.get('likeCount', 0))),
                    'comments': safe_int_conversion(stats.get('commentCount', 0)),
                    'duration': duration,
                    'duration_formatted': self._format_duration(duration),
                    'is_short': duration <= 1.05,
                    'short_indicator': "ü©≥ Shorts" if duration <= 1.05 else "üìπ –í–∏–¥–µ–æ",
                    'tags': video_snippet.get('tags', [])[:20],
                    'thumbnail': snippet_item['snippet'].get('thumbnails', {}).get('medium', {}).get('url', ''),
                    'video_url': f"https://www.youtube.com/watch?v={video_id}",
                    'video_url_markdown': f"[–°—Å—ã–ª–∫–∞](https://www.youtube.com/watch?v={video_id})"
                }
                videos.append(video_data)
            
            progress_bar.empty()
            status_text.empty()
            
            self.cache.set(cache_key, videos, 'search')
            logger.info(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(videos)} –≤–∏–¥–µ–æ –¥–ª—è '{keyword}'")
            return videos
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ search_videos –¥–ª—è '{keyword}': {e}", exc_info=True)
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∏–¥–µ–æ: {e}")
            return None

    def _parse_duration(self, duration_str: str) -> float:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ ISO 8601"""
        if not duration_str: return 0
        try:
            match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
            if not match: return 0
            h, m, s = (safe_int_conversion(g) for g in match.groups())
            return h * 60 + m + s / 60
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ '{duration_str}': {e}")
            return 0
    
    def _format_duration(self, duration_minutes: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        try:
            if duration_minutes is None: return "0:00"
            if duration_minutes < 1: return f"0:{int(duration_minutes * 60):02d}"
            total_seconds = int(duration_minutes * 60)
            hours, rem = divmod(total_seconds, 3600)
            minutes, seconds = divmod(rem, 60)
            if hours > 0: return f"{hours}:{minutes:02d}:{seconds:02d}"
            else: return f"{minutes}:{seconds:02d}"
        except Exception:
            return "0:00"
    
    def analyze_competition(self, videos: list):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ"""
        if not videos: return {}, pd.DataFrame()
        try:
            df = pd.DataFrame(videos)
            df['published'] = pd.to_datetime(df['published'], errors='coerce', utc=True).dt.tz_localize(None)
            df = df.dropna(subset=['published', 'views'])
            if df.empty:
                logger.warning("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö DataFrame –ø—É—Å—Ç")
                return {}, pd.DataFrame()
            
            df['days_ago'] = (datetime.now() - df['published']).dt.days.fillna(0)
            df['engagement_rate'] = np.where(df['views'] > 0, ((df['likes'] + df['comments']) / df['views']) * 100, 0)
            
            analysis = {
                'total_videos': len(df),
                'avg_views': safe_float_conversion(df['views'].mean()),
                'median_views': safe_float_conversion(df['views'].median()),
                'top_10_avg_views': safe_float_conversion(df.nlargest(min(10, len(df)), 'views')['views'].mean()),
                'engagement_rate': safe_float_conversion(df['engagement_rate'].mean()),
                'videos_last_week': len(df[df['days_ago'] <= 7]),
                'unique_channels': df['channel'].nunique(),
            }

            score = 0
            if analysis['top_10_avg_views'] < 20000: score += 4
            elif analysis['top_10_avg_views'] < 50000: score += 3
            elif analysis['top_10_avg_views'] < 200000: score += 2
            
            if analysis['videos_last_week'] < 2: score += 3
            elif analysis['videos_last_week'] < 5: score += 2
            
            if analysis['unique_channels'] < 15: score += 2
            elif analysis['unique_channels'] < 30: score += 1
            
            competition_levels = {
                0: '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥', 1: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥', 2: '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è üî¥',
                3: '–í—ã—Å–æ–∫–∞—è üü†', 4: '–í—ã—Å–æ–∫–∞—è üü†', 5: '–°—Ä–µ–¥–Ω—è—è üü°', 6: '–°—Ä–µ–¥–Ω—è—è üü°',
                7: '–ù–∏–∑–∫–∞—è üü¢', 8: '–ù–∏–∑–∫–∞—è üü¢', 9: '–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è üü¢', 10: '–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è üü¢'
            }
            analysis['competition_level'] = competition_levels.get(score, '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è üî¥')
            return analysis, df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}", exc_info=True)
            return {}, pd.DataFrame()

class AdvancedTrendsAnalyzer:
    def __init__(self, cache: CacheManager):
        self.cache = cache
        
    def _get_pytrends(self):
        try:
            return TrendReq(hl='ru-RU', tz=180, timeout=(10, 25), retries=2, backoff_factor=0.1)
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Trends: {e}")
            return None

    def analyze_keyword_trends(self, keyword: str):
        cache_key = self.cache.generate_key('advanced_trends', keyword)
        if cached_data := self.cache.get(cache_key):
            st.toast("üìà –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞!", icon="‚ö°Ô∏è")
            return cached_data
            
        pytrends = self._get_pytrends()
        if not pytrends: return None
            
        try:
            pytrends.build_payload([keyword], timeframe='today 12-m', geo='')
            interest_12m = pytrends.interest_over_time()
            
            if interest_12m.empty or keyword not in interest_12m.columns:
                 st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è '{keyword}'.")
                 return None
            
            result = {'interest_df': interest_12m}
            self.cache.set(cache_key, result, 'trends')
            return result
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Trends: {str(e)}")
            return None

# --- 4. –ì–õ–ê–í–ù–´–ô –ò–ù–¢–ï–†–§–ï–ô–° ---

def main():
    st.markdown('<h1 class="main-header">YouTube Data Strategist üìà</h1>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        st.subheader("üîë YouTube API")
        youtube_api_key = st.text_input("YouTube API Key", type="password", help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –≤ Google Cloud Console", key="youtube_api_key")
        
        if youtube_api_key:
            if validate_youtube_api_key(youtube_api_key):
                st.success("‚úÖ YouTube API –∫–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            else:
                st.warning("‚ö†Ô∏è –§–æ—Ä–º–∞—Ç –∫–ª—é—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–µ—Ä–Ω—ã–º")
        
        st.markdown("---")
        st.subheader("üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞")
        max_results = st.slider("–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", 20, 200, 100, 10, key="max_results")
        date_range_options = {"–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è": None, "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥": 365, "–ó–∞ 6 –º–µ—Å—è—Ü–µ–≤": 180, "–ó–∞ 3 –º–µ—Å—è—Ü–∞": 90, "–ó–∞ –º–µ—Å—è—Ü": 30}
        selected_date_range = st.selectbox("–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞:", list(date_range_options.keys()), index=1, key="date_range")
        days_limit = date_range_options[selected_date_range]
        
        if not youtube_api_key:
            st.warning("üëÜ –í–≤–µ–¥–∏—Ç–µ YouTube API –∫–ª—é—á –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
            st.stop()
        
        cache = CacheManager()
        st.markdown("---")
        st.subheader("üíæ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º")
        cache_info = cache.get_cache_info()
        if 'error' not in cache_info:
            st.info(f"–ó–∞–ø–∏—Å–µ–π: {cache_info.get('total_records', 0)}, –†–∞–∑–º–µ—Ä: {cache_info.get('total_size_mb', 0)} MB, Hit Rate: {cache_info.get('hit_rate', 0)}%")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–π"):
                st.success(f"–£–¥–∞–ª–µ–Ω–æ {cache.clean_expired()} –∑–∞–ø–∏—Å–µ–π")
                st.rerun()
        with col2:
            if st.button("üí• –û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"):
                if cache.db_path.exists(): cache.db_path.unlink(missing_ok=True)
                st.success("–ö—ç—à –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω"); st.rerun()
        
        st.markdown("---")
        st.info("–ê–≤—Ç–æ—Ä: [Telegram](https://t.me/i_gma)")

    keyword = st.text_input("üéØ –í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: n8n –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö...", key="keyword_input")
    
    col1, col2, col3 = st.columns(3)
    examples = ["python –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö", "–º–æ–Ω—Ç–∞–∂ –≤–∏–¥–µ–æ", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –∞–∫—Ü–∏–∏"]
    if col1.button(f"üìå {examples[0]}", use_container_width=True): st.session_state.keyword_input = examples[0]; st.rerun()
    if col2.button(f"üìå {examples[1]}", use_container_width=True): st.session_state.keyword_input = examples[1]; st.rerun()
    if col3.button(f"üìå {examples[2]}", use_container_width=True): st.session_state.keyword_input = examples[2]; st.rerun()
            
    if st.button("üöÄ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑!", type="primary", use_container_width=True, disabled=not keyword):
        try:
            analyzer = YouTubeAnalyzer(youtube_api_key, cache)
            if not analyzer.test_connection():
                st.stop()
            
            with st.spinner("üåä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é YouTube..."):
                published_after_date = (datetime.now() - timedelta(days=days_limit)).isoformat("T") + "Z" if days_limit else None
                videos = analyzer.search_videos(keyword, max_results, published_after=published_after_date)
                
                if not videos:
                    st.warning(f"üîç –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{keyword}'. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞.")
                    st.stop()
                
                comp_analysis, df = analyzer.analyze_competition(videos)
                trends_analyzer = AdvancedTrendsAnalyzer(cache)
                trends_data = trends_analyzer.analyze_keyword_trends(keyword)

            st.markdown(f"# üìä –ê–Ω–∞–ª–∏–∑ –Ω–∏—à–∏: **{keyword}**")
            
            cols = st.columns(5)
            cols[0].metric("üìπ –í–∏–¥–µ–æ", f"{len(df)}")
            cols[1].metric("üèÜ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è", comp_analysis.get('competition_level', 'N/A').split()[0])
            cols[2].metric("üëÄ –°—Ä–µ–¥. –ø—Ä–æ—Å–º–æ—Ç—Ä—ã", safe_format_number(int(comp_analysis.get('avg_views', 0))))
            cols[3].metric("üí¨ –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", f"{comp_analysis.get('engagement_rate', 0):.1f}%")
            cols[4].metric("üì∫ –ö–∞–Ω–∞–ª–æ–≤", comp_analysis.get('unique_channels', 0))

            tab1, tab2, tab3, tab4 = st.tabs(["üìà –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å", "üèÜ –¢–æ–ø –≤–∏–¥–µ–æ", "üè∑Ô∏è –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏", "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"])

            with tab1:
                if trends_data and 'interest_df' in trends_data and not trends_data['interest_df'].empty:
                    fig = px.line(trends_data['interest_df'], x=trends_data['interest_df'].index, y=keyword, title=f'–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–º—ã: "{keyword}" –∑–∞ 12 –º–µ—Å—è—Ü–µ–≤')
                    fig.update_layout(template='plotly_dark')
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("üìà –î–∞–Ω–Ω—ã–µ Google Trends –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")

            with tab2:
                st.markdown("### üèÜ –¢–æ–ø-10 –≤–∏–¥–µ–æ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º")
                if not df.empty:
                    for _, video in df.nlargest(10, 'views').iterrows():
                        with st.container(border=True):
                            col1, col2 = st.columns([1, 4])
                            with col1:
                                st.image(video.get('thumbnail', ''))
                            with col2:
                                st.markdown(f"""
                                **[{video['title']}]({video['video_url']})**<br>
                                üì∫ **{video['channel']}** ({video['subscribers_formatted']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)<br>
                                üëÄ {video['views_formatted']} ‚Ä¢ üëç {video['likes_formatted']} ‚Ä¢ ‚è±Ô∏è {video['duration_formatted']}
                                """, unsafe_allow_html=True)
            
            with tab3:
                st.markdown("### üè∑Ô∏è –°–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏ –≤ –Ω–∏—à–µ")
                all_tags = [tag.lower() for v in videos if v.get('tags') for tag in v['tags']]
                if all_tags:
                    tag_counts = Counter(all_tags).most_common(25)
                    tags_df = pd.DataFrame(tag_counts, columns=['–¢–µ–≥', '–ß–∞—Å—Ç–æ—Ç–∞'])
                    fig = px.bar(tags_df, x='–ß–∞—Å—Ç–æ—Ç–∞', y='–¢–µ–≥', orientation='h', title='–¢–æ–ø-25 —Ç–µ–≥–æ–≤')
                    fig.update_layout(template='plotly_dark', yaxis={'categoryorder':'total ascending'})
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.")
            
            with tab4:
                st.markdown("### üóÇÔ∏è –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ")
                if not df.empty:
                    display_df = df[['title', 'channel', 'subscribers', 'views', 'likes', 'duration_formatted', 'short_indicator', 'video_url_markdown', 'published']]
                    st.dataframe(display_df.rename(columns={
                        'title':'–ó–∞–≥–æ–ª–æ–≤–æ–∫',
                        'channel':'–ö–∞–Ω–∞–ª',
                        'subscribers': '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏',
                        'views':'–ü—Ä–æ—Å–º–æ—Ç—Ä—ã',
                        'likes':'–õ–∞–π–∫–∏',
                        'duration_formatted':'–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                        'short_indicator': '–¢–∏–ø –≤–∏–¥–µ–æ',
                        'video_url_markdown': 'URL',
                        'published':'–î–∞—Ç–∞'
                    }), use_container_width=True, hide_index=True)

                    csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV)", csv_data, f'youtube_analysis_{keyword.replace(" ", "_")}.csv', 'text/csv')

        except Exception as e:
            st.error(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}", exc_info=True)

if __name__ == "__main__":
    main()
